/* Umpire wrappers for all traceable MPI functions */

/* !!!!! This file is automatically generated by a wrapper-engine !!!!! */

#include <setjmp.h>
#include <errno.h>
#include "umpi_internal.h"

/* This is a hack to avoid warnings with MPICH, which doesn't declare this function.
   If we don't declare this, return type isimplicit int, and we get warnings about casting.
   ints to pointers on systems where sizeof(void*) != sizeof(int). */
extern void* MPIR_ToPointer(int);

#ifndef NO_REENTRY_GUARD
/* This is a guard against MPI implementations that make raw MPI calls within MPI calls.
   If you don't want to pay for this, #define NO_REENTRY_GUARD. */
static int in_wrapper=0;
#endif /* NO_REENTRY_GUARD */

#include "umpi_pc.h"

#include "Event.h"
#include "Trace.h"
#include "PtrHandler.h"
#include "ReqHandler.h"
#include "radix_tree.h"
#include "mpi_util.h"

int start_flag = 0;
#include <sys/time.h>
#include <mpi.h>
struct timeval IObegin, IOend, NPBbegin, NPBend;


/*--------------------------------------------- MPI_Abort */

/*-------------------- Wrapper for MPI_Abort omitted */

/*--------------------------------------------- MPI_Address */

/*-------------------- Wrapper for MPI_Address omitted */

/*--------------------------------------------- MPI_Allgather */

static int
umpi_mpi_MPI_Allgather_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Allgather_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Allgather;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Allgather_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Allgather_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, RECVCOUNT, op->data.mpi.recvcount, my_rank);
	addScalarValue(event, RECVTYPE, type_to_index(record_ptr, op->data.mpi.recvtype), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Allgather\n");
 }
 #endif 
 #ifdef ONLINE_CLUSTERING
   #ifdef AUTOMATIC_MARKER
   #ifdef ALLGATHER_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_Allgather) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Allgather_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Allgather;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Allgather_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Allgather( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Allgather(  (buf),
(count),
(datatype),
(recvbuf),
(recvcount),
(recvtype),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Allgather_pre(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			comm);
	rc = PMPI_Allgather(  (buf),
(count),
(datatype),
(recvbuf),
(recvcount),
(recvtype),
(comm));

umpi_mpi_MPI_Allgather_post(rc, pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Allgather(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		void *recvbuf ,
		int recvcount ,
		MPI_Datatype recvtype ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Allgather(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			comm );
	return rc;
}

static void mpi_allgather_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Allgather(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			recvbuf,
			*(recvcount),
			(MPI_Datatype)(*(recvtype)),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Allgather(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			recvbuf,
			*(recvcount),
			MPI_Type_f2c(*(recvtype)),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_ALLGATHER(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allgather_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, comm, ierr); }
extern void mpi_allgather(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allgather_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, comm, ierr); }
extern void mpi_allgather_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allgather_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, comm, ierr); }
extern void mpi_allgather__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allgather_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, comm, ierr); }

/*--------------------------------------------- MPI_Allgatherv */

static int
umpi_mpi_MPI_Allgatherv_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Allgatherv_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
const int *recvcounts ,
const int *displs ,
MPI_Datatype recvtype ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Allgatherv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.recvcounts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.recvcounts);
bcopy (recvcounts, uop->data.mpi.recvcounts, temp_umpi_comm_size * sizeof (int));
}
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.displs = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.displs);
bcopy (displs, uop->data.mpi.displs, temp_umpi_comm_size * sizeof (int));
}
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Allgatherv_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Allgatherv_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	int comm_size;
	PMPI_Comm_size(op->data.mpi.comm, &comm_size);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, RECVTYPE, type_to_index(record_ptr, op->data.mpi.recvtype), my_rank);
	addVectorValue(event, RECVCOUNTS, comm_size, op->data.mpi.recvcounts, my_rank);
	addVectorValue(event, DISPLS, comm_size, op->data.mpi.recvcounts, my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Allgatherv_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
const int *recvcounts ,
const int *displs ,
MPI_Datatype recvtype ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Allgatherv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.recvcounts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.recvcounts);
bcopy (recvcounts, uop->data.mpi.recvcounts, temp_umpi_comm_size * sizeof (int));
}
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.displs = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.displs);
bcopy (displs, uop->data.mpi.displs, temp_umpi_comm_size * sizeof (int));
}
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Allgatherv_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Allgatherv( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
const int *recvcounts ,
const int *displs ,
MPI_Datatype recvtype ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Allgatherv(  (buf),
(count),
(datatype),
(recvbuf),
(recvcounts),
(displs),
(recvtype),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Allgatherv_pre(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcounts,
			displs,
			recvtype,
			comm);
	rc = PMPI_Allgatherv(  (buf),
(count),
(datatype),
(recvbuf),
(recvcounts),
(displs),
(recvtype),
(comm));

umpi_mpi_MPI_Allgatherv_post(rc, pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcounts,
			displs,
			recvtype,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Allgatherv(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		void *recvbuf ,
		const int *recvcounts ,
		const int *displs ,
		MPI_Datatype recvtype ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Allgatherv(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcounts,
			displs,
			recvtype,
			comm );
	return rc;
}

static void mpi_allgatherv_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *displs , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Allgatherv(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			recvbuf,
			recvcounts,
			displs,
			(MPI_Datatype)(*(recvtype)),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Allgatherv(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			recvbuf,
			recvcounts,
			displs,
			MPI_Type_f2c(*(recvtype)),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_ALLGATHERV(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *displs , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allgatherv_f_wrap(buf, count, datatype, recvbuf, recvcounts, displs, recvtype, comm, ierr); }
extern void mpi_allgatherv(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *displs , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allgatherv_f_wrap(buf, count, datatype, recvbuf, recvcounts, displs, recvtype, comm, ierr); }
extern void mpi_allgatherv_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *displs , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allgatherv_f_wrap(buf, count, datatype, recvbuf, recvcounts, displs, recvtype, comm, ierr); }
extern void mpi_allgatherv__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *displs , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allgatherv_f_wrap(buf, count, datatype, recvbuf, recvcounts, displs, recvtype, comm, ierr); }

/*--------------------------------------------- MPI_Allreduce */

static int
umpi_mpi_MPI_Allreduce_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Allreduce_pre( 
void * pc , const void *buf ,
void *recvbuf ,
int count ,
MPI_Datatype datatype ,
MPI_Op op ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Allreduce;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.op = op;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Allreduce_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Allreduce_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, MPI_OP, op_to_index(record_ptr, op->data.mpi.op), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Allreduce\n");
 }
 #endif 
 #ifdef ONLINE_CLUSTERING
   #ifdef AUTOMATIC_MARKER
   #ifdef ALLREDUCE_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_Allreduce) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Allreduce_post( 
int MPI_rc, 
void * pc , const void *buf ,
void *recvbuf ,
int count ,
MPI_Datatype datatype ,
MPI_Op op ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Allreduce;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.op = op;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Allreduce_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Allreduce( 
void * pc , const void *buf ,
void *recvbuf ,
int count ,
MPI_Datatype datatype ,
MPI_Op op ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Allreduce(  (buf),
(recvbuf),
(count),
(datatype),
(op),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Allreduce_pre(pc,
			buf,
			recvbuf,
			count,
			datatype,
			op,
			comm);
	rc = PMPI_Allreduce(  (buf),
(recvbuf),
(count),
(datatype),
(op),
(comm));

umpi_mpi_MPI_Allreduce_post(rc, pc,
			buf,
			recvbuf,
			count,
			datatype,
			op,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Allreduce(  const void *buf ,
		void *recvbuf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Op op ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Allreduce(pc,
			buf,
			recvbuf,
			count,
			datatype,
			op,
			comm );
	return rc;
}

static void mpi_allreduce_f_wrap(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Allreduce(pc ,
			buf,
			recvbuf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Op)(*(op)),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Allreduce(pc ,
			buf,
			recvbuf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			MPI_Op_f2c(*(op)),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_ALLREDUCE(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allreduce_f_wrap(buf, recvbuf, count, datatype, op, comm, ierr); }
extern void mpi_allreduce(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allreduce_f_wrap(buf, recvbuf, count, datatype, op, comm, ierr); }
extern void mpi_allreduce_(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allreduce_f_wrap(buf, recvbuf, count, datatype, op, comm, ierr); }
extern void mpi_allreduce__(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_allreduce_f_wrap(buf, recvbuf, count, datatype, op, comm, ierr); }

/*--------------------------------------------- MPI_Alltoall */

static int
umpi_mpi_MPI_Alltoall_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Alltoall_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Alltoall;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Alltoall_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Alltoall_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, RECVCOUNT, op->data.mpi.recvcount, my_rank);
	addScalarValue(event, RECVTYPE, type_to_index(record_ptr, op->data.mpi.recvtype), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Alltoall\n");
 }
 #endif 
 #ifdef ONLINE_CLUSTERING
   #ifdef AUTOMATIC_MARKER
   #ifdef ALLTOALL_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_Alltoall) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Alltoall_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Alltoall;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Alltoall_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Alltoall( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Alltoall(  (buf),
(count),
(datatype),
(recvbuf),
(recvcount),
(recvtype),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Alltoall_pre(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			comm);
	rc = PMPI_Alltoall(  (buf),
(count),
(datatype),
(recvbuf),
(recvcount),
(recvtype),
(comm));

umpi_mpi_MPI_Alltoall_post(rc, pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Alltoall(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		void *recvbuf ,
		int recvcount ,
		MPI_Datatype recvtype ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Alltoall(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			comm );
	return rc;
}

static void mpi_alltoall_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Alltoall(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			recvbuf,
			*(recvcount),
			(MPI_Datatype)(*(recvtype)),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Alltoall(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			recvbuf,
			*(recvcount),
			MPI_Type_f2c(*(recvtype)),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_ALLTOALL(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_alltoall_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, comm, ierr); }
extern void mpi_alltoall(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_alltoall_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, comm, ierr); }
extern void mpi_alltoall_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_alltoall_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, comm, ierr); }
extern void mpi_alltoall__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_alltoall_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, comm, ierr); }

/*--------------------------------------------- MPI_Alltoallv */

static int
umpi_mpi_MPI_Alltoallv_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Alltoallv_pre( 
void * pc , const void *buf ,
const int *counts ,
const int *sdispls ,
MPI_Datatype datatype ,
void *recvbuf ,
const int *recvcounts ,
const int *rdispls ,
MPI_Datatype recvtype ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Alltoallv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.counts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.counts);
bcopy (counts, uop->data.mpi.counts, temp_umpi_comm_size * sizeof (int));
}
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.sdispls = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.sdispls);
bcopy (sdispls, uop->data.mpi.sdispls, temp_umpi_comm_size * sizeof (int));
}
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.recvcounts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.recvcounts);
bcopy (recvcounts, uop->data.mpi.recvcounts, temp_umpi_comm_size * sizeof (int));
}
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.rdispls = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.rdispls);
bcopy (rdispls, uop->data.mpi.rdispls, temp_umpi_comm_size * sizeof (int));
}
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Alltoallv_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Alltoallv_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	int comm_size;
	PMPI_Comm_size(op->data.mpi.comm, &comm_size);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, RECVTYPE, type_to_index(record_ptr, op->data.mpi.recvtype), my_rank);
	addVectorValue(event, COUNTS, comm_size, op->data.mpi.counts, my_rank);
	addVectorValue(event, DISPLS, comm_size, op->data.mpi.sdispls, my_rank);
	addVectorValue(event, RECVCOUNTS, comm_size, op->data.mpi.recvcounts, my_rank);
	addVectorValue(event, RDISPLS, comm_size, op->data.mpi.rdispls, my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Alltoallv\n");
 }
 #endif 
 #ifdef ONLINE_CLUSTERING
   #ifdef AUTOMATIC_MARKER
   #ifdef ALLTOALLV_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_Alltoallv) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Alltoallv_post( 
int MPI_rc, 
void * pc , const void *buf ,
const int *counts ,
const int *sdispls ,
MPI_Datatype datatype ,
void *recvbuf ,
const int *recvcounts ,
const int *rdispls ,
MPI_Datatype recvtype ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Alltoallv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.counts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.counts);
bcopy (counts, uop->data.mpi.counts, temp_umpi_comm_size * sizeof (int));
}
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.sdispls = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.sdispls);
bcopy (sdispls, uop->data.mpi.sdispls, temp_umpi_comm_size * sizeof (int));
}
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.recvcounts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.recvcounts);
bcopy (recvcounts, uop->data.mpi.recvcounts, temp_umpi_comm_size * sizeof (int));
}
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.rdispls = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.rdispls);
bcopy (rdispls, uop->data.mpi.rdispls, temp_umpi_comm_size * sizeof (int));
}
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Alltoallv_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Alltoallv( 
void * pc , const void *buf ,
const int *counts ,
const int *sdispls ,
MPI_Datatype datatype ,
void *recvbuf ,
const int *recvcounts ,
const int *rdispls ,
MPI_Datatype recvtype ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Alltoallv(  (buf),
(counts),
(sdispls),
(datatype),
(recvbuf),
(recvcounts),
(rdispls),
(recvtype),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Alltoallv_pre(pc,
			buf,
			counts,
			sdispls,
			datatype,
			recvbuf,
			recvcounts,
			rdispls,
			recvtype,
			comm);
	rc = PMPI_Alltoallv(  (buf),
(counts),
(sdispls),
(datatype),
(recvbuf),
(recvcounts),
(rdispls),
(recvtype),
(comm));

umpi_mpi_MPI_Alltoallv_post(rc, pc,
			buf,
			counts,
			sdispls,
			datatype,
			recvbuf,
			recvcounts,
			rdispls,
			recvtype,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Alltoallv(  const void *buf ,
		const int *counts ,
		const int *sdispls ,
		MPI_Datatype datatype ,
		void *recvbuf ,
		const int *recvcounts ,
		const int *rdispls ,
		MPI_Datatype recvtype ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Alltoallv(pc,
			buf,
			counts,
			sdispls,
			datatype,
			recvbuf,
			recvcounts,
			rdispls,
			recvtype,
			comm );
	return rc;
}

static void mpi_alltoallv_f_wrap(MPI_Fint *buf , MPI_Fint *counts , MPI_Fint *sdispls , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *rdispls , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Alltoallv(pc ,
			buf,
			counts,
			sdispls,
			(MPI_Datatype)(*(datatype)),
			recvbuf,
			recvcounts,
			rdispls,
			(MPI_Datatype)(*(recvtype)),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Alltoallv(pc ,
			buf,
			counts,
			sdispls,
			MPI_Type_f2c(*(datatype)),
			recvbuf,
			recvcounts,
			rdispls,
			MPI_Type_f2c(*(recvtype)),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_ALLTOALLV(MPI_Fint *buf , MPI_Fint *counts , MPI_Fint *sdispls , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *rdispls , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_alltoallv_f_wrap(buf, counts, sdispls, datatype, recvbuf, recvcounts, rdispls, recvtype, comm, ierr); }
extern void mpi_alltoallv(MPI_Fint *buf , MPI_Fint *counts , MPI_Fint *sdispls , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *rdispls , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_alltoallv_f_wrap(buf, counts, sdispls, datatype, recvbuf, recvcounts, rdispls, recvtype, comm, ierr); }
extern void mpi_alltoallv_(MPI_Fint *buf , MPI_Fint *counts , MPI_Fint *sdispls , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *rdispls , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_alltoallv_f_wrap(buf, counts, sdispls, datatype, recvbuf, recvcounts, rdispls, recvtype, comm, ierr); }
extern void mpi_alltoallv__(MPI_Fint *buf , MPI_Fint *counts , MPI_Fint *sdispls , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *rdispls , MPI_Fint *recvtype , MPI_Fint *comm , MPI_Fint * ierr) { mpi_alltoallv_f_wrap(buf, counts, sdispls, datatype, recvbuf, recvcounts, rdispls, recvtype, comm, ierr); }

/*--------------------------------------------- MPI_Barrier */

static int
umpi_mpi_MPI_Barrier_immediate_local_pre(umpi_op_t *op)
{

{
{
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Barrier_pre( 
void * pc , MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Barrier;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Barrier_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Barrier_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Barrier\n");
 }
 #endif 
 #ifdef MARKER_VALUE 
 if (my_rank==0){
 	printf("#### Chameleon: Manual Marker Values => comm_to_index(record_ptr, op->data.mpi.comm): %d\n", comm_to_index(record_ptr, op->data.mpi.comm));
 }
 #endif
 #ifdef ONLINE_CLUSTERING
  #ifdef MANUAL_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Manual Marker(MPI_Barrier) ===============>\n");
	}
   #ifdef FORTRAN_MARKER
   if (comm_to_index(record_ptr, op->data.mpi.comm) == getMarkerCommValue(&trace))	
   #else
   if (comm_to_index(record_ptr, op->data.mpi.comm) == getMarkerCommValue(&trace))
   #endif
   {	
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Marker ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
  }
  #endif
  #ifdef AUTOMATIC_MARKER
  #ifndef MANUAL_MARKER
   #ifdef BARRIER_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_Barrier) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Barrier_post( 
int MPI_rc, 
void * pc , MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Barrier;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Barrier_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Barrier( 
void * pc , MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Barrier(  (comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Barrier_pre(pc,
			comm);
	rc = PMPI_Barrier(  (comm));

umpi_mpi_MPI_Barrier_post(rc, pc,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Barrier(  MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Barrier(pc,
			comm );
	return rc;
}

static void mpi_barrier_f_wrap(MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Barrier(pc ,
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Barrier(pc ,
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_BARRIER(MPI_Fint *comm , MPI_Fint * ierr) { mpi_barrier_f_wrap(comm, ierr); }
extern void mpi_barrier(MPI_Fint *comm , MPI_Fint * ierr) { mpi_barrier_f_wrap(comm, ierr); }
extern void mpi_barrier_(MPI_Fint *comm , MPI_Fint * ierr) { mpi_barrier_f_wrap(comm, ierr); }
extern void mpi_barrier__(MPI_Fint *comm , MPI_Fint * ierr) { mpi_barrier_f_wrap(comm, ierr); }

/*--------------------------------------------- MPI_Bcast */

static int
umpi_mpi_MPI_Bcast_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Bcast_pre( 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Bcast;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Bcast_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Bcast_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, RT, op->data.mpi.root, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Bcast\n");
 }
 #endif 
 #ifdef ONLINE_CLUSTERING
   #ifdef AUTOMATIC_MARKER
   #ifdef BCAST_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_Bcast) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Bcast_post( 
int MPI_rc, 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Bcast;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Bcast_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Bcast( 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int root ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Bcast(  (buf),
(count),
(datatype),
(root),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Bcast_pre(pc,
			buf,
			count,
			datatype,
			root,
			comm);
	rc = PMPI_Bcast(  (buf),
(count),
(datatype),
(root),
(comm));

umpi_mpi_MPI_Bcast_post(rc, pc,
			buf,
			count,
			datatype,
			root,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Bcast(  void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int root ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Bcast(pc,
			buf,
			count,
			datatype,
			root,
			comm );
	return rc;
}

static void mpi_bcast_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Bcast(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(root),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Bcast(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(root),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_BCAST(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_bcast_f_wrap(buf, count, datatype, root, comm, ierr); }
extern void mpi_bcast(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_bcast_f_wrap(buf, count, datatype, root, comm, ierr); }
extern void mpi_bcast_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_bcast_f_wrap(buf, count, datatype, root, comm, ierr); }
extern void mpi_bcast__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_bcast_f_wrap(buf, count, datatype, root, comm, ierr); }

/*--------------------------------------------- MPI_Bsend */

static int
umpi_mpi_MPI_Bsend_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Bsend_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Bsend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Bsend_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Bsend_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, DEST, ENCODE_DEST(op->data.mpi.dest), my_rank);
	addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Bsend_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Bsend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Bsend_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Bsend( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Bsend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Bsend_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm);
	rc = PMPI_Bsend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm));

umpi_mpi_MPI_Bsend_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Bsend(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Bsend(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm );
	return rc;
}

static void mpi_bsend_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Bsend(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Bsend(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_BSEND(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_bsend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_bsend(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_bsend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_bsend_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_bsend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_bsend__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_bsend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }

/*--------------------------------------------- MPI_Bsend_init */

static int
umpi_mpi_MPI_Bsend_init_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Bsend_init_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Bsend_init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Bsend_init_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Bsend_init_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  operation.fields = COUNT | DATATYPE | DEST | TAG | COMM;
  event_set_param_list(event, op->op, op->seq_num, COUNT | DATATYPE | DEST | TAG | COMM);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  operation.datatype = type_to_index(record_ptr, op->data.mpi.datatype);
  add_scalar_param_int(event, TYPE_DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype));
  operation.dest = ENCODE_DEST(op->data.mpi.dest);
  add_scalar_param_int(event, TYPE_DEST, ENCODE_DEST(op->data.mpi.dest));
  operation.tag = ENCODE_TAG ( op->data.mpi.tag );
  add_scalar_param_int(event, TYPE_TAG, ENCODE_TAG ( op->data.mpi.tag ));
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  add_request_entry (record_req, op->data.mpi.request);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Bsend_init_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Bsend_init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Bsend_init_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Bsend_init( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Bsend_init(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Bsend_init_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
	rc = PMPI_Bsend_init(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

umpi_mpi_MPI_Bsend_init_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Bsend_init(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Bsend_init(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request );
	return rc;
}

static void mpi_bsend_init_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Bsend_init(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Bsend_init(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_BSEND_INIT(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_bsend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_bsend_init(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_bsend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_bsend_init_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_bsend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_bsend_init__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_bsend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }

/*--------------------------------------------- MPI_Buffer_attach */

/*-------------------- Wrapper for MPI_Buffer_attach omitted */

/*--------------------------------------------- MPI_Buffer_detach */

/*-------------------- Wrapper for MPI_Buffer_detach omitted */

/*--------------------------------------------- MPI_Cancel */

static int
umpi_mpi_MPI_Cancel_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
  int req = 0;
  lookup_offset (record_req, op->data.mpi.request, &operation.request);
  lookup_offset (record_req, op->data.mpi.request, &req);
  add_scalar_param_int (event, TYPE_REQUEST, req);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Cancel_pre( 
void * pc , MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Cancel;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Cancel_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Cancel_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  operation.fields = REQUEST;
  event_set_param_list(event, op->op, op->seq_num, REQUEST);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  if (op->data.mpi.request == MPI_REQUEST_NULL)
  {
    int req = 0;
    event_get_param (event, TYPE_REQUEST, (void *)&req);
    reset_offset (record_req, req);
  }
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Cancel_post( 
int MPI_rc, 
void * pc , MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Cancel;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Cancel_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Cancel( 
void * pc , MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Cancel(  (request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Cancel_pre(pc,
			request);
	rc = PMPI_Cancel(  (request));

umpi_mpi_MPI_Cancel_post(rc, pc,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Cancel(  MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Cancel(pc,
			request );
	return rc;
}

static void mpi_cancel_f_wrap(MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Cancel(pc ,
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Cancel(pc ,
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_CANCEL(MPI_Fint *request , MPI_Fint * ierr) { mpi_cancel_f_wrap(request, ierr); }
extern void mpi_cancel(MPI_Fint *request , MPI_Fint * ierr) { mpi_cancel_f_wrap(request, ierr); }
extern void mpi_cancel_(MPI_Fint *request , MPI_Fint * ierr) { mpi_cancel_f_wrap(request, ierr); }
extern void mpi_cancel__(MPI_Fint *request , MPI_Fint * ierr) { mpi_cancel_f_wrap(request, ierr); }

/*--------------------------------------------- MPI_Cart_coords */

/*-------------------- Wrapper for MPI_Cart_coords omitted */

/*--------------------------------------------- MPI_Cart_create */

static int
umpi_mpi_MPI_Cart_create_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Cart_create_pre( 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_dims ,
const int *array_of_periods ,
int reorder ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Cart_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_dims = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_dims);
bcopy (array_of_dims, uop->data.mpi.array_of_dims, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_periods = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_periods);
bcopy (array_of_periods, uop->data.mpi.array_of_periods, uop->data.mpi.count * sizeof (int));
		uop->data.mpi.reorder = reorder;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Cart_create_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Cart_create_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	/*char dims[2*op->data.mpi.count+1], periods[2*op->data.mpi.count+1];
	arryint_to_arrychar(op->data.mpi.array_of_dims, op->data.mpi.count, dims);
	arryint_to_arrychar(op->data.mpi.array_of_periods, op->data.mpi.count, periods);
	addCharValue(event, ARRAY_OF_DIMS, dims, my_rank);
	addCharValue(event, ARRAY_OF_PERIODS, periods, my_rank);
*/
	addVectorValue(event, ARRAY_OF_DIMS, op->data.mpi.count, op->data.mpi.array_of_dims, my_rank);
	addVectorValue(event, ARRAY_OF_PERIODS, op->data.mpi.count, op->data.mpi.array_of_periods, my_rank);
	addScalarValue(event, REORDER, op->data.mpi.reorder, my_rank);
	add_comm_entry(record_ptr, op->data.mpi.comm_out);
	addScalarValue(event, COMM_OUT, comm_to_index(record_ptr, op->data.mpi.comm_out), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Cart_create_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_dims ,
const int *array_of_periods ,
int reorder ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Cart_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_dims = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_dims);
bcopy (array_of_dims, uop->data.mpi.array_of_dims, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_periods = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_periods);
bcopy (array_of_periods, uop->data.mpi.array_of_periods, uop->data.mpi.count * sizeof (int));
		uop->data.mpi.reorder = reorder;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Cart_create_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Cart_create( 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_dims ,
const int *array_of_periods ,
int reorder ,
MPI_Comm *comm_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Cart_create(  (comm),
(count),
(array_of_dims),
(array_of_periods),
(reorder),
(comm_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Cart_create_pre(pc,
			comm,
			count,
			array_of_dims,
			array_of_periods,
			reorder,
			comm_out);
	rc = PMPI_Cart_create(  (comm),
(count),
(array_of_dims),
(array_of_periods),
(reorder),
(comm_out));

umpi_mpi_MPI_Cart_create_post(rc, pc,
			comm,
			count,
			array_of_dims,
			array_of_periods,
			reorder,
			comm_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Cart_create(  MPI_Comm comm ,
		int count ,
		const int *array_of_dims ,
		const int *array_of_periods ,
		int reorder ,
		MPI_Comm *comm_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Cart_create(pc,
			comm,
			count,
			array_of_dims,
			array_of_periods,
			reorder,
			comm_out );
	return rc;
}

static void mpi_cart_create_f_wrap(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint *array_of_periods , MPI_Fint *reorder , MPI_Fint *comm_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Cart_create(pc ,
			(MPI_Comm)(*(comm)),
			*(count),
			array_of_dims,
			array_of_periods,
			*(reorder),
			(MPI_Comm *)comm_out);
#else /* other mpi's need conversions */
		MPI_Comm temp_comm_out;
		temp_comm_out = MPI_Comm_f2c(*(comm_out));
		rc = gwrap_MPI_Cart_create(pc ,
			MPI_Comm_f2c(*(comm)),
			*(count),
			array_of_dims,
			array_of_periods,
			*(reorder),
			&temp_comm_out);
		*(comm_out) = MPI_Comm_c2f(temp_comm_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_CART_CREATE(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint *array_of_periods , MPI_Fint *reorder , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_cart_create_f_wrap(comm, count, array_of_dims, array_of_periods, reorder, comm_out, ierr); }
extern void mpi_cart_create(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint *array_of_periods , MPI_Fint *reorder , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_cart_create_f_wrap(comm, count, array_of_dims, array_of_periods, reorder, comm_out, ierr); }
extern void mpi_cart_create_(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint *array_of_periods , MPI_Fint *reorder , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_cart_create_f_wrap(comm, count, array_of_dims, array_of_periods, reorder, comm_out, ierr); }
extern void mpi_cart_create__(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint *array_of_periods , MPI_Fint *reorder , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_cart_create_f_wrap(comm, count, array_of_dims, array_of_periods, reorder, comm_out, ierr); }

/*--------------------------------------------- MPI_Cart_get */

/*-------------------- Wrapper for MPI_Cart_get omitted */

/*--------------------------------------------- MPI_Cart_map */

static int
umpi_mpi_MPI_Cart_map_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Cart_map_pre( 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_dims ,
const int *array_of_periods ,
int *newrank )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Cart_map;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_dims = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_dims);
bcopy (array_of_dims, uop->data.mpi.array_of_dims, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_periods = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_periods);
bcopy (array_of_periods, uop->data.mpi.array_of_periods, uop->data.mpi.count * sizeof (int));
uop->data.mpi.newrank = *(newrank);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Cart_map_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Cart_map_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  operation.fields = COMM | COUNT | NEWRANK;
  event_set_param_list(event, op->op, op->seq_num, COMM | COUNT | NEWRANK);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.arrays = ARRAY_OF_DIMS | ARRAY_OF_PERIODS;
  // array_of_dims 
  operation.array1 = OP_ARRAY_ALLOC(op->data.mpi.count);
  operation.array_size1 = op->data.mpi.count;
  if(!operation.array1) {
    perror("malloc operation.array1");
    exit(0);
  }
  OP_ARRAY_COPY(operation.array1, op->data.mpi.array_of_dims, op->data.mpi.count);
  // array_of_periods 
  operation.array2 = OP_ARRAY_ALLOC(op->data.mpi.count);
  operation.array_size2 = op->data.mpi.count;
  if(!operation.array2) {
    perror("malloc operation.array2");
    exit(0);
  }
  OP_ARRAY_COPY(operation.array2, op->data.mpi.array_of_periods, op->data.mpi.count);
  add_vector_param(event, ARRAY_OF_DIMS, op->data.mpi.array_of_dims, op->data.mpi.count);
  add_vector_param(event, ARRAY_OF_PERIODS, op->data.mpi.array_of_periods, op->data.mpi.count);
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  operation.newrank = op->data.mpi.newrank;
  add_scalar_param_int(event, TYPE_NEWRANK, op->data.mpi.newrank);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Cart_map_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_dims ,
const int *array_of_periods ,
int *newrank )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Cart_map;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_dims = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_dims);
bcopy (array_of_dims, uop->data.mpi.array_of_dims, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_periods = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_periods);
bcopy (array_of_periods, uop->data.mpi.array_of_periods, uop->data.mpi.count * sizeof (int));
uop->data.mpi.newrank = *(newrank);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Cart_map_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Cart_map( 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_dims ,
const int *array_of_periods ,
int *newrank )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Cart_map(  (comm),
(count),
(array_of_dims),
(array_of_periods),
(newrank));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Cart_map_pre(pc,
			comm,
			count,
			array_of_dims,
			array_of_periods,
			newrank);
	rc = PMPI_Cart_map(  (comm),
(count),
(array_of_dims),
(array_of_periods),
(newrank));

umpi_mpi_MPI_Cart_map_post(rc, pc,
			comm,
			count,
			array_of_dims,
			array_of_periods,
			newrank);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Cart_map(  MPI_Comm comm ,
		int count ,
		const int *array_of_dims ,
		const int *array_of_periods ,
		int *newrank )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Cart_map(pc,
			comm,
			count,
			array_of_dims,
			array_of_periods,
			newrank );
	return rc;
}

static void mpi_cart_map_f_wrap(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint *array_of_periods , MPI_Fint *newrank , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Cart_map(pc ,
			(MPI_Comm)(*(comm)),
			*(count),
			array_of_dims,
			array_of_periods,
			newrank);
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Cart_map(pc ,
			MPI_Comm_f2c(*(comm)),
			*(count),
			array_of_dims,
			array_of_periods,
			newrank);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_CART_MAP(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint *array_of_periods , MPI_Fint *newrank , MPI_Fint * ierr) { mpi_cart_map_f_wrap(comm, count, array_of_dims, array_of_periods, newrank, ierr); }
extern void mpi_cart_map(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint *array_of_periods , MPI_Fint *newrank , MPI_Fint * ierr) { mpi_cart_map_f_wrap(comm, count, array_of_dims, array_of_periods, newrank, ierr); }
extern void mpi_cart_map_(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint *array_of_periods , MPI_Fint *newrank , MPI_Fint * ierr) { mpi_cart_map_f_wrap(comm, count, array_of_dims, array_of_periods, newrank, ierr); }
extern void mpi_cart_map__(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint *array_of_periods , MPI_Fint *newrank , MPI_Fint * ierr) { mpi_cart_map_f_wrap(comm, count, array_of_dims, array_of_periods, newrank, ierr); }

/*--------------------------------------------- MPI_Cart_rank */

/*-------------------- Wrapper for MPI_Cart_rank omitted */

/*--------------------------------------------- MPI_Cart_shift */

/*-------------------- Wrapper for MPI_Cart_shift omitted */

/*--------------------------------------------- MPI_Cart_sub */

static int
umpi_mpi_MPI_Cart_sub_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Cart_sub_pre( 
void * pc , MPI_Comm comm ,
const int *dimsp ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Cart_sub;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
{
PMPI_Cartdim_get (uop->data.mpi.comm, &uop->data.mpi.size);
uop->data.mpi.dimsp = (int *) malloc (uop->data.mpi.size * sizeof (int));
assert (uop->data.mpi.dimsp);
bcopy (dimsp, uop->data.mpi.dimsp, uop->data.mpi.size * sizeof (int));
}
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Cart_sub_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Cart_sub_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  operation.fields = COMM;
  event_set_param_list(event, op->op, op->seq_num, COMM);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.arrays = DIMSP;
  // recvcounts 
  operation.array1 = OP_ARRAY_ALLOC(op->data.mpi.size);
  operation.array_size1 = op->data.mpi.size;
  if(!operation.array1) {
    perror("malloc operation.array1");
    exit(0);
  }
  OP_ARRAY_COPY(operation.array1, op->data.mpi.dimsp, op->data.mpi.size);
  add_vector_param (event, TYPE_DIMSP, op->data.mpi.dimsp, op->data.mpi.size);
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  add_comm_entry (record_ptr, op->data.mpi.comm_out);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Cart_sub_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
const int *dimsp ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Cart_sub;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
{
PMPI_Cartdim_get (uop->data.mpi.comm, &uop->data.mpi.size);
uop->data.mpi.dimsp = (int *) malloc (uop->data.mpi.size * sizeof (int));
assert (uop->data.mpi.dimsp);
bcopy (dimsp, uop->data.mpi.dimsp, uop->data.mpi.size * sizeof (int));
}
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Cart_sub_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Cart_sub( 
void * pc , MPI_Comm comm ,
const int *dimsp ,
MPI_Comm *comm_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Cart_sub(  (comm),
(dimsp),
(comm_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Cart_sub_pre(pc,
			comm,
			dimsp,
			comm_out);
	rc = PMPI_Cart_sub(  (comm),
(dimsp),
(comm_out));

umpi_mpi_MPI_Cart_sub_post(rc, pc,
			comm,
			dimsp,
			comm_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Cart_sub(  MPI_Comm comm ,
		const int *dimsp ,
		MPI_Comm *comm_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Cart_sub(pc,
			comm,
			dimsp,
			comm_out );
	return rc;
}

static void mpi_cart_sub_f_wrap(MPI_Fint *comm , MPI_Fint *dimsp , MPI_Fint *comm_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Cart_sub(pc ,
			(MPI_Comm)(*(comm)),
			dimsp,
			(MPI_Comm *)comm_out);
#else /* other mpi's need conversions */
		MPI_Comm temp_comm_out;
		temp_comm_out = MPI_Comm_f2c(*(comm_out));
		rc = gwrap_MPI_Cart_sub(pc ,
			MPI_Comm_f2c(*(comm)),
			dimsp,
			&temp_comm_out);
		*(comm_out) = MPI_Comm_c2f(temp_comm_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_CART_SUB(MPI_Fint *comm , MPI_Fint *dimsp , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_cart_sub_f_wrap(comm, dimsp, comm_out, ierr); }
extern void mpi_cart_sub(MPI_Fint *comm , MPI_Fint *dimsp , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_cart_sub_f_wrap(comm, dimsp, comm_out, ierr); }
extern void mpi_cart_sub_(MPI_Fint *comm , MPI_Fint *dimsp , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_cart_sub_f_wrap(comm, dimsp, comm_out, ierr); }
extern void mpi_cart_sub__(MPI_Fint *comm , MPI_Fint *dimsp , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_cart_sub_f_wrap(comm, dimsp, comm_out, ierr); }

/*--------------------------------------------- MPI_Cartdim_get */

/*-------------------- Wrapper for MPI_Cartdim_get omitted */

/*--------------------------------------------- MPI_Comm_compare */

/*-------------------- Wrapper for MPI_Comm_compare omitted */

/*--------------------------------------------- MPI_Comm_create */

static int
umpi_mpi_MPI_Comm_create_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_create_pre( 
void * pc , MPI_Comm comm ,
MPI_Group group ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.group = group;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Comm_create_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Comm_create_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	addScalarValue(event, GROUP, group_to_index(record_ptr, op->data.mpi.group), my_rank);
	appendEvent(&trace, event);
	add_comm_entry(record_ptr, op->data.mpi.comm_out);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_create_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
MPI_Group group ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.group = group;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Comm_create_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Comm_create( 
void * pc , MPI_Comm comm ,
MPI_Group group ,
MPI_Comm *comm_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Comm_create(  (comm),
(group),
(comm_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Comm_create_pre(pc,
			comm,
			group,
			comm_out);
	rc = PMPI_Comm_create(  (comm),
(group),
(comm_out));

umpi_mpi_MPI_Comm_create_post(rc, pc,
			comm,
			group,
			comm_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Comm_create(  MPI_Comm comm ,
		MPI_Group group ,
		MPI_Comm *comm_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Comm_create(pc,
			comm,
			group,
			comm_out );
	return rc;
}

static void mpi_comm_create_f_wrap(MPI_Fint *comm , MPI_Fint *group , MPI_Fint *comm_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Comm_create(pc ,
			(MPI_Comm)(*(comm)),
			(MPI_Group)(*(group)),
			(MPI_Comm *)comm_out);
#else /* other mpi's need conversions */
		MPI_Comm temp_comm_out;
		temp_comm_out = MPI_Comm_f2c(*(comm_out));
		rc = gwrap_MPI_Comm_create(pc ,
			MPI_Comm_f2c(*(comm)),
			MPI_Group_f2c(*(group)),
			&temp_comm_out);
		*(comm_out) = MPI_Comm_c2f(temp_comm_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_COMM_CREATE(MPI_Fint *comm , MPI_Fint *group , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_create_f_wrap(comm, group, comm_out, ierr); }
extern void mpi_comm_create(MPI_Fint *comm , MPI_Fint *group , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_create_f_wrap(comm, group, comm_out, ierr); }
extern void mpi_comm_create_(MPI_Fint *comm , MPI_Fint *group , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_create_f_wrap(comm, group, comm_out, ierr); }
extern void mpi_comm_create__(MPI_Fint *comm , MPI_Fint *group , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_create_f_wrap(comm, group, comm_out, ierr); }

/*--------------------------------------------- MPI_Comm_dup */

static int
umpi_mpi_MPI_Comm_dup_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_dup_pre( 
void * pc , MPI_Comm comm ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_dup;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Comm_dup_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Comm_dup_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
	add_comm_entry(record_ptr, op->data.mpi.comm_out);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_dup_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_dup;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Comm_dup_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Comm_dup( 
void * pc , MPI_Comm comm ,
MPI_Comm *comm_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Comm_dup(  (comm),
(comm_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Comm_dup_pre(pc,
			comm,
			comm_out);
	rc = PMPI_Comm_dup(  (comm),
(comm_out));

umpi_mpi_MPI_Comm_dup_post(rc, pc,
			comm,
			comm_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Comm_dup(  MPI_Comm comm ,
		MPI_Comm *comm_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Comm_dup(pc,
			comm,
			comm_out );
	return rc;
}

static void mpi_comm_dup_f_wrap(MPI_Fint *comm , MPI_Fint *comm_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Comm_dup(pc ,
			(MPI_Comm)(*(comm)),
			(MPI_Comm *)comm_out);
#else /* other mpi's need conversions */
		MPI_Comm temp_comm_out;
		temp_comm_out = MPI_Comm_f2c(*(comm_out));
		rc = gwrap_MPI_Comm_dup(pc ,
			MPI_Comm_f2c(*(comm)),
			&temp_comm_out);
		*(comm_out) = MPI_Comm_c2f(temp_comm_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_COMM_DUP(MPI_Fint *comm , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_dup_f_wrap(comm, comm_out, ierr); }
extern void mpi_comm_dup(MPI_Fint *comm , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_dup_f_wrap(comm, comm_out, ierr); }
extern void mpi_comm_dup_(MPI_Fint *comm , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_dup_f_wrap(comm, comm_out, ierr); }
extern void mpi_comm_dup__(MPI_Fint *comm , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_dup_f_wrap(comm, comm_out, ierr); }

/*--------------------------------------------- MPI_Comm_free */

static int
umpi_mpi_MPI_Comm_free_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
  	remove_comm_entry (record_ptr, op->data.mpi.comm);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_free_pre( 
void * pc , MPI_Comm *comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.comm = *(comm);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Comm_free_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Comm_free_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_free_post( 
int MPI_rc, 
void * pc , MPI_Comm *comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.comm = *(comm);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Comm_free_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Comm_free( 
void * pc , MPI_Comm *comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Comm_free(  (comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Comm_free_pre(pc,
			comm);
	rc = PMPI_Comm_free(  (comm));

umpi_mpi_MPI_Comm_free_post(rc, pc,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Comm_free(  MPI_Comm *comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Comm_free(pc,
			comm );
	return rc;
}

static void mpi_comm_free_f_wrap(MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Comm_free(pc ,
			(MPI_Comm *)comm);
#else /* other mpi's need conversions */
		MPI_Comm temp_comm;
		temp_comm = MPI_Comm_f2c(*(comm));
		rc = gwrap_MPI_Comm_free(pc ,
			&temp_comm);
		*(comm) = MPI_Comm_c2f(temp_comm);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_COMM_FREE(MPI_Fint *comm , MPI_Fint * ierr) { mpi_comm_free_f_wrap(comm, ierr); }
extern void mpi_comm_free(MPI_Fint *comm , MPI_Fint * ierr) { mpi_comm_free_f_wrap(comm, ierr); }
extern void mpi_comm_free_(MPI_Fint *comm , MPI_Fint * ierr) { mpi_comm_free_f_wrap(comm, ierr); }
extern void mpi_comm_free__(MPI_Fint *comm , MPI_Fint * ierr) { mpi_comm_free_f_wrap(comm, ierr); }

/*--------------------------------------------- MPI_Comm_get_name */

/*-------------------- Wrapper for MPI_Comm_get_name omitted */

/*--------------------------------------------- MPI_Comm_group */

static int
umpi_mpi_MPI_Comm_group_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_group_pre( 
void * pc , MPI_Comm comm ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_group;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Comm_group_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Comm_group_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
	add_group_entry (record_ptr, op->data.mpi.group_out);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_group_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_group;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Comm_group_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Comm_group( 
void * pc , MPI_Comm comm ,
MPI_Group *group_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Comm_group(  (comm),
(group_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Comm_group_pre(pc,
			comm,
			group_out);
	rc = PMPI_Comm_group(  (comm),
(group_out));

umpi_mpi_MPI_Comm_group_post(rc, pc,
			comm,
			group_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Comm_group(  MPI_Comm comm ,
		MPI_Group *group_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Comm_group(pc,
			comm,
			group_out );
	return rc;
}

static void mpi_comm_group_f_wrap(MPI_Fint *comm , MPI_Fint *group_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Comm_group(pc ,
			(MPI_Comm)(*(comm)),
			(MPI_Group *)group_out);
#else /* other mpi's need conversions */
		MPI_Group temp_group_out;
		temp_group_out = MPI_Group_f2c(*(group_out));
		rc = gwrap_MPI_Comm_group(pc ,
			MPI_Comm_f2c(*(comm)),
			&temp_group_out);
		*(group_out) = MPI_Group_c2f(temp_group_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_COMM_GROUP(MPI_Fint *comm , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_comm_group_f_wrap(comm, group_out, ierr); }
extern void mpi_comm_group(MPI_Fint *comm , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_comm_group_f_wrap(comm, group_out, ierr); }
extern void mpi_comm_group_(MPI_Fint *comm , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_comm_group_f_wrap(comm, group_out, ierr); }
extern void mpi_comm_group__(MPI_Fint *comm , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_comm_group_f_wrap(comm, group_out, ierr); }

/*--------------------------------------------- MPI_Comm_rank */

/*-------------------- Wrapper for MPI_Comm_rank omitted */

/*--------------------------------------------- MPI_Comm_remote_group */

/*-------------------- Wrapper for MPI_Comm_remote_group omitted */

/*--------------------------------------------- MPI_Comm_remote_size */

/*-------------------- Wrapper for MPI_Comm_remote_size omitted */

/*--------------------------------------------- MPI_Comm_set_name */

/*-------------------- Wrapper for MPI_Comm_set_name omitted */

/*--------------------------------------------- MPI_Comm_size */

static int
umpi_mpi_MPI_Comm_size_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_size_pre( 
void * pc , MPI_Comm comm ,
int *size )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_size;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
uop->data.mpi.size = *(size);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Comm_size_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Comm_size_immediate_local_post(umpi_op_t *op)
{

{
{
 setMarkerCommValue(&trace, comm_to_index(record_ptr, op->data.mpi.comm));
 #ifdef MARKER_VALUE 
 if (my_rank==0){
 	printf("#### Chameleon: Manual Marker Values => comm_to_index(record_ptr, op->data.mpi.comm): %d\n", comm_to_index(record_ptr, op->data.mpi.comm));
 }
 #endif
/*
  operation.fields = NO_FIELDS;
  event_set_param_list(event, op->op, op->seq_num, NO_FIELDS);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_size_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
int *size )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_size;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
uop->data.mpi.size = *(size);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Comm_size_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Comm_size( 
void * pc , MPI_Comm comm ,
int *size )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Comm_size(  (comm),
(size));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Comm_size_pre(pc,
			comm,
			size);
	rc = PMPI_Comm_size(  (comm),
(size));

umpi_mpi_MPI_Comm_size_post(rc, pc,
			comm,
			size);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Comm_size(  MPI_Comm comm ,
		int *size )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Comm_size(pc,
			comm,
			size );
	return rc;
}

static void mpi_comm_size_f_wrap(MPI_Fint *comm , MPI_Fint *size , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Comm_size(pc ,
			(MPI_Comm)(*(comm)),
			size);
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Comm_size(pc ,
			MPI_Comm_f2c(*(comm)),
			size);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_COMM_SIZE(MPI_Fint *comm , MPI_Fint *size , MPI_Fint * ierr) { mpi_comm_size_f_wrap(comm, size, ierr); }
extern void mpi_comm_size(MPI_Fint *comm , MPI_Fint *size , MPI_Fint * ierr) { mpi_comm_size_f_wrap(comm, size, ierr); }
extern void mpi_comm_size_(MPI_Fint *comm , MPI_Fint *size , MPI_Fint * ierr) { mpi_comm_size_f_wrap(comm, size, ierr); }
extern void mpi_comm_size__(MPI_Fint *comm , MPI_Fint *size , MPI_Fint * ierr) { mpi_comm_size_f_wrap(comm, size, ierr); }

/*--------------------------------------------- MPI_Comm_split */

static int
umpi_mpi_MPI_Comm_split_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_split_pre( 
void * pc , MPI_Comm comm ,
int color ,
int key ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_split;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.color = color;
		uop->data.mpi.key = key;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Comm_split_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Comm_split_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	addScalarValue(event, COLOR, op->data.mpi.color, my_rank);
	addScalarValue(event, KEY, ENCODE_KEY(op->data.mpi.key), my_rank);
	appendEvent(&trace, event);
	add_comm_entry (record_ptr, op->data.mpi.comm_out);
	addScalarValue(event, COMM_OUT, comm_to_index(record_ptr, op->data.mpi.comm_out), my_rank);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Comm_split_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
int color ,
int key ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Comm_split;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.color = color;
		uop->data.mpi.key = key;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Comm_split_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Comm_split( 
void * pc , MPI_Comm comm ,
int color ,
int key ,
MPI_Comm *comm_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Comm_split(  (comm),
(color),
(key),
(comm_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Comm_split_pre(pc,
			comm,
			color,
			key,
			comm_out);
	rc = PMPI_Comm_split(  (comm),
(color),
(key),
(comm_out));

umpi_mpi_MPI_Comm_split_post(rc, pc,
			comm,
			color,
			key,
			comm_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Comm_split(  MPI_Comm comm ,
		int color ,
		int key ,
		MPI_Comm *comm_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Comm_split(pc,
			comm,
			color,
			key,
			comm_out );
	return rc;
}

static void mpi_comm_split_f_wrap(MPI_Fint *comm , MPI_Fint *color , MPI_Fint *key , MPI_Fint *comm_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Comm_split(pc ,
			(MPI_Comm)(*(comm)),
			*(color),
			*(key),
			(MPI_Comm *)comm_out);
#else /* other mpi's need conversions */
		MPI_Comm temp_comm_out;
		temp_comm_out = MPI_Comm_f2c(*(comm_out));
		rc = gwrap_MPI_Comm_split(pc ,
			MPI_Comm_f2c(*(comm)),
			*(color),
			*(key),
			&temp_comm_out);
		*(comm_out) = MPI_Comm_c2f(temp_comm_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_COMM_SPLIT(MPI_Fint *comm , MPI_Fint *color , MPI_Fint *key , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_split_f_wrap(comm, color, key, comm_out, ierr); }
extern void mpi_comm_split(MPI_Fint *comm , MPI_Fint *color , MPI_Fint *key , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_split_f_wrap(comm, color, key, comm_out, ierr); }
extern void mpi_comm_split_(MPI_Fint *comm , MPI_Fint *color , MPI_Fint *key , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_split_f_wrap(comm, color, key, comm_out, ierr); }
extern void mpi_comm_split__(MPI_Fint *comm , MPI_Fint *color , MPI_Fint *key , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_comm_split_f_wrap(comm, color, key, comm_out, ierr); }

/*--------------------------------------------- MPI_Comm_test_inter */

/*-------------------- Wrapper for MPI_Comm_test_inter omitted */

/*--------------------------------------------- MPI_Dims_create */

static int
umpi_mpi_MPI_Dims_create_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Dims_create_pre( 
void * pc , int nnodes ,
int count ,
int *array_of_dims )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Dims_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.nnodes = nnodes;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_dims = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_dims);
bcopy (array_of_dims, uop->data.mpi.array_of_dims, uop->data.mpi.count * sizeof (int));
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Dims_create_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Dims_create_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  operation.fields_extn = COMM_SIZE | NDIMS;
  event_set_param_list(event, op->op, op->seq_num, COMM_SIZE | NDIMS);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.comm_size = op->data.mpi.nnodes;
  add_scalar_param_int(event, TYPE_COMM_SIZE, op->data.mpi.nnodes);
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_NDIMS, op->data.mpi.count);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Dims_create_post( 
int MPI_rc, 
void * pc , int nnodes ,
int count ,
int *array_of_dims )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Dims_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.nnodes = nnodes;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_dims = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_dims);
bcopy (array_of_dims, uop->data.mpi.array_of_dims, uop->data.mpi.count * sizeof (int));
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Dims_create_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Dims_create( 
void * pc , int nnodes ,
int count ,
int *array_of_dims )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Dims_create(  (nnodes),
(count),
(array_of_dims));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Dims_create_pre(pc,
			nnodes,
			count,
			array_of_dims);
	rc = PMPI_Dims_create(  (nnodes),
(count),
(array_of_dims));

umpi_mpi_MPI_Dims_create_post(rc, pc,
			nnodes,
			count,
			array_of_dims);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Dims_create(  int nnodes ,
		int count ,
		int *array_of_dims )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Dims_create(pc,
			nnodes,
			count,
			array_of_dims );
	return rc;
}

static void mpi_dims_create_f_wrap(MPI_Fint *nnodes , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
		rc = gwrap_MPI_Dims_create(pc ,
			*(nnodes),
			*(count),
			array_of_dims);
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_DIMS_CREATE(MPI_Fint *nnodes , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint * ierr) { mpi_dims_create_f_wrap(nnodes, count, array_of_dims, ierr); }
extern void mpi_dims_create(MPI_Fint *nnodes , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint * ierr) { mpi_dims_create_f_wrap(nnodes, count, array_of_dims, ierr); }
extern void mpi_dims_create_(MPI_Fint *nnodes , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint * ierr) { mpi_dims_create_f_wrap(nnodes, count, array_of_dims, ierr); }
extern void mpi_dims_create__(MPI_Fint *nnodes , MPI_Fint *count , MPI_Fint *array_of_dims , MPI_Fint * ierr) { mpi_dims_create_f_wrap(nnodes, count, array_of_dims, ierr); }

/*--------------------------------------------- MPI_Errhandler_free */

static int
umpi_mpi_MPI_Errhandler_free_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Errhandler_free_pre( 
void * pc , MPI_Errhandler *errhandler )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Errhandler_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.errhandler = *(errhandler);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Errhandler_free_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Errhandler_free_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  operation.fields = NO_FIELDS;
  event_set_param_list(event, op->op, op->seq_num, NO_FIELDS);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  // probably should throw an error if we try to replay
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Errhandler_free_post( 
int MPI_rc, 
void * pc , MPI_Errhandler *errhandler )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Errhandler_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.errhandler = *(errhandler);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Errhandler_free_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Errhandler_free( 
void * pc , MPI_Errhandler *errhandler )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Errhandler_free(  (errhandler));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Errhandler_free_pre(pc,
			errhandler);
	rc = PMPI_Errhandler_free(  (errhandler));

umpi_mpi_MPI_Errhandler_free_post(rc, pc,
			errhandler);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Errhandler_free(  MPI_Errhandler *errhandler )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Errhandler_free(pc,
			errhandler );
	return rc;
}

static void mpi_errhandler_free_f_wrap(MPI_Fint *errhandler , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Errhandler_free(pc ,
			(MPI_Errhandler *)errhandler);
#else /* other mpi's need conversions */
		MPI_Errhandler temp_errhandler;
		temp_errhandler = MPI_Errhandler_f2c(*(errhandler));
		rc = gwrap_MPI_Errhandler_free(pc ,
			&temp_errhandler);
		*(errhandler) = MPI_Errhandler_c2f(temp_errhandler);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_ERRHANDLER_FREE(MPI_Fint *errhandler , MPI_Fint * ierr) { mpi_errhandler_free_f_wrap(errhandler, ierr); }
extern void mpi_errhandler_free(MPI_Fint *errhandler , MPI_Fint * ierr) { mpi_errhandler_free_f_wrap(errhandler, ierr); }
extern void mpi_errhandler_free_(MPI_Fint *errhandler , MPI_Fint * ierr) { mpi_errhandler_free_f_wrap(errhandler, ierr); }
extern void mpi_errhandler_free__(MPI_Fint *errhandler , MPI_Fint * ierr) { mpi_errhandler_free_f_wrap(errhandler, ierr); }

/*--------------------------------------------- MPI_Errhandler_get */

/*-------------------- Wrapper for MPI_Errhandler_get omitted */

/*--------------------------------------------- MPI_Error_class */

/*-------------------- Wrapper for MPI_Error_class omitted */

/*--------------------------------------------- MPI_Error_string */

/*-------------------- Wrapper for MPI_Error_string omitted */

/*--------------------------------------------- MPI_Finalize */

static int
umpi_mpi_MPI_Finalize_immediate_local_pre(umpi_op_t *op)
{

{
{
	PMPI_Barrier (MPI_COMM_WORLD);
	createEvent(&event, op->op, my_rank);
	gettimeofday(&IOend, NULL);
	long long iotime = (IOend.tv_sec-IObegin.tv_sec)*1000000+(IOend.tv_usec-IObegin.tv_usec);
	addScalarValue(event, TIME, iotime, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	appendEvent(&trace, event);
	finalize_ptr(record_ptr);
	finalize_req(record_req);
	start_flag = 0;
  #ifdef FEATURE_LOOP_LCS
    int dw = 0;
    while(dw);
	finalizePendingIterations(&trace); 
    cleanupLoops(&trace);
  #endif
	gettimeofday(&NPBend, NULL);
	if(my_rank == 0){
		printf("time=(%f)\n", (double)((NPBend.tv_sec-NPBbegin.tv_sec)*1000000+(NPBend.tv_usec-NPBbegin.tv_usec))/1000000);
	}
 #ifndef ONLINE_CLUSTERING
  #ifndef COMPRESS_CROSS_NODE
	char path[1024];
	sprintf(path, "trace_%d/%d", my_size, my_rank);
	outputTrace(&trace, path);
  #else
	int lchild = get_child(my_rank, LEFT);
	if(lchild < my_size) {
		recvTrace(&left_trace, lchild);
		mergeTrace(&trace, &left_trace); 
    }
	int rchild = get_child(my_rank, RIGHT);
	if(rchild < my_size) {
		recvTrace(&right_trace, rchild);
		mergeTrace(&trace, &right_trace); 
    }
	if(my_rank != 0) {
		int parent = get_parent(my_rank);
		sendTrace(&trace, parent);
	} else {
		char path[1024];
		sprintf(path, "trace_%d/%d", my_size, my_rank);
		outputTrace(&trace, path);
	}
  #endif
 #else
 	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		fprintf("#### Chameleon: Start MPI_Finalized - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	clock_t	t;
	if(my_rank==0)
		t = clock();
//	int CLUSTERING = 0;//matchSignatures(&trace, my_rank, my_size);
	long int NumberCalls=1;
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
	CLUSTERING = -4;
	if(my_rank==0)
		printf("\n#### Chameleon: <---------------------- MPI_Finalize ---------------------> \n"); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
/*	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
*/
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	//setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
	{
		if(CLUSTERING == -4)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			} 
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                              	mergeOnlineTrace(&trace, &OnlineTrace, -2);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
					//printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
					//printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
					//printf("<============= Receive is different from Node 0 ========>\n\n ");
	                                mergeOnlineTrace(&trace, &OnlineTrace, -2);
					//printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
					//printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
					//printf("My rank is *********: %d SENDING \n ", my_rank);
                			sendTrace(&trace, 0);
				}
			}
		}	
		//cleanup internal traces
			deleteTrace(&trace);
	}
	if (my_rank ==0){
		//printf("\n<---------------------- MPI_Finalize ---------------------> \n"); 
		char path[1024];
		//printf("before printing\n"); 
                sprintf(path, "trace_%d/Chameleon", my_size); // temp_rank);
       	        outputTrace(&OnlineTrace, path); 
                //printf("Done w/ printing\n"); 
	}
	if(my_rank==0){
		printClusteringCounters(&trace, my_rank);
		printf("#### Chameleon: Overall Execution Overhead:\n");
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
 #endif
/*
  char output_trace_name[4096];
  //PMPI_Barrier (MPI_COMM_WORLD);
  int world_size = get_my_size();
 #if defined (OUTPUT_TIMING)
  double start = all_start.tv_sec + all_start.tv_usec / 1e6;
 #endif
 #ifdef PROFILE_TIMING
  if (my_rank == 0)
    profile_timing ();
 #endif
  FILE *time_file;
  // This is the usual MPI instrumentation. 
  init_op (&operation);
  init_event(event);
  //recordEventTime(event, 0);
  operation.fields = NO_FIELDS;
  event_set_param_list(event, op->op, op->seq_num, NO_FIELDS);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  recordTime (&operation, 0);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
 #ifdef OUTPUT_TIMING
  if (my_rank == 0) {
    char filename[4096];
    sprintf(filename, "%s/apptimes", output_trace_dir);
    time_file = fopen(filename, "w");
    output_time(time_file, "APP");
  }
 #endif
  sprintf(output_trace_name, "%s/%d", output_trace_dir, my_rank);
  // From here on, we're done recording all routines.  Global finalization follows. 
  #if RESULTS_TASKLEVEL
    output_trace = fopen(output_trace_name, "w");
    if (!output_trace) {
      fprintf(stderr, "Error: Couldn't open %s for tracing: %s\n", 
	      output_trace_name, strerror(errno));
      exit(1);
    }
    output_rsd_queue(output_trace, &op_queue);
    fflush(output_trace);
    fclose(output_trace);
  #ifdef PROFILE_TIMING
  if (my_rank == 0)
    profile_timing (); // output 
  #endif
  #else // !RESULTS_TASKLEVEL 
    // fetch and merge left child's queue 
    int lchild = get_child(my_rank, LEFT);
    if(lchild < world_size) {
      rsd_queue child_queue;
      recv_rsd_queue(&child_queue, MPI_COMM_WORLD, lchild);
  #ifdef VERIFY_TRANSFER
      dump_queue_with_label(&op_queue, lchild, my_rank, "beforerecv");
      dump_queue_with_label(&child_queue, lchild, my_rank, "recv");
  #endif // VERIFY_TRANSFER 
      merge_queues(&op_queue, &child_queue, lchild); 
      //dump_queue_with_label(&op_queue, lchild, my_rank, "mergerecv");
      free_rsd_nodes(&child_queue);
    }
    // fetch and merge right child's queue. 
    int rchild = get_child(my_rank, RIGHT);
    if(rchild < world_size) {
      rsd_queue child_queue;
      recv_rsd_queue(&child_queue, MPI_COMM_WORLD, rchild);
  #ifdef VERIFY_TRANSFER
      dump_queue_with_label(&op_queue, rchild, my_rank, "beforerecv");
      dump_queue_with_label(&child_queue, rchild, my_rank, "recv");
  #endif // VERIFY_TRANSFER 
      merge_queues(&op_queue, &child_queue, rchild);
      //dump_queue_with_label(&op_queue, rchild, my_rank, "mergerecv");
      free_rsd_nodes(&child_queue);
    }
    // send to parent 
    if(my_rank != 0) {
      int parent = get_parent(my_rank);
  #ifdef VERIFY_TRANSFER
      dump_queue_with_label(&op_queue, my_rank, parent, "send");
  #endif // VERIFY_TRANSFER 
      send_rsd_queue(&op_queue, MPI_COMM_WORLD, parent);
    } else {
  #ifdef PROFILE_TIMING
      profile_timing (); // merge 
  #endif
      // node 0 dumps out results 
      output_trace = fopen(output_trace_name, "w");
      if (!output_trace) {
        fprintf(stderr, "Error: Couldn't open %s for tracing: %s\n", 
            output_trace_name, strerror(errno));
        exit(1);
      }
      output_rsd_queue(output_trace, &op_queue);
      fflush(output_trace);
      fclose(output_trace);
  #ifdef PROFILE_TIMING
      profile_timing (); // output 
  #endif
    }
  #endif // RESULTS_TASKLEVEL 
  #ifdef RESULTS_MEMUSAGE
    int size = 0;//rsd_queue_packed_size (&op_queue, MPI_COMM_WORLD);
  #endif
    //free_rsd_nodes(&op_queue);
 #ifdef OUTPUT_TIMING
    // output timing to a separate file. 
    if (my_rank == 0) {
      double now = output_time(time_file, "MERGE");
      double total_time = now - start;
      fprintf(time_file, "TOTAL:\t%g\n", total_time);
      fclose(time_file);
    }
 #endif
    finalize_req(record_req);
 #ifdef PROFILE_TIMING
    if (my_rank == 0) {
      char filename[4096];
      sprintf(filename, "%s/times", output_trace_dir);
      time_file = fopen(filename, "w");
      output_profile_timing (time_file);
      fclose (time_file);
    }
  #endif
    #if RESULTS_MEMUSAGE
    {
      int minsize,maxsize,sumsize;
      FILE * mem_file;
      PMPI_Reduce (&size, &minsize,1,MPI_INT,MPI_MIN,0,MPI_COMM_WORLD);
      PMPI_Reduce (&size, &maxsize,1,MPI_INT,MPI_MAX,0,MPI_COMM_WORLD);
      PMPI_Reduce (&size,&sumsize,1,MPI_INT,MPI_SUM,0,MPI_COMM_WORLD);
      if (my_rank==0) {
        char filename[4096];
        sprintf(filename, "%s/memory", output_trace_dir);
        mem_file = fopen(filename, "w");
        fprintf(mem_file, "min:%d\nmax:%d\navg:%f\nzero:%d\n",
            minsize, maxsize, ((double)sumsize)/((double) world_size), size);
        fclose (mem_file);
      }
    }
    #endif
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Finalize_pre( 
void * pc)

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Finalize;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Finalize_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
gwrap_MPI_Finalize( 
void * pc)
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Finalize( );

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Finalize_pre(pc);
	rc = PMPI_Finalize( );

#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Finalize( )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Finalize(pc );
	return rc;
}

static void mpi_finalize_f_wrap(MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
		rc = gwrap_MPI_Finalize(pc );
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FINALIZE(MPI_Fint * ierr) { mpi_finalize_f_wrap(ierr); }
extern void mpi_finalize(MPI_Fint * ierr) { mpi_finalize_f_wrap(ierr); }
extern void mpi_finalize_(MPI_Fint * ierr) { mpi_finalize_f_wrap(ierr); }
extern void mpi_finalize__(MPI_Fint * ierr) { mpi_finalize_f_wrap(ierr); }

/*--------------------------------------------- MPI_Finalized */

/*-------------------- Wrapper for MPI_Finalized omitted */

/*--------------------------------------------- MPI_Gather */

static int
umpi_mpi_MPI_Gather_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Gather_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Gather;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Gather_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Gather_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, RECVCOUNT, op->data.mpi.recvcount, my_rank);
	addScalarValue(event, RECVTYPE, type_to_index(record_ptr, op->data.mpi.recvtype), my_rank);
	addScalarValue(event, RT, op->data.mpi.root, my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Gather\n");
 }
 #endif 
 #ifdef ONLINE_CLUSTERING
   #ifdef AUTOMATIC_MARKER
   #ifdef GATHER_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_GATHER) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Gather_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Gather;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Gather_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Gather( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Gather(  (buf),
(count),
(datatype),
(recvbuf),
(recvcount),
(recvtype),
(root),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Gather_pre(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			root,
			comm);
	rc = PMPI_Gather(  (buf),
(count),
(datatype),
(recvbuf),
(recvcount),
(recvtype),
(root),
(comm));

umpi_mpi_MPI_Gather_post(rc, pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			root,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Gather(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		void *recvbuf ,
		int recvcount ,
		MPI_Datatype recvtype ,
		int root ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Gather(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			root,
			comm );
	return rc;
}

static void mpi_gather_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Gather(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			recvbuf,
			*(recvcount),
			(MPI_Datatype)(*(recvtype)),
			*(root),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Gather(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			recvbuf,
			*(recvcount),
			MPI_Type_f2c(*(recvtype)),
			*(root),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GATHER(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_gather_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }
extern void mpi_gather(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_gather_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }
extern void mpi_gather_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_gather_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }
extern void mpi_gather__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_gather_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }

/*--------------------------------------------- MPI_Gatherv */

static int
umpi_mpi_MPI_Gatherv_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Gatherv_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
const int *recvcounts ,
const int *displs ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Gatherv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
{
int temp_umpi_comm_size, temp_umpi_lrank;
/* recvcounts only significant at root... */
PMPI_Comm_rank (comm, &temp_umpi_lrank);
if (temp_umpi_lrank == root) {
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.recvcounts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.recvcounts);
bcopy (recvcounts, uop->data.mpi.recvcounts, temp_umpi_comm_size * sizeof (int));
}
else {
uop->data.mpi.recvcounts = NULL;
}
}
{
int temp_umpi_comm_size, temp_umpi_lrank;
/* displs only significant at root... */
PMPI_Comm_rank (comm, &temp_umpi_lrank);
if (temp_umpi_lrank == root) {
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.displs = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.displs);
bcopy (displs, uop->data.mpi.displs, temp_umpi_comm_size * sizeof (int));
}
else {
uop->data.mpi.displs = NULL;
}
}
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Gatherv_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Gatherv_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addVectorValue(event, RECVCOUNTS, op->data.mpi.size, op->data.mpi.recvcounts, my_rank);
	addVectorValue(event, DISPLS, op->data.mpi.size, op->data.mpi.displs, my_rank);
	addScalarValue(event, RECVTYPE, type_to_index(record_ptr, op->data.mpi.recvtype), my_rank);
	addScalarValue(event, RT, op->data.mpi.root, my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Gatherv_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
const int *recvcounts ,
const int *displs ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Gatherv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
{
int temp_umpi_comm_size, temp_umpi_lrank;
/* recvcounts only significant at root... */
PMPI_Comm_rank (comm, &temp_umpi_lrank);
if (temp_umpi_lrank == root) {
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.recvcounts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.recvcounts);
bcopy (recvcounts, uop->data.mpi.recvcounts, temp_umpi_comm_size * sizeof (int));
}
else {
uop->data.mpi.recvcounts = NULL;
}
}
{
int temp_umpi_comm_size, temp_umpi_lrank;
/* displs only significant at root... */
PMPI_Comm_rank (comm, &temp_umpi_lrank);
if (temp_umpi_lrank == root) {
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.displs = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.displs);
bcopy (displs, uop->data.mpi.displs, temp_umpi_comm_size * sizeof (int));
}
else {
uop->data.mpi.displs = NULL;
}
}
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Gatherv_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Gatherv( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
const int *recvcounts ,
const int *displs ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Gatherv(  (buf),
(count),
(datatype),
(recvbuf),
(recvcounts),
(displs),
(recvtype),
(root),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Gatherv_pre(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcounts,
			displs,
			recvtype,
			root,
			comm);
	rc = PMPI_Gatherv(  (buf),
(count),
(datatype),
(recvbuf),
(recvcounts),
(displs),
(recvtype),
(root),
(comm));

umpi_mpi_MPI_Gatherv_post(rc, pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcounts,
			displs,
			recvtype,
			root,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Gatherv(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		void *recvbuf ,
		const int *recvcounts ,
		const int *displs ,
		MPI_Datatype recvtype ,
		int root ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Gatherv(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcounts,
			displs,
			recvtype,
			root,
			comm );
	return rc;
}

static void mpi_gatherv_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *displs , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Gatherv(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			recvbuf,
			recvcounts,
			displs,
			(MPI_Datatype)(*(recvtype)),
			*(root),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Gatherv(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			recvbuf,
			recvcounts,
			displs,
			MPI_Type_f2c(*(recvtype)),
			*(root),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GATHERV(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *displs , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_gatherv_f_wrap(buf, count, datatype, recvbuf, recvcounts, displs, recvtype, root, comm, ierr); }
extern void mpi_gatherv(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *displs , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_gatherv_f_wrap(buf, count, datatype, recvbuf, recvcounts, displs, recvtype, root, comm, ierr); }
extern void mpi_gatherv_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *displs , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_gatherv_f_wrap(buf, count, datatype, recvbuf, recvcounts, displs, recvtype, root, comm, ierr); }
extern void mpi_gatherv__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *displs , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_gatherv_f_wrap(buf, count, datatype, recvbuf, recvcounts, displs, recvtype, root, comm, ierr); }

/*--------------------------------------------- MPI_Get_count */

/*-------------------- Wrapper for MPI_Get_count omitted */

/*--------------------------------------------- MPI_Get_elements */

/*-------------------- Wrapper for MPI_Get_elements omitted */

/*--------------------------------------------- MPI_Get_processor_name */

/*-------------------- Wrapper for MPI_Get_processor_name omitted */

/*--------------------------------------------- MPI_Get_version */

/*-------------------- Wrapper for MPI_Get_version omitted */

/*--------------------------------------------- MPI_Graph_create */

static int
umpi_mpi_MPI_Graph_create_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Graph_create_pre( 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_degrees ,
const int *edges ,
int reorder ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Graph_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_degrees = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_degrees);
bcopy (array_of_degrees, uop->data.mpi.array_of_degrees, uop->data.mpi.count * sizeof (int));
{
int temp_umpi_nedges = uop->data.mpi.array_of_degrees[uop->data.mpi.count-1];
uop->data.mpi.edges = (int *) malloc (temp_umpi_nedges * sizeof (int));
assert (uop->data.mpi.edges);
bcopy (edges, uop->data.mpi.edges, temp_umpi_nedges * sizeof (int));
}
		uop->data.mpi.reorder = reorder;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Graph_create_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Graph_create_immediate_local_post(umpi_op_t *op)
{

{
{
/*  
    operation.fields = COMM | COUNT | REORDER ;
  event_set_param_list(event, op->op, op->seq_num, COMM | COUNT | REORDER | ARRAY_OF_DEGREES | ARRAY_OF_EDGES);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.arrays = ARRAY_OF_DEGREES | ARRAY_OF_EDGES;
  // array_of_degrees 
  operation.array1 = OP_ARRAY_ALLOC(op->data.mpi.count);
  operation.array_size1 = op->data.mpi.count;
  if(!operation.array1) {
    perror("malloc operation.array1");
    exit(0);
  }
  OP_ARRAY_COPY(operation.array1, op->data.mpi.array_of_degrees, op->data.mpi.count);
  operation.array2 = OP_ARRAY_ALLOC(op->data.mpi.array_of_degrees[op->data.mpi.count - 1]);
  operation.array_size2 = op->data.mpi.array_of_degrees[op->data.mpi.count - 1];
  if(!operation.array2) {
    perror("malloc operation.array2");
    exit(0);
  }
  OP_ARRAY_COPY(operation.array2, op->data.mpi.edges, op->data.mpi.array_of_degrees[op->data.mpi.count - 1]);
  add_vector_param(event, TYPE_ARRAY_OF_DEGREES, op->data.mpi.array_of_degrees, op->data.mpi.count);
  add_vector_param(event, TYPE_ARRAY_OF_EDGES, op->data.mpi.edges, op->data.mpi.array_of_degrees[op->data.mpi.count - 1]);
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  operation.reorder = op->data.mpi.reorder;
  add_scalar_param_int(event, TYPE_REORDER, op->data.mpi.reorder);
  add_comm_entry (record_ptr, op->data.mpi.comm_out);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Graph_create_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_degrees ,
const int *edges ,
int reorder ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Graph_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_degrees = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_degrees);
bcopy (array_of_degrees, uop->data.mpi.array_of_degrees, uop->data.mpi.count * sizeof (int));
{
int temp_umpi_nedges = uop->data.mpi.array_of_degrees[uop->data.mpi.count-1];
uop->data.mpi.edges = (int *) malloc (temp_umpi_nedges * sizeof (int));
assert (uop->data.mpi.edges);
bcopy (edges, uop->data.mpi.edges, temp_umpi_nedges * sizeof (int));
}
		uop->data.mpi.reorder = reorder;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Graph_create_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Graph_create( 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_degrees ,
const int *edges ,
int reorder ,
MPI_Comm *comm_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Graph_create(  (comm),
(count),
(array_of_degrees),
(edges),
(reorder),
(comm_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Graph_create_pre(pc,
			comm,
			count,
			array_of_degrees,
			edges,
			reorder,
			comm_out);
	rc = PMPI_Graph_create(  (comm),
(count),
(array_of_degrees),
(edges),
(reorder),
(comm_out));

umpi_mpi_MPI_Graph_create_post(rc, pc,
			comm,
			count,
			array_of_degrees,
			edges,
			reorder,
			comm_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Graph_create(  MPI_Comm comm ,
		int count ,
		const int *array_of_degrees ,
		const int *edges ,
		int reorder ,
		MPI_Comm *comm_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Graph_create(pc,
			comm,
			count,
			array_of_degrees,
			edges,
			reorder,
			comm_out );
	return rc;
}

static void mpi_graph_create_f_wrap(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_degrees , MPI_Fint *edges , MPI_Fint *reorder , MPI_Fint *comm_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Graph_create(pc ,
			(MPI_Comm)(*(comm)),
			*(count),
			array_of_degrees,
			edges,
			*(reorder),
			(MPI_Comm *)comm_out);
#else /* other mpi's need conversions */
		MPI_Comm temp_comm_out;
		temp_comm_out = MPI_Comm_f2c(*(comm_out));
		rc = gwrap_MPI_Graph_create(pc ,
			MPI_Comm_f2c(*(comm)),
			*(count),
			array_of_degrees,
			edges,
			*(reorder),
			&temp_comm_out);
		*(comm_out) = MPI_Comm_c2f(temp_comm_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GRAPH_CREATE(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_degrees , MPI_Fint *edges , MPI_Fint *reorder , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_graph_create_f_wrap(comm, count, array_of_degrees, edges, reorder, comm_out, ierr); }
extern void mpi_graph_create(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_degrees , MPI_Fint *edges , MPI_Fint *reorder , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_graph_create_f_wrap(comm, count, array_of_degrees, edges, reorder, comm_out, ierr); }
extern void mpi_graph_create_(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_degrees , MPI_Fint *edges , MPI_Fint *reorder , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_graph_create_f_wrap(comm, count, array_of_degrees, edges, reorder, comm_out, ierr); }
extern void mpi_graph_create__(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_degrees , MPI_Fint *edges , MPI_Fint *reorder , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_graph_create_f_wrap(comm, count, array_of_degrees, edges, reorder, comm_out, ierr); }

/*--------------------------------------------- MPI_Graph_get */

/*-------------------- Wrapper for MPI_Graph_get omitted */

/*--------------------------------------------- MPI_Graph_map */

static int
umpi_mpi_MPI_Graph_map_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Graph_map_pre( 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_degrees ,
const int *edges ,
int *newrank )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Graph_map;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_degrees = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_degrees);
bcopy (array_of_degrees, uop->data.mpi.array_of_degrees, uop->data.mpi.count * sizeof (int));
{
int temp_umpi_nedges = uop->data.mpi.array_of_degrees[uop->data.mpi.count-1];
uop->data.mpi.edges = (int *) malloc (temp_umpi_nedges * sizeof (int));
assert (uop->data.mpi.edges);
bcopy (edges, uop->data.mpi.edges, temp_umpi_nedges * sizeof (int));
}
uop->data.mpi.newrank = *(newrank);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Graph_map_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Graph_map_immediate_local_post(umpi_op_t *op)
{

{
{
  /*
    operation.fields = COMM | COUNT | NEWRANK;
  event_set_param_list(event, op->op, op->seq_num, COMM | COUNT | NEWRANK | ARRAY_OF_DEGREES | ARRAY_OF_EDGES);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.arrays = ARRAY_OF_DEGREES | ARRAY_OF_EDGES;
  operation.array1 = OP_ARRAY_ALLOC(op->data.mpi.count);
  operation.array_size1 = op->data.mpi.count;
  if(!operation.array1) {
    perror("malloc operation.array1");
    exit(0);
  }
  OP_ARRAY_COPY(operation.array1, op->data.mpi.array_of_degrees, op->data.mpi.count);
  operation.array2 = OP_ARRAY_ALLOC(op->data.mpi.array_of_degrees[op->data.mpi.count - 1]);
  operation.array_size2 = op->data.mpi.array_of_degrees[op->data.mpi.count - 1];
  if(!operation.array2) {
    perror("malloc operation.array2");
    exit(0);
  }
  OP_ARRAY_COPY(operation.array2, op->data.mpi.edges, op->data.mpi.array_of_degrees[op->data.mpi.count - 1]);
  add_vector_param(event, TYPE_ARRAY_OF_DEGREES, op->data.mpi.array_of_degrees, op->data.mpi.count);
  add_vector_param(event, TYPE_ARRAY_OF_EDGES, op->data.mpi.edges, op->data.mpi.array_of_degrees[op->data.mpi.count - 1]);
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  operation.newrank = op->data.mpi.newrank;
  add_scalar_param_int(event, TYPE_NEWRANK, op->data.mpi.newrank);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Graph_map_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_degrees ,
const int *edges ,
int *newrank )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Graph_map;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_degrees = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_degrees);
bcopy (array_of_degrees, uop->data.mpi.array_of_degrees, uop->data.mpi.count * sizeof (int));
{
int temp_umpi_nedges = uop->data.mpi.array_of_degrees[uop->data.mpi.count-1];
uop->data.mpi.edges = (int *) malloc (temp_umpi_nedges * sizeof (int));
assert (uop->data.mpi.edges);
bcopy (edges, uop->data.mpi.edges, temp_umpi_nedges * sizeof (int));
}
uop->data.mpi.newrank = *(newrank);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Graph_map_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Graph_map( 
void * pc , MPI_Comm comm ,
int count ,
const int *array_of_degrees ,
const int *edges ,
int *newrank )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Graph_map(  (comm),
(count),
(array_of_degrees),
(edges),
(newrank));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Graph_map_pre(pc,
			comm,
			count,
			array_of_degrees,
			edges,
			newrank);
	rc = PMPI_Graph_map(  (comm),
(count),
(array_of_degrees),
(edges),
(newrank));

umpi_mpi_MPI_Graph_map_post(rc, pc,
			comm,
			count,
			array_of_degrees,
			edges,
			newrank);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Graph_map(  MPI_Comm comm ,
		int count ,
		const int *array_of_degrees ,
		const int *edges ,
		int *newrank )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Graph_map(pc,
			comm,
			count,
			array_of_degrees,
			edges,
			newrank );
	return rc;
}

static void mpi_graph_map_f_wrap(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_degrees , MPI_Fint *edges , MPI_Fint *newrank , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Graph_map(pc ,
			(MPI_Comm)(*(comm)),
			*(count),
			array_of_degrees,
			edges,
			newrank);
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Graph_map(pc ,
			MPI_Comm_f2c(*(comm)),
			*(count),
			array_of_degrees,
			edges,
			newrank);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GRAPH_MAP(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_degrees , MPI_Fint *edges , MPI_Fint *newrank , MPI_Fint * ierr) { mpi_graph_map_f_wrap(comm, count, array_of_degrees, edges, newrank, ierr); }
extern void mpi_graph_map(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_degrees , MPI_Fint *edges , MPI_Fint *newrank , MPI_Fint * ierr) { mpi_graph_map_f_wrap(comm, count, array_of_degrees, edges, newrank, ierr); }
extern void mpi_graph_map_(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_degrees , MPI_Fint *edges , MPI_Fint *newrank , MPI_Fint * ierr) { mpi_graph_map_f_wrap(comm, count, array_of_degrees, edges, newrank, ierr); }
extern void mpi_graph_map__(MPI_Fint *comm , MPI_Fint *count , MPI_Fint *array_of_degrees , MPI_Fint *edges , MPI_Fint *newrank , MPI_Fint * ierr) { mpi_graph_map_f_wrap(comm, count, array_of_degrees, edges, newrank, ierr); }

/*--------------------------------------------- MPI_Graph_neighbors */

/*-------------------- Wrapper for MPI_Graph_neighbors omitted */

/*--------------------------------------------- MPI_Graph_neighbors_count */

/*-------------------- Wrapper for MPI_Graph_neighbors_count omitted */

/*--------------------------------------------- MPI_Graphdims_get */

/*-------------------- Wrapper for MPI_Graphdims_get omitted */

/*--------------------------------------------- MPI_Group_compare */

/*-------------------- Wrapper for MPI_Group_compare omitted */

/*--------------------------------------------- MPI_Group_difference */

static int
umpi_mpi_MPI_Group_difference_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_difference_pre( 
void * pc , MPI_Group group1 ,
MPI_Group group2 ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_difference;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group1 = group1;
		uop->data.mpi.group2 = group2;
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Group_difference_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Group_difference_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, GROUP1, group_to_index(record_ptr, op->data.mpi.group1), my_rank);
	addScalarValue(event, GROUP2, group_to_index(record_ptr, op->data.mpi.group2), my_rank);
	appendEvent(&trace, event);
	add_group_entry (record_ptr, op->data.mpi.group_out);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_difference_post( 
int MPI_rc, 
void * pc , MPI_Group group1 ,
MPI_Group group2 ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_difference;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group1 = group1;
		uop->data.mpi.group2 = group2;
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Group_difference_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Group_difference( 
void * pc , MPI_Group group1 ,
MPI_Group group2 ,
MPI_Group *group_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Group_difference(  (group1),
(group2),
(group_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Group_difference_pre(pc,
			group1,
			group2,
			group_out);
	rc = PMPI_Group_difference(  (group1),
(group2),
(group_out));

umpi_mpi_MPI_Group_difference_post(rc, pc,
			group1,
			group2,
			group_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Group_difference(  MPI_Group group1 ,
		MPI_Group group2 ,
		MPI_Group *group_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Group_difference(pc,
			group1,
			group2,
			group_out );
	return rc;
}

static void mpi_group_difference_f_wrap(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Group_difference(pc ,
			(MPI_Group)(*(group1)),
			(MPI_Group)(*(group2)),
			(MPI_Group *)group_out);
#else /* other mpi's need conversions */
		MPI_Group temp_group_out;
		temp_group_out = MPI_Group_f2c(*(group_out));
		rc = gwrap_MPI_Group_difference(pc ,
			MPI_Group_f2c(*(group1)),
			MPI_Group_f2c(*(group2)),
			&temp_group_out);
		*(group_out) = MPI_Group_c2f(temp_group_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GROUP_DIFFERENCE(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_difference_f_wrap(group1, group2, group_out, ierr); }
extern void mpi_group_difference(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_difference_f_wrap(group1, group2, group_out, ierr); }
extern void mpi_group_difference_(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_difference_f_wrap(group1, group2, group_out, ierr); }
extern void mpi_group_difference__(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_difference_f_wrap(group1, group2, group_out, ierr); }

/*--------------------------------------------- MPI_Group_excl */

static int
umpi_mpi_MPI_Group_excl_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_excl_pre( 
void * pc , MPI_Group group ,
int count ,
const int *array_of_ranks ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_excl;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group = group;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_ranks = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_ranks);
bcopy (array_of_ranks, uop->data.mpi.array_of_ranks, uop->data.mpi.count * sizeof (int));
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Group_excl_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Group_excl_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, GROUP, group_to_index(record_ptr, op->data.mpi.group), my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addVectorValue(event, ARRAY_OF_RANKS, op->data.mpi.count, op->data.mpi.array_of_ranks, my_rank);
	appendEvent(&trace, event);
	add_group_entry (record_ptr, op->data.mpi.group_out);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_excl_post( 
int MPI_rc, 
void * pc , MPI_Group group ,
int count ,
const int *array_of_ranks ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_excl;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group = group;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_ranks = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_ranks);
bcopy (array_of_ranks, uop->data.mpi.array_of_ranks, uop->data.mpi.count * sizeof (int));
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Group_excl_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Group_excl( 
void * pc , MPI_Group group ,
int count ,
const int *array_of_ranks ,
MPI_Group *group_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Group_excl(  (group),
(count),
(array_of_ranks),
(group_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Group_excl_pre(pc,
			group,
			count,
			array_of_ranks,
			group_out);
	rc = PMPI_Group_excl(  (group),
(count),
(array_of_ranks),
(group_out));

umpi_mpi_MPI_Group_excl_post(rc, pc,
			group,
			count,
			array_of_ranks,
			group_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Group_excl(  MPI_Group group ,
		int count ,
		const int *array_of_ranks ,
		MPI_Group *group_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Group_excl(pc,
			group,
			count,
			array_of_ranks,
			group_out );
	return rc;
}

static void mpi_group_excl_f_wrap(MPI_Fint *group , MPI_Fint *count , MPI_Fint *array_of_ranks , MPI_Fint *group_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Group_excl(pc ,
			(MPI_Group)(*(group)),
			*(count),
			array_of_ranks,
			(MPI_Group *)group_out);
#else /* other mpi's need conversions */
		MPI_Group temp_group_out;
		temp_group_out = MPI_Group_f2c(*(group_out));
		rc = gwrap_MPI_Group_excl(pc ,
			MPI_Group_f2c(*(group)),
			*(count),
			array_of_ranks,
			&temp_group_out);
		*(group_out) = MPI_Group_c2f(temp_group_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GROUP_EXCL(MPI_Fint *group , MPI_Fint *count , MPI_Fint *array_of_ranks , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_excl_f_wrap(group, count, array_of_ranks, group_out, ierr); }
extern void mpi_group_excl(MPI_Fint *group , MPI_Fint *count , MPI_Fint *array_of_ranks , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_excl_f_wrap(group, count, array_of_ranks, group_out, ierr); }
extern void mpi_group_excl_(MPI_Fint *group , MPI_Fint *count , MPI_Fint *array_of_ranks , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_excl_f_wrap(group, count, array_of_ranks, group_out, ierr); }
extern void mpi_group_excl__(MPI_Fint *group , MPI_Fint *count , MPI_Fint *array_of_ranks , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_excl_f_wrap(group, count, array_of_ranks, group_out, ierr); }

/*--------------------------------------------- MPI_Group_free */

static int
umpi_mpi_MPI_Group_free_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	addScalarValue(event, GROUP, group_to_index(record_ptr, op->data.mpi.group_out), my_rank);
	remove_group_entry (record_ptr, op->data.mpi.group_out);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_free_pre( 
void * pc , MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Group_free_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Group_free_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_free_post( 
int MPI_rc, 
void * pc , MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Group_free_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Group_free( 
void * pc , MPI_Group *group_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Group_free(  (group_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Group_free_pre(pc,
			group_out);
	rc = PMPI_Group_free(  (group_out));

umpi_mpi_MPI_Group_free_post(rc, pc,
			group_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Group_free(  MPI_Group *group_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Group_free(pc,
			group_out );
	return rc;
}

static void mpi_group_free_f_wrap(MPI_Fint *group_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Group_free(pc ,
			(MPI_Group *)group_out);
#else /* other mpi's need conversions */
		MPI_Group temp_group_out;
		temp_group_out = MPI_Group_f2c(*(group_out));
		rc = gwrap_MPI_Group_free(pc ,
			&temp_group_out);
		*(group_out) = MPI_Group_c2f(temp_group_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GROUP_FREE(MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_free_f_wrap(group_out, ierr); }
extern void mpi_group_free(MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_free_f_wrap(group_out, ierr); }
extern void mpi_group_free_(MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_free_f_wrap(group_out, ierr); }
extern void mpi_group_free__(MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_free_f_wrap(group_out, ierr); }

/*--------------------------------------------- MPI_Group_incl */

static int
umpi_mpi_MPI_Group_incl_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_incl_pre( 
void * pc , MPI_Group group ,
int count ,
const int *array_of_ranks ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_incl;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group = group;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_ranks = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_ranks);
bcopy (array_of_ranks, uop->data.mpi.array_of_ranks, uop->data.mpi.count * sizeof (int));
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Group_incl_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Group_incl_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, GROUP, group_to_index(record_ptr, op->data.mpi.group), my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addVectorValue(event, ARRAY_OF_RANKS, op->data.mpi.count, op->data.mpi.array_of_ranks, my_rank);
	appendEvent(&trace, event);
	add_group_entry (record_ptr, op->data.mpi.group_out);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_incl_post( 
int MPI_rc, 
void * pc , MPI_Group group ,
int count ,
const int *array_of_ranks ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_incl;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group = group;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_ranks = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_ranks);
bcopy (array_of_ranks, uop->data.mpi.array_of_ranks, uop->data.mpi.count * sizeof (int));
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Group_incl_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Group_incl( 
void * pc , MPI_Group group ,
int count ,
const int *array_of_ranks ,
MPI_Group *group_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Group_incl(  (group),
(count),
(array_of_ranks),
(group_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Group_incl_pre(pc,
			group,
			count,
			array_of_ranks,
			group_out);
	rc = PMPI_Group_incl(  (group),
(count),
(array_of_ranks),
(group_out));

umpi_mpi_MPI_Group_incl_post(rc, pc,
			group,
			count,
			array_of_ranks,
			group_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Group_incl(  MPI_Group group ,
		int count ,
		const int *array_of_ranks ,
		MPI_Group *group_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Group_incl(pc,
			group,
			count,
			array_of_ranks,
			group_out );
	return rc;
}

static void mpi_group_incl_f_wrap(MPI_Fint *group , MPI_Fint *count , MPI_Fint *array_of_ranks , MPI_Fint *group_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Group_incl(pc ,
			(MPI_Group)(*(group)),
			*(count),
			array_of_ranks,
			(MPI_Group *)group_out);
#else /* other mpi's need conversions */
		MPI_Group temp_group_out;
		temp_group_out = MPI_Group_f2c(*(group_out));
		rc = gwrap_MPI_Group_incl(pc ,
			MPI_Group_f2c(*(group)),
			*(count),
			array_of_ranks,
			&temp_group_out);
		*(group_out) = MPI_Group_c2f(temp_group_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GROUP_INCL(MPI_Fint *group , MPI_Fint *count , MPI_Fint *array_of_ranks , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_incl_f_wrap(group, count, array_of_ranks, group_out, ierr); }
extern void mpi_group_incl(MPI_Fint *group , MPI_Fint *count , MPI_Fint *array_of_ranks , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_incl_f_wrap(group, count, array_of_ranks, group_out, ierr); }
extern void mpi_group_incl_(MPI_Fint *group , MPI_Fint *count , MPI_Fint *array_of_ranks , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_incl_f_wrap(group, count, array_of_ranks, group_out, ierr); }
extern void mpi_group_incl__(MPI_Fint *group , MPI_Fint *count , MPI_Fint *array_of_ranks , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_incl_f_wrap(group, count, array_of_ranks, group_out, ierr); }

/*--------------------------------------------- MPI_Group_intersection */

static int
umpi_mpi_MPI_Group_intersection_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_intersection_pre( 
void * pc , MPI_Group group1 ,
MPI_Group group2 ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_intersection;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group1 = group1;
		uop->data.mpi.group2 = group2;
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Group_intersection_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Group_intersection_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, GROUP1, group_to_index(record_ptr, op->data.mpi.group1), my_rank);
	addScalarValue(event, GROUP2, group_to_index(record_ptr, op->data.mpi.group2), my_rank);
	appendEvent(&trace, event);
	add_group_entry (record_ptr, op->data.mpi.group_out);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_intersection_post( 
int MPI_rc, 
void * pc , MPI_Group group1 ,
MPI_Group group2 ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_intersection;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group1 = group1;
		uop->data.mpi.group2 = group2;
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Group_intersection_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Group_intersection( 
void * pc , MPI_Group group1 ,
MPI_Group group2 ,
MPI_Group *group_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Group_intersection(  (group1),
(group2),
(group_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Group_intersection_pre(pc,
			group1,
			group2,
			group_out);
	rc = PMPI_Group_intersection(  (group1),
(group2),
(group_out));

umpi_mpi_MPI_Group_intersection_post(rc, pc,
			group1,
			group2,
			group_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Group_intersection(  MPI_Group group1 ,
		MPI_Group group2 ,
		MPI_Group *group_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Group_intersection(pc,
			group1,
			group2,
			group_out );
	return rc;
}

static void mpi_group_intersection_f_wrap(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Group_intersection(pc ,
			(MPI_Group)(*(group1)),
			(MPI_Group)(*(group2)),
			(MPI_Group *)group_out);
#else /* other mpi's need conversions */
		MPI_Group temp_group_out;
		temp_group_out = MPI_Group_f2c(*(group_out));
		rc = gwrap_MPI_Group_intersection(pc ,
			MPI_Group_f2c(*(group1)),
			MPI_Group_f2c(*(group2)),
			&temp_group_out);
		*(group_out) = MPI_Group_c2f(temp_group_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GROUP_INTERSECTION(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_intersection_f_wrap(group1, group2, group_out, ierr); }
extern void mpi_group_intersection(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_intersection_f_wrap(group1, group2, group_out, ierr); }
extern void mpi_group_intersection_(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_intersection_f_wrap(group1, group2, group_out, ierr); }
extern void mpi_group_intersection__(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_intersection_f_wrap(group1, group2, group_out, ierr); }

/*--------------------------------------------- MPI_Group_range_excl */

static int
umpi_mpi_MPI_Group_range_excl_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_range_excl_pre( 
void * pc , MPI_Group group ,
int count ,
int ranges [][3],
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_range_excl;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group = group;
		uop->data.mpi.count = count;
uop->data.mpi.ranges = (int *) malloc (3 * uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.ranges);
bcopy (ranges, uop->data.mpi.ranges, 3 * uop->data.mpi.count * sizeof (int));
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Group_range_excl_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Group_range_excl_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, GROUP, group_to_index(record_ptr, op->data.mpi.group), my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addVectorValue(event, RANGES, op->data.mpi.count * 3, op->data.mpi.ranges, my_rank);
	appendEvent(&trace, event);
	add_group_entry (record_ptr, op->data.mpi.group_out);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_range_excl_post( 
int MPI_rc, 
void * pc , MPI_Group group ,
int count ,
int ranges [][3],
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_range_excl;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group = group;
		uop->data.mpi.count = count;
uop->data.mpi.ranges = (int *) malloc (3 * uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.ranges);
bcopy (ranges, uop->data.mpi.ranges, 3 * uop->data.mpi.count * sizeof (int));
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Group_range_excl_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Group_range_excl( 
void * pc , MPI_Group group ,
int count ,
int ranges [][3],
MPI_Group *group_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Group_range_excl(  (group),
(count),
(ranges),
(group_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Group_range_excl_pre(pc,
			group,
			count,
			ranges,
			group_out);
	rc = PMPI_Group_range_excl(  (group),
(count),
(ranges),
(group_out));

umpi_mpi_MPI_Group_range_excl_post(rc, pc,
			group,
			count,
			ranges,
			group_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Group_range_excl(  MPI_Group group ,
		int count ,
		int ranges [][3],
		MPI_Group *group_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Group_range_excl(pc,
			group,
			count,
			ranges,
			group_out );
	return rc;
}

static void mpi_group_range_excl_f_wrap(MPI_Fint *group , MPI_Fint *count , MPI_Fint ranges [][3], MPI_Fint *group_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Group_range_excl(pc ,
			(MPI_Group)(*(group)),
			*(count),
			ranges,
			(MPI_Group *)group_out);
#else /* other mpi's need conversions */
		MPI_Group temp_group_out;
		temp_group_out = MPI_Group_f2c(*(group_out));
		rc = gwrap_MPI_Group_range_excl(pc ,
			MPI_Group_f2c(*(group)),
			*(count),
			ranges,
			&temp_group_out);
		*(group_out) = MPI_Group_c2f(temp_group_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GROUP_RANGE_EXCL(MPI_Fint *group , MPI_Fint *count , MPI_Fint ranges [][3], MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_range_excl_f_wrap(group, count, ranges, group_out, ierr); }
extern void mpi_group_range_excl(MPI_Fint *group , MPI_Fint *count , MPI_Fint ranges [][3], MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_range_excl_f_wrap(group, count, ranges, group_out, ierr); }
extern void mpi_group_range_excl_(MPI_Fint *group , MPI_Fint *count , MPI_Fint ranges [][3], MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_range_excl_f_wrap(group, count, ranges, group_out, ierr); }
extern void mpi_group_range_excl__(MPI_Fint *group , MPI_Fint *count , MPI_Fint ranges [][3], MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_range_excl_f_wrap(group, count, ranges, group_out, ierr); }

/*--------------------------------------------- MPI_Group_range_incl */

static int
umpi_mpi_MPI_Group_range_incl_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_range_incl_pre( 
void * pc , MPI_Group group ,
int count ,
int ranges [][3],
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_range_incl;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group = group;
		uop->data.mpi.count = count;
uop->data.mpi.ranges = (int *) malloc (3 * uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.ranges);
bcopy (ranges, uop->data.mpi.ranges, 3 * uop->data.mpi.count * sizeof (int));
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Group_range_incl_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Group_range_incl_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, GROUP, group_to_index(record_ptr, op->data.mpi.group), my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addVectorValue(event, RANGES, op->data.mpi.count * 3, op->data.mpi.ranges, my_rank);
	appendEvent(&trace, event);
	add_group_entry (record_ptr, op->data.mpi.group_out);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_range_incl_post( 
int MPI_rc, 
void * pc , MPI_Group group ,
int count ,
int ranges [][3],
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_range_incl;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group = group;
		uop->data.mpi.count = count;
uop->data.mpi.ranges = (int *) malloc (3 * uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.ranges);
bcopy (ranges, uop->data.mpi.ranges, 3 * uop->data.mpi.count * sizeof (int));
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Group_range_incl_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Group_range_incl( 
void * pc , MPI_Group group ,
int count ,
int ranges [][3],
MPI_Group *group_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Group_range_incl(  (group),
(count),
(ranges),
(group_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Group_range_incl_pre(pc,
			group,
			count,
			ranges,
			group_out);
	rc = PMPI_Group_range_incl(  (group),
(count),
(ranges),
(group_out));

umpi_mpi_MPI_Group_range_incl_post(rc, pc,
			group,
			count,
			ranges,
			group_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Group_range_incl(  MPI_Group group ,
		int count ,
		int ranges [][3],
		MPI_Group *group_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Group_range_incl(pc,
			group,
			count,
			ranges,
			group_out );
	return rc;
}

static void mpi_group_range_incl_f_wrap(MPI_Fint *group , MPI_Fint *count , MPI_Fint ranges [][3], MPI_Fint *group_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Group_range_incl(pc ,
			(MPI_Group)(*(group)),
			*(count),
			ranges,
			(MPI_Group *)group_out);
#else /* other mpi's need conversions */
		MPI_Group temp_group_out;
		temp_group_out = MPI_Group_f2c(*(group_out));
		rc = gwrap_MPI_Group_range_incl(pc ,
			MPI_Group_f2c(*(group)),
			*(count),
			ranges,
			&temp_group_out);
		*(group_out) = MPI_Group_c2f(temp_group_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GROUP_RANGE_INCL(MPI_Fint *group , MPI_Fint *count , MPI_Fint ranges [][3], MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_range_incl_f_wrap(group, count, ranges, group_out, ierr); }
extern void mpi_group_range_incl(MPI_Fint *group , MPI_Fint *count , MPI_Fint ranges [][3], MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_range_incl_f_wrap(group, count, ranges, group_out, ierr); }
extern void mpi_group_range_incl_(MPI_Fint *group , MPI_Fint *count , MPI_Fint ranges [][3], MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_range_incl_f_wrap(group, count, ranges, group_out, ierr); }
extern void mpi_group_range_incl__(MPI_Fint *group , MPI_Fint *count , MPI_Fint ranges [][3], MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_range_incl_f_wrap(group, count, ranges, group_out, ierr); }

/*--------------------------------------------- MPI_Group_rank */

/*-------------------- Wrapper for MPI_Group_rank omitted */

/*--------------------------------------------- MPI_Group_size */

/*-------------------- Wrapper for MPI_Group_size omitted */

/*--------------------------------------------- MPI_Group_translate_ranks */

/*-------------------- Wrapper for MPI_Group_translate_ranks omitted */

/*--------------------------------------------- MPI_Group_union */

static int
umpi_mpi_MPI_Group_union_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_union_pre( 
void * pc , MPI_Group group1 ,
MPI_Group group2 ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_union;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group1 = group1;
		uop->data.mpi.group2 = group2;
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Group_union_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Group_union_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, GROUP1, group_to_index(record_ptr, op->data.mpi.group1), my_rank);
	addScalarValue(event, GROUP2, group_to_index(record_ptr, op->data.mpi.group2), my_rank);
	appendEvent(&trace, event);
	add_group_entry (record_ptr, op->data.mpi.group_out);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Group_union_post( 
int MPI_rc, 
void * pc , MPI_Group group1 ,
MPI_Group group2 ,
MPI_Group *group_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Group_union;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.group1 = group1;
		uop->data.mpi.group2 = group2;
uop->data.mpi.group_out = *(group_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Group_union_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Group_union( 
void * pc , MPI_Group group1 ,
MPI_Group group2 ,
MPI_Group *group_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Group_union(  (group1),
(group2),
(group_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Group_union_pre(pc,
			group1,
			group2,
			group_out);
	rc = PMPI_Group_union(  (group1),
(group2),
(group_out));

umpi_mpi_MPI_Group_union_post(rc, pc,
			group1,
			group2,
			group_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Group_union(  MPI_Group group1 ,
		MPI_Group group2 ,
		MPI_Group *group_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Group_union(pc,
			group1,
			group2,
			group_out );
	return rc;
}

static void mpi_group_union_f_wrap(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Group_union(pc ,
			(MPI_Group)(*(group1)),
			(MPI_Group)(*(group2)),
			(MPI_Group *)group_out);
#else /* other mpi's need conversions */
		MPI_Group temp_group_out;
		temp_group_out = MPI_Group_f2c(*(group_out));
		rc = gwrap_MPI_Group_union(pc ,
			MPI_Group_f2c(*(group1)),
			MPI_Group_f2c(*(group2)),
			&temp_group_out);
		*(group_out) = MPI_Group_c2f(temp_group_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_GROUP_UNION(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_union_f_wrap(group1, group2, group_out, ierr); }
extern void mpi_group_union(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_union_f_wrap(group1, group2, group_out, ierr); }
extern void mpi_group_union_(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_union_f_wrap(group1, group2, group_out, ierr); }
extern void mpi_group_union__(MPI_Fint *group1 , MPI_Fint *group2 , MPI_Fint *group_out , MPI_Fint * ierr) { mpi_group_union_f_wrap(group1, group2, group_out, ierr); }

/*--------------------------------------------- MPI_Handle2int */

/*-------------------- Wrapper for MPI_Handle2int omitted */

/*--------------------------------------------- MPI_Ibsend */

static int
umpi_mpi_MPI_Ibsend_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Ibsend_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Ibsend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Ibsend_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Ibsend_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
    addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
    addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
    addScalarValue(event, DEST, ENCODE_DEST(op->data.mpi.dest), my_rank);
    addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
    addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
    appendEvent(&trace, event);
  #ifndef FEATURE_PARAM_HISTO
    add_request_entry (record_req, op->data.mpi.request);
  #endif
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Ibsend_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Ibsend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Ibsend_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Ibsend( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Ibsend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Ibsend_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
	rc = PMPI_Ibsend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

umpi_mpi_MPI_Ibsend_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Ibsend(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Ibsend(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request );
	return rc;
}

static void mpi_ibsend_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Ibsend(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Ibsend(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_IBSEND(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_ibsend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_ibsend(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_ibsend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_ibsend_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_ibsend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_ibsend__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_ibsend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }

/*--------------------------------------------- MPI_Info_c2f */

/*-------------------- Wrapper for MPI_Info_c2f omitted */

/*--------------------------------------------- MPI_Info_create */

/*-------------------- Wrapper for MPI_Info_create omitted */

/*--------------------------------------------- MPI_Info_delete */

/*-------------------- Wrapper for MPI_Info_delete omitted */

/*--------------------------------------------- MPI_Info_dup */

/*-------------------- Wrapper for MPI_Info_dup omitted */

/*--------------------------------------------- MPI_Info_f2c */

/*-------------------- Wrapper for MPI_Info_f2c omitted */

/*--------------------------------------------- MPI_Info_free */

/*-------------------- Wrapper for MPI_Info_free omitted */

/*--------------------------------------------- MPI_Info_get */

/*-------------------- Wrapper for MPI_Info_get omitted */

/*--------------------------------------------- MPI_Info_get_nkeys */

/*-------------------- Wrapper for MPI_Info_get_nkeys omitted */

/*--------------------------------------------- MPI_Info_get_nthkey */

/*-------------------- Wrapper for MPI_Info_get_nthkey omitted */

/*--------------------------------------------- MPI_Info_get_valuelen */

/*-------------------- Wrapper for MPI_Info_get_valuelen omitted */

/*--------------------------------------------- MPI_Info_set */

/*-------------------- Wrapper for MPI_Info_set omitted */

/*--------------------------------------------- MPI_Init */

static int
umpi_mpi_MPI_Init_immediate_local_pre(umpi_op_t *op)
{

{
{
	gettimeofday(&NPBbegin, NULL);
}
}
return 0;
}


static int
umpi_mpi_MPI_Init_pre( 
void * pc , int *argc ,
char ***argv )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.argc = *(argc);
uop->data.mpi.argv = *(argv);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Init_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Init_immediate_local_post(umpi_op_t *op)
{

{
{
	PMPI_Comm_rank(MPI_COMM_WORLD, &my_rank); 
	PMPI_Comm_size(MPI_COMM_WORLD, &my_size);
	/*
	int dw=1;
	if(my_rank == 0)
		while(dw);
	*/
	char dir_name[32];
	sprintf(dir_name, "trace_%d", my_size);
	mkdir(dir_name, 0750);
	record_ptr = init_ptr (PTR_HANDLER_SZ);
	init_req(&record_req, REQ_BUF_SIZE);
	gettimeofday(&IObegin, NULL);
	createEvent(&event, op->op, my_rank);
	addScalarValue(event, GLOBAL_WORLD, comm_to_index(record_ptr, MPI_COMM_WORLD), my_rank);
	addScalarValue(event, NULL_REQ, 0, my_rank);
	appendEvent(&trace, event);
	PMPI_Barrier(MPI_COMM_WORLD);
	resetStats(headEvent(&trace), PHASE_COMP);
	start_flag = 1;
}
}
return 0;
}


static int
umpi_mpi_MPI_Init_post( 
int MPI_rc, 
void * pc , int *argc ,
char ***argv )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.argc = *(argc);
uop->data.mpi.argv = *(argv);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Init_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Init( 
void * pc , int *argc ,
char ***argv )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Init(  (argc),
(argv));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Init_pre(pc,
			argc,
			argv);
	rc = PMPI_Init(  (argc),
(argv));

umpi_mpi_MPI_Init_post(rc, pc,
			argc,
			argv);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Init(  int *argc ,
		char ***argv )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Init(pc,
			argc,
			argv );
	return rc;
}

static void mpi_init_f_wrap(MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	void *argc = (void *) ierr;
	void *argv = (void *) ierr;
	// DEFINE_PC;
	// GATHER_PC;
	{
		rc = gwrap_MPI_Init(pc ,
			argc,
			argv);
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_INIT(MPI_Fint * ierr) { mpi_init_f_wrap(ierr); }
extern void mpi_init(MPI_Fint * ierr) { mpi_init_f_wrap(ierr); }
extern void mpi_init_(MPI_Fint * ierr) { mpi_init_f_wrap(ierr); }
extern void mpi_init__(MPI_Fint * ierr) { mpi_init_f_wrap(ierr); }

/*--------------------------------------------- MPI_Initialized */

/*-------------------- Wrapper for MPI_Initialized omitted */

/*--------------------------------------------- MPI_Int2handle */

/*-------------------- Wrapper for MPI_Int2handle omitted */

/*--------------------------------------------- MPI_Intercomm_create */

static int
umpi_mpi_MPI_Intercomm_create_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Intercomm_create_pre( 
void * pc , MPI_Comm comm ,
int local_leader ,
MPI_Comm peer_comm ,
int remote_leader ,
int tag ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Intercomm_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.local_leader = local_leader;
		uop->data.mpi.peer_comm = peer_comm;
		uop->data.mpi.remote_leader = remote_leader;
		uop->data.mpi.tag = tag;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Intercomm_create_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Intercomm_create_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = COMM | LOCAL_LEADER | PEER_COMM | REMOTE_LEADER | TAG ;
  event_set_param_list(event, op->op, op->seq_num, COMM | LOCAL_LEADER | PEER_COMM | REMOTE_LEADER | TAG );
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  operation.local_leader = op->data.mpi.local_leader;
  add_scalar_param_int(event, TYPE_LOCAL_LEADER, op->data.mpi.local_leader);
  operation.peer_comm = comm_to_index (record_ptr, op->data.mpi.peer_comm);
  add_scalar_param_int(event, TYPE_PEER_COMM, comm_to_index (record_ptr, op->data.mpi.peer_comm));
  operation.remote_leader = op->data.mpi.remote_leader;
  add_scalar_param_int(event, TYPE_REMOTE_LEADER, op->data.mpi.remote_leader);
  operation.tag = op->data.mpi.tag;
  add_scalar_param_int(event, TYPE_TAG, op->data.mpi.tag);
  add_comm_entry (record_ptr, op->data.mpi.comm_out);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Intercomm_create_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
int local_leader ,
MPI_Comm peer_comm ,
int remote_leader ,
int tag ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Intercomm_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.local_leader = local_leader;
		uop->data.mpi.peer_comm = peer_comm;
		uop->data.mpi.remote_leader = remote_leader;
		uop->data.mpi.tag = tag;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Intercomm_create_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Intercomm_create( 
void * pc , MPI_Comm comm ,
int local_leader ,
MPI_Comm peer_comm ,
int remote_leader ,
int tag ,
MPI_Comm *comm_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Intercomm_create(  (comm),
(local_leader),
(peer_comm),
(remote_leader),
(tag),
(comm_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Intercomm_create_pre(pc,
			comm,
			local_leader,
			peer_comm,
			remote_leader,
			tag,
			comm_out);
	rc = PMPI_Intercomm_create(  (comm),
(local_leader),
(peer_comm),
(remote_leader),
(tag),
(comm_out));

umpi_mpi_MPI_Intercomm_create_post(rc, pc,
			comm,
			local_leader,
			peer_comm,
			remote_leader,
			tag,
			comm_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Intercomm_create(  MPI_Comm comm ,
		int local_leader ,
		MPI_Comm peer_comm ,
		int remote_leader ,
		int tag ,
		MPI_Comm *comm_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Intercomm_create(pc,
			comm,
			local_leader,
			peer_comm,
			remote_leader,
			tag,
			comm_out );
	return rc;
}

static void mpi_intercomm_create_f_wrap(MPI_Fint *comm , MPI_Fint *local_leader , MPI_Fint *peer_comm , MPI_Fint *remote_leader , MPI_Fint *tag , MPI_Fint *comm_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Intercomm_create(pc ,
			(MPI_Comm)(*(comm)),
			*(local_leader),
			(MPI_Comm)(*(peer_comm)),
			*(remote_leader),
			*(tag),
			(MPI_Comm *)comm_out);
#else /* other mpi's need conversions */
		MPI_Comm temp_comm_out;
		temp_comm_out = MPI_Comm_f2c(*(comm_out));
		rc = gwrap_MPI_Intercomm_create(pc ,
			MPI_Comm_f2c(*(comm)),
			*(local_leader),
			MPI_Comm_f2c(*(peer_comm)),
			*(remote_leader),
			*(tag),
			&temp_comm_out);
		*(comm_out) = MPI_Comm_c2f(temp_comm_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_INTERCOMM_CREATE(MPI_Fint *comm , MPI_Fint *local_leader , MPI_Fint *peer_comm , MPI_Fint *remote_leader , MPI_Fint *tag , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_intercomm_create_f_wrap(comm, local_leader, peer_comm, remote_leader, tag, comm_out, ierr); }
extern void mpi_intercomm_create(MPI_Fint *comm , MPI_Fint *local_leader , MPI_Fint *peer_comm , MPI_Fint *remote_leader , MPI_Fint *tag , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_intercomm_create_f_wrap(comm, local_leader, peer_comm, remote_leader, tag, comm_out, ierr); }
extern void mpi_intercomm_create_(MPI_Fint *comm , MPI_Fint *local_leader , MPI_Fint *peer_comm , MPI_Fint *remote_leader , MPI_Fint *tag , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_intercomm_create_f_wrap(comm, local_leader, peer_comm, remote_leader, tag, comm_out, ierr); }
extern void mpi_intercomm_create__(MPI_Fint *comm , MPI_Fint *local_leader , MPI_Fint *peer_comm , MPI_Fint *remote_leader , MPI_Fint *tag , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_intercomm_create_f_wrap(comm, local_leader, peer_comm, remote_leader, tag, comm_out, ierr); }

/*--------------------------------------------- MPI_Intercomm_merge */

static int
umpi_mpi_MPI_Intercomm_merge_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Intercomm_merge_pre( 
void * pc , MPI_Comm comm ,
int high ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Intercomm_merge;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.high = high;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Intercomm_merge_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Intercomm_merge_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = COMM | HIGH;
  event_set_param_list(event, op->op, op->seq_num, COMM | HIGH);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  operation.high = op->data.mpi.high;
  add_scalar_param_int(event, TYPE_HIGH, op->data.mpi.high);
  add_comm_entry (record_ptr, op->data.mpi.comm_out);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Intercomm_merge_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
int high ,
MPI_Comm *comm_out )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Intercomm_merge;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
		uop->data.mpi.high = high;
uop->data.mpi.comm_out = *(comm_out);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Intercomm_merge_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Intercomm_merge( 
void * pc , MPI_Comm comm ,
int high ,
MPI_Comm *comm_out )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Intercomm_merge(  (comm),
(high),
(comm_out));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Intercomm_merge_pre(pc,
			comm,
			high,
			comm_out);
	rc = PMPI_Intercomm_merge(  (comm),
(high),
(comm_out));

umpi_mpi_MPI_Intercomm_merge_post(rc, pc,
			comm,
			high,
			comm_out);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Intercomm_merge(  MPI_Comm comm ,
		int high ,
		MPI_Comm *comm_out )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Intercomm_merge(pc,
			comm,
			high,
			comm_out );
	return rc;
}

static void mpi_intercomm_merge_f_wrap(MPI_Fint *comm , MPI_Fint *high , MPI_Fint *comm_out , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Intercomm_merge(pc ,
			(MPI_Comm)(*(comm)),
			*(high),
			(MPI_Comm *)comm_out);
#else /* other mpi's need conversions */
		MPI_Comm temp_comm_out;
		temp_comm_out = MPI_Comm_f2c(*(comm_out));
		rc = gwrap_MPI_Intercomm_merge(pc ,
			MPI_Comm_f2c(*(comm)),
			*(high),
			&temp_comm_out);
		*(comm_out) = MPI_Comm_c2f(temp_comm_out);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_INTERCOMM_MERGE(MPI_Fint *comm , MPI_Fint *high , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_intercomm_merge_f_wrap(comm, high, comm_out, ierr); }
extern void mpi_intercomm_merge(MPI_Fint *comm , MPI_Fint *high , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_intercomm_merge_f_wrap(comm, high, comm_out, ierr); }
extern void mpi_intercomm_merge_(MPI_Fint *comm , MPI_Fint *high , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_intercomm_merge_f_wrap(comm, high, comm_out, ierr); }
extern void mpi_intercomm_merge__(MPI_Fint *comm , MPI_Fint *high , MPI_Fint *comm_out , MPI_Fint * ierr) { mpi_intercomm_merge_f_wrap(comm, high, comm_out, ierr); }

/*--------------------------------------------- MPI_Iprobe */

/*-------------------- Wrapper for MPI_Iprobe omitted */

/*--------------------------------------------- MPI_Irecv */

static int
umpi_mpi_MPI_Irecv_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Irecv_pre( 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int source ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Irecv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.source = source;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Irecv_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Irecv_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
    addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
    addScalarValue(event, SOURCE, ENCODE_SOURCE(op->data.mpi.source), my_rank);
    addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
    addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
    appendEvent(&trace, event);
  #ifndef FEATURE_PARAM_HISTO
    add_request_entry (record_req, op->data.mpi.request);
  #endif
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Irecv_post( 
int MPI_rc, 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int source ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Irecv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.source = source;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Irecv_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Irecv( 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int source ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Irecv(  (buf),
(count),
(datatype),
(source),
(tag),
(comm),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Irecv_pre(pc,
			buf,
			count,
			datatype,
			source,
			tag,
			comm,
			request);
	rc = PMPI_Irecv(  (buf),
(count),
(datatype),
(source),
(tag),
(comm),
(request));

umpi_mpi_MPI_Irecv_post(rc, pc,
			buf,
			count,
			datatype,
			source,
			tag,
			comm,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Irecv(  void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int source ,
		int tag ,
		MPI_Comm comm ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Irecv(pc,
			buf,
			count,
			datatype,
			source,
			tag,
			comm,
			request );
	return rc;
}

static void mpi_irecv_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Irecv(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(source),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Irecv(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(source),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_IRECV(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_irecv_f_wrap(buf, count, datatype, source, tag, comm, request, ierr); }
extern void mpi_irecv(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_irecv_f_wrap(buf, count, datatype, source, tag, comm, request, ierr); }
extern void mpi_irecv_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_irecv_f_wrap(buf, count, datatype, source, tag, comm, request, ierr); }
extern void mpi_irecv__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_irecv_f_wrap(buf, count, datatype, source, tag, comm, request, ierr); }

/*--------------------------------------------- MPI_Irsend */

static int
umpi_mpi_MPI_Irsend_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Irsend_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Irsend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Irsend_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Irsend_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, DEST, ENCODE_DEST(op->data.mpi.dest), my_rank);
	addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
  #ifndef FEATURE_PARAM_HISTO
	add_request_entry (record_req, op->data.mpi.request);
  #endif
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Irsend_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Irsend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Irsend_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Irsend( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Irsend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Irsend_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
	rc = PMPI_Irsend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

umpi_mpi_MPI_Irsend_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Irsend(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Irsend(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request );
	return rc;
}

static void mpi_irsend_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Irsend(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Irsend(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_IRSEND(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_irsend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_irsend(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_irsend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_irsend_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_irsend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_irsend__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_irsend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }

/*--------------------------------------------- MPI_Isend */

static int
umpi_mpi_MPI_Isend_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Isend_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Isend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Isend_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Isend_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, DEST, ENCODE_DEST(op->data.mpi.dest), my_rank);
	addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
  #ifndef FEATURE_PARAM_HISTO
	add_request_entry (record_req, op->data.mpi.request);
  #endif
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Isend_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Isend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Isend_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Isend( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Isend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Isend_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
	rc = PMPI_Isend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

umpi_mpi_MPI_Isend_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Isend(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Isend(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request );
	return rc;
}

static void mpi_isend_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Isend(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Isend(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_ISEND(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_isend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_isend(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_isend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_isend_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_isend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_isend__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_isend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }

/*--------------------------------------------- MPI_Issend */

static int
umpi_mpi_MPI_Issend_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Issend_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Issend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Issend_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Issend_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, DEST, ENCODE_DEST(op->data.mpi.dest), my_rank);
	addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
  #ifndef FEATURE_PARAM_HISTO
	add_request_entry (record_req, op->data.mpi.request);
  #endif
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Issend_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Issend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Issend_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Issend( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Issend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Issend_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
	rc = PMPI_Issend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

umpi_mpi_MPI_Issend_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Issend(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Issend(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request );
	return rc;
}

static void mpi_issend_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Issend(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Issend(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_ISSEND(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_issend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_issend(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_issend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_issend_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_issend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_issend__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_issend_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }

/*--------------------------------------------- MPI_Keyval_create */

static int
umpi_mpi_MPI_Keyval_create_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Keyval_create_pre( 
void * pc , MPI_Copy_function *copy_function ,
MPI_Delete_function *delete_function ,
int *keyval ,
void *extra_state )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Keyval_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.copy_function = &(copy_function);
uop->data.mpi.delete_function = &(delete_function);
uop->data.mpi.keyval = *(keyval);
uop->data.mpi.extra_state = extra_state;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Keyval_create_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Keyval_create_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = NO_FIELDS;
  event_set_param_list(event, op->op, op->seq_num, NO_FIELDS);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  // probably should throw an error if we try to replay
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Keyval_create_post( 
int MPI_rc, 
void * pc , MPI_Copy_function *copy_function ,
MPI_Delete_function *delete_function ,
int *keyval ,
void *extra_state )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Keyval_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.copy_function = &(copy_function);
uop->data.mpi.delete_function = &(delete_function);
uop->data.mpi.keyval = *(keyval);
uop->data.mpi.extra_state = extra_state;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Keyval_create_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Keyval_create( 
void * pc , MPI_Copy_function *copy_function ,
MPI_Delete_function *delete_function ,
int *keyval ,
void *extra_state )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Keyval_create(  (copy_function),
(delete_function),
(keyval),
(extra_state));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Keyval_create_pre(pc,
			copy_function,
			delete_function,
			keyval,
			extra_state);
	rc = PMPI_Keyval_create(  (copy_function),
(delete_function),
(keyval),
(extra_state));

umpi_mpi_MPI_Keyval_create_post(rc, pc,
			copy_function,
			delete_function,
			keyval,
			extra_state);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Keyval_create(  MPI_Copy_function *copy_function ,
		MPI_Delete_function *delete_function ,
		int *keyval ,
		void *extra_state )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Keyval_create(pc,
			copy_function,
			delete_function,
			keyval,
			extra_state );
	return rc;
}

static void mpi_keyval_create_f_wrap(MPI_Copy_function *copy_function , MPI_Delete_function *delete_function , MPI_Fint *keyval , MPI_Fint *extra_state , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
		rc = gwrap_MPI_Keyval_create(pc ,
			copy_function,
			delete_function,
			keyval,
			extra_state);
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_KEYVAL_CREATE(MPI_Copy_function *copy_function , MPI_Delete_function *delete_function , MPI_Fint *keyval , MPI_Fint *extra_state , MPI_Fint * ierr) { mpi_keyval_create_f_wrap(copy_function, delete_function, keyval, extra_state, ierr); }
extern void mpi_keyval_create(MPI_Copy_function *copy_function , MPI_Delete_function *delete_function , MPI_Fint *keyval , MPI_Fint *extra_state , MPI_Fint * ierr) { mpi_keyval_create_f_wrap(copy_function, delete_function, keyval, extra_state, ierr); }
extern void mpi_keyval_create_(MPI_Copy_function *copy_function , MPI_Delete_function *delete_function , MPI_Fint *keyval , MPI_Fint *extra_state , MPI_Fint * ierr) { mpi_keyval_create_f_wrap(copy_function, delete_function, keyval, extra_state, ierr); }
extern void mpi_keyval_create__(MPI_Copy_function *copy_function , MPI_Delete_function *delete_function , MPI_Fint *keyval , MPI_Fint *extra_state , MPI_Fint * ierr) { mpi_keyval_create_f_wrap(copy_function, delete_function, keyval, extra_state, ierr); }

/*--------------------------------------------- MPI_Op_create */

static int
umpi_mpi_MPI_Op_create_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Op_create_pre( 
void * pc , MPI_User_function *function ,
int commute ,
MPI_Op *op_ptr )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Op_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.function = &(function);
		uop->data.mpi.commute = commute;
uop->data.mpi.op = *(op_ptr);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Op_create_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Op_create_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = NO_FIELDS;
  event_set_param_list(event, op->op, op->seq_num, NO_FIELDS);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  // probably should throw an error if we try to replay
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Op_create_post( 
int MPI_rc, 
void * pc , MPI_User_function *function ,
int commute ,
MPI_Op *op_ptr )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Op_create;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.function = &(function);
		uop->data.mpi.commute = commute;
uop->data.mpi.op = *(op_ptr);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Op_create_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Op_create( 
void * pc , MPI_User_function *function ,
int commute ,
MPI_Op *op_ptr )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Op_create(  (function),
(commute),
(op_ptr));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Op_create_pre(pc,
			function,
			commute,
			op_ptr);
	rc = PMPI_Op_create(  (function),
(commute),
(op_ptr));

umpi_mpi_MPI_Op_create_post(rc, pc,
			function,
			commute,
			op_ptr);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Op_create(  MPI_User_function *function ,
		int commute ,
		MPI_Op *op_ptr )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Op_create(pc,
			function,
			commute,
			op_ptr );
	return rc;
}

static void mpi_op_create_f_wrap(MPI_User_function *function , MPI_Fint *commute , MPI_Fint *op_ptr , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Op_create(pc ,
			function,
			*(commute),
			(MPI_Op *)op_ptr);
#else /* other mpi's need conversions */
		MPI_Op temp_op_ptr;
		temp_op_ptr = MPI_Op_f2c(*(op_ptr));
		rc = gwrap_MPI_Op_create(pc ,
			function,
			*(commute),
			&temp_op_ptr);
		*(op_ptr) = MPI_Op_c2f(temp_op_ptr);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_OP_CREATE(MPI_User_function *function , MPI_Fint *commute , MPI_Fint *op_ptr , MPI_Fint * ierr) { mpi_op_create_f_wrap(function, commute, op_ptr, ierr); }
extern void mpi_op_create(MPI_User_function *function , MPI_Fint *commute , MPI_Fint *op_ptr , MPI_Fint * ierr) { mpi_op_create_f_wrap(function, commute, op_ptr, ierr); }
extern void mpi_op_create_(MPI_User_function *function , MPI_Fint *commute , MPI_Fint *op_ptr , MPI_Fint * ierr) { mpi_op_create_f_wrap(function, commute, op_ptr, ierr); }
extern void mpi_op_create__(MPI_User_function *function , MPI_Fint *commute , MPI_Fint *op_ptr , MPI_Fint * ierr) { mpi_op_create_f_wrap(function, commute, op_ptr, ierr); }

/*--------------------------------------------- MPI_Op_free */

static int
umpi_mpi_MPI_Op_free_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Op_free_pre( 
void * pc , MPI_Op *op_ptr )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Op_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.op = *(op_ptr);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Op_free_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Op_free_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = NO_FIELDS;
  event_set_param_list(event, op->op, op->seq_num, NO_FIELDS);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  // probably should throw an error if we try to replay
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Op_free_post( 
int MPI_rc, 
void * pc , MPI_Op *op_ptr )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Op_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.op = *(op_ptr);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Op_free_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Op_free( 
void * pc , MPI_Op *op_ptr )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Op_free(  (op_ptr));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Op_free_pre(pc,
			op_ptr);
	rc = PMPI_Op_free(  (op_ptr));

umpi_mpi_MPI_Op_free_post(rc, pc,
			op_ptr);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Op_free(  MPI_Op *op_ptr )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Op_free(pc,
			op_ptr );
	return rc;
}

static void mpi_op_free_f_wrap(MPI_Fint *op_ptr , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Op_free(pc ,
			(MPI_Op *)op_ptr);
#else /* other mpi's need conversions */
		MPI_Op temp_op_ptr;
		temp_op_ptr = MPI_Op_f2c(*(op_ptr));
		rc = gwrap_MPI_Op_free(pc ,
			&temp_op_ptr);
		*(op_ptr) = MPI_Op_c2f(temp_op_ptr);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_OP_FREE(MPI_Fint *op_ptr , MPI_Fint * ierr) { mpi_op_free_f_wrap(op_ptr, ierr); }
extern void mpi_op_free(MPI_Fint *op_ptr , MPI_Fint * ierr) { mpi_op_free_f_wrap(op_ptr, ierr); }
extern void mpi_op_free_(MPI_Fint *op_ptr , MPI_Fint * ierr) { mpi_op_free_f_wrap(op_ptr, ierr); }
extern void mpi_op_free__(MPI_Fint *op_ptr , MPI_Fint * ierr) { mpi_op_free_f_wrap(op_ptr, ierr); }

/*--------------------------------------------- MPI_Pack */

/*-------------------- Wrapper for MPI_Pack omitted */

/*--------------------------------------------- MPI_Pack_size */

/*-------------------- Wrapper for MPI_Pack_size omitted */

/*--------------------------------------------- MPI_Pcontrol */

static int
umpi_mpi_MPI_Pcontrol_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op(&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime(&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Pcontrol_pre( 
void * pc , const int level ,
... )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Pcontrol;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->profile_level = level;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Pcontrol_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Pcontrol_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  operation.fields = NO_FIELDS; 
  event_set_param_list(event, op->op, op->seq_num, NO_FIELDS );
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  recordTime(&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
  recordTime(0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Pcontrol_post( 
int MPI_rc, 
void * pc , const int level ,
... )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Pcontrol;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->profile_level = level;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Pcontrol_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Pcontrol( 
void * pc , const int level ,
... )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Pcontrol(  (level));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Pcontrol_pre(pc,
			level);
	rc = PMPI_Pcontrol(  (level));

umpi_mpi_MPI_Pcontrol_post(rc, pc,
			level);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Pcontrol(  const int level ,
		... )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Pcontrol(pc,
			level );
	return rc;
}

static void mpi_pcontrol_f_wrap(MPI_Fint *level , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
		rc = gwrap_MPI_Pcontrol(pc ,
			*(level));
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_PCONTROL(MPI_Fint *level , MPI_Fint * ierr) { mpi_pcontrol_f_wrap(level, ierr); }
extern void mpi_pcontrol(MPI_Fint *level , MPI_Fint * ierr) { mpi_pcontrol_f_wrap(level, ierr); }
extern void mpi_pcontrol_(MPI_Fint *level , MPI_Fint * ierr) { mpi_pcontrol_f_wrap(level, ierr); }
extern void mpi_pcontrol__(MPI_Fint *level , MPI_Fint * ierr) { mpi_pcontrol_f_wrap(level, ierr); }

/*--------------------------------------------- MPI_Probe */

/*-------------------- Wrapper for MPI_Probe omitted */

/*--------------------------------------------- MPI_Recv */

static int
umpi_mpi_MPI_Recv_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Recv_pre( 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int source ,
int tag ,
MPI_Comm comm ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Recv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.source = source;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Recv_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Recv_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, SOURCE, ENCODE_SOURCE(op->data.mpi.source), my_rank);
	addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Recv_post( 
int MPI_rc, 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int source ,
int tag ,
MPI_Comm comm ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Recv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.source = source;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Recv_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Recv( 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int source ,
int tag ,
MPI_Comm comm ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Recv(  (buf),
(count),
(datatype),
(source),
(tag),
(comm),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Recv_pre(pc,
			buf,
			count,
			datatype,
			source,
			tag,
			comm,
			status);
	rc = PMPI_Recv(  (buf),
(count),
(datatype),
(source),
(tag),
(comm),
(status));

umpi_mpi_MPI_Recv_post(rc, pc,
			buf,
			count,
			datatype,
			source,
			tag,
			comm,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Recv(  void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int source ,
		int tag ,
		MPI_Comm comm ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Recv(pc,
			buf,
			count,
			datatype,
			source,
			tag,
			comm,
			status );
	return rc;
}

static void mpi_recv_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Recv(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(source),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_Recv(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(source),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_RECV(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_recv_f_wrap(buf, count, datatype, source, tag, comm, status, ierr); }
extern void mpi_recv(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_recv_f_wrap(buf, count, datatype, source, tag, comm, status, ierr); }
extern void mpi_recv_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_recv_f_wrap(buf, count, datatype, source, tag, comm, status, ierr); }
extern void mpi_recv__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_recv_f_wrap(buf, count, datatype, source, tag, comm, status, ierr); }

/*--------------------------------------------- MPI_Recv_init */

static int
umpi_mpi_MPI_Recv_init_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Recv_init_pre( 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int source ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Recv_init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.source = source;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Recv_init_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Recv_init_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
    addScalarValue(event, SOURCE, ENCODE_SOURCE(op->data.mpi.source), my_rank);
    addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
    addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
    appendEvent(&trace, event);
  #ifndef FEATURE_PARAM_HISTO
    add_request_entry (record_req, op->data.mpi.request);
  #endif
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Recv_init_post( 
int MPI_rc, 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int source ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Recv_init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.source = source;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Recv_init_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Recv_init( 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int source ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Recv_init(  (buf),
(count),
(datatype),
(source),
(tag),
(comm),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Recv_init_pre(pc,
			buf,
			count,
			datatype,
			source,
			tag,
			comm,
			request);
	rc = PMPI_Recv_init(  (buf),
(count),
(datatype),
(source),
(tag),
(comm),
(request));

umpi_mpi_MPI_Recv_init_post(rc, pc,
			buf,
			count,
			datatype,
			source,
			tag,
			comm,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Recv_init(  void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int source ,
		int tag ,
		MPI_Comm comm ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Recv_init(pc,
			buf,
			count,
			datatype,
			source,
			tag,
			comm,
			request );
	return rc;
}

static void mpi_recv_init_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Recv_init(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(source),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Recv_init(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(source),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_RECV_INIT(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_recv_init_f_wrap(buf, count, datatype, source, tag, comm, request, ierr); }
extern void mpi_recv_init(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_recv_init_f_wrap(buf, count, datatype, source, tag, comm, request, ierr); }
extern void mpi_recv_init_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_recv_init_f_wrap(buf, count, datatype, source, tag, comm, request, ierr); }
extern void mpi_recv_init__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *source , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_recv_init_f_wrap(buf, count, datatype, source, tag, comm, request, ierr); }

/*--------------------------------------------- MPI_Reduce */

static int
umpi_mpi_MPI_Reduce_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Reduce_pre( 
void * pc , const void *buf ,
void *recvbuf ,
int count ,
MPI_Datatype datatype ,
MPI_Op op ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Reduce;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.op = op;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Reduce_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Reduce_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
    addScalarValue(event, MPI_OP, op_to_index(record_ptr, op->data.mpi.op), my_rank);
    addScalarValue(event, RT, op->data.mpi.root, my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
    appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Reduce\n");
 }
 #endif 
 #ifdef ONLINE_CLUSTERING
   #ifdef AUTOMATIC_MARKER
   #ifdef REDUCE_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_REDUCE) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Reduce_post( 
int MPI_rc, 
void * pc , const void *buf ,
void *recvbuf ,
int count ,
MPI_Datatype datatype ,
MPI_Op op ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Reduce;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.op = op;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Reduce_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Reduce( 
void * pc , const void *buf ,
void *recvbuf ,
int count ,
MPI_Datatype datatype ,
MPI_Op op ,
int root ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Reduce(  (buf),
(recvbuf),
(count),
(datatype),
(op),
(root),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Reduce_pre(pc,
			buf,
			recvbuf,
			count,
			datatype,
			op,
			root,
			comm);
	rc = PMPI_Reduce(  (buf),
(recvbuf),
(count),
(datatype),
(op),
(root),
(comm));

umpi_mpi_MPI_Reduce_post(rc, pc,
			buf,
			recvbuf,
			count,
			datatype,
			op,
			root,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Reduce(  const void *buf ,
		void *recvbuf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Op op ,
		int root ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Reduce(pc,
			buf,
			recvbuf,
			count,
			datatype,
			op,
			root,
			comm );
	return rc;
}

static void mpi_reduce_f_wrap(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Reduce(pc ,
			buf,
			recvbuf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Op)(*(op)),
			*(root),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Reduce(pc ,
			buf,
			recvbuf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			MPI_Op_f2c(*(op)),
			*(root),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_REDUCE(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_reduce_f_wrap(buf, recvbuf, count, datatype, op, root, comm, ierr); }
extern void mpi_reduce(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_reduce_f_wrap(buf, recvbuf, count, datatype, op, root, comm, ierr); }
extern void mpi_reduce_(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_reduce_f_wrap(buf, recvbuf, count, datatype, op, root, comm, ierr); }
extern void mpi_reduce__(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_reduce_f_wrap(buf, recvbuf, count, datatype, op, root, comm, ierr); }

/*--------------------------------------------- MPI_Reduce_scatter */

static int
umpi_mpi_MPI_Reduce_scatter_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Reduce_scatter_pre( 
void * pc , const void *buf ,
void *recvbuf ,
const int *recvcounts ,
MPI_Datatype datatype ,
MPI_Op op ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Reduce_scatter;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
uop->data.mpi.recvbuf = recvbuf;
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.recvcounts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.recvcounts);
bcopy (recvcounts, uop->data.mpi.recvcounts, temp_umpi_comm_size * sizeof (int));
}
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.op = op;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Reduce_scatter_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Reduce_scatter_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
    addScalarValue(event, MPI_OP, op_to_index(record_ptr, op->data.mpi.op), my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	addVectorValue(event, RECVCOUNTS, op->data.mpi.size, op->data.mpi.recvcounts, my_rank);
    appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Reduce_scatter_post( 
int MPI_rc, 
void * pc , const void *buf ,
void *recvbuf ,
const int *recvcounts ,
MPI_Datatype datatype ,
MPI_Op op ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Reduce_scatter;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
uop->data.mpi.recvbuf = recvbuf;
{
int temp_umpi_comm_size;
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.recvcounts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.recvcounts);
bcopy (recvcounts, uop->data.mpi.recvcounts, temp_umpi_comm_size * sizeof (int));
}
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.op = op;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Reduce_scatter_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Reduce_scatter( 
void * pc , const void *buf ,
void *recvbuf ,
const int *recvcounts ,
MPI_Datatype datatype ,
MPI_Op op ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Reduce_scatter(  (buf),
(recvbuf),
(recvcounts),
(datatype),
(op),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Reduce_scatter_pre(pc,
			buf,
			recvbuf,
			recvcounts,
			datatype,
			op,
			comm);
	rc = PMPI_Reduce_scatter(  (buf),
(recvbuf),
(recvcounts),
(datatype),
(op),
(comm));

umpi_mpi_MPI_Reduce_scatter_post(rc, pc,
			buf,
			recvbuf,
			recvcounts,
			datatype,
			op,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Reduce_scatter(  const void *buf ,
		void *recvbuf ,
		const int *recvcounts ,
		MPI_Datatype datatype ,
		MPI_Op op ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Reduce_scatter(pc,
			buf,
			recvbuf,
			recvcounts,
			datatype,
			op,
			comm );
	return rc;
}

static void mpi_reduce_scatter_f_wrap(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Reduce_scatter(pc ,
			buf,
			recvbuf,
			recvcounts,
			(MPI_Datatype)(*(datatype)),
			(MPI_Op)(*(op)),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Reduce_scatter(pc ,
			buf,
			recvbuf,
			recvcounts,
			MPI_Type_f2c(*(datatype)),
			MPI_Op_f2c(*(op)),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_REDUCE_SCATTER(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_reduce_scatter_f_wrap(buf, recvbuf, recvcounts, datatype, op, comm, ierr); }
extern void mpi_reduce_scatter(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_reduce_scatter_f_wrap(buf, recvbuf, recvcounts, datatype, op, comm, ierr); }
extern void mpi_reduce_scatter_(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_reduce_scatter_f_wrap(buf, recvbuf, recvcounts, datatype, op, comm, ierr); }
extern void mpi_reduce_scatter__(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *recvcounts , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_reduce_scatter_f_wrap(buf, recvbuf, recvcounts, datatype, op, comm, ierr); }

/*--------------------------------------------- MPI_Request_c2f */

/*-------------------- Wrapper for MPI_Request_c2f omitted */

/*--------------------------------------------- MPI_Request_free */

static int
umpi_mpi_MPI_Request_free_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op(&operation);
  init_event(event);
  recordEventTime(event, 0);
  operation.fields = REQUEST;
  event_set_param_list(event, op->op, op->seq_num, REQUEST);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  lookup_offset (record_req, op->data.mpi.request, &operation.request);
  int req = 0;
  lookup_offset (record_req, op->data.mpi.request, &req);
  add_scalar_param_int(event, TYPE_REQUEST, req);
  recordTime (&operation, 0);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Request_free_pre( 
void * pc , MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Request_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Request_free_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Request_free_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  recordTime (&operation, 1);
  if (op->data.mpi.request == MPI_REQUEST_NULL)
  {
    int req = 0;
    event_get_param (event, TYPE_REQUEST, (void *)&req);
    reset_offset (record_req, req);
  }
  recordTime (0,0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Request_free_post( 
int MPI_rc, 
void * pc , MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Request_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Request_free_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Request_free( 
void * pc , MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Request_free(  (request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Request_free_pre(pc,
			request);
	rc = PMPI_Request_free(  (request));

umpi_mpi_MPI_Request_free_post(rc, pc,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Request_free(  MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Request_free(pc,
			request );
	return rc;
}

static void mpi_request_free_f_wrap(MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Request_free(pc ,
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Request_free(pc ,
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_REQUEST_FREE(MPI_Fint *request , MPI_Fint * ierr) { mpi_request_free_f_wrap(request, ierr); }
extern void mpi_request_free(MPI_Fint *request , MPI_Fint * ierr) { mpi_request_free_f_wrap(request, ierr); }
extern void mpi_request_free_(MPI_Fint *request , MPI_Fint * ierr) { mpi_request_free_f_wrap(request, ierr); }
extern void mpi_request_free__(MPI_Fint *request , MPI_Fint * ierr) { mpi_request_free_f_wrap(request, ierr); }

/*--------------------------------------------- MPI_Rsend */

static int
umpi_mpi_MPI_Rsend_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Rsend_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Rsend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Rsend_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Rsend_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, DEST, ENCODE_DEST(op->data.mpi.dest), my_rank);
	addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Rsend_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Rsend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Rsend_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Rsend( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Rsend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Rsend_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm);
	rc = PMPI_Rsend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm));

umpi_mpi_MPI_Rsend_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Rsend(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Rsend(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm );
	return rc;
}

static void mpi_rsend_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Rsend(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Rsend(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_RSEND(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_rsend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_rsend(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_rsend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_rsend_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_rsend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_rsend__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_rsend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }

/*--------------------------------------------- MPI_Rsend_init */

static int
umpi_mpi_MPI_Rsend_init_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Rsend_init_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Rsend_init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Rsend_init_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Rsend_init_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = COUNT | DATATYPE | DEST | TAG | COMM;
  event_set_param_list(event, op->op, op->seq_num, COUNT | DATATYPE | DEST | TAG | COMM);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  operation.datatype = type_to_index(record_ptr, op->data.mpi.datatype);
  add_scalar_param_int(event, TYPE_DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype));
  operation.dest = ENCODE_DEST(op->data.mpi.dest);
  add_scalar_param_int(event, TYPE_DEST, ENCODE_DEST(op->data.mpi.dest));
  operation.tag = ENCODE_TAG ( op->data.mpi.tag );
  add_scalar_param_int(event, TYPE_TAG, ENCODE_TAG ( op->data.mpi.tag ));
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  add_request_entry (record_req, op->data.mpi.request);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Rsend_init_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Rsend_init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Rsend_init_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Rsend_init( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Rsend_init(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Rsend_init_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
	rc = PMPI_Rsend_init(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

umpi_mpi_MPI_Rsend_init_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Rsend_init(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Rsend_init(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request );
	return rc;
}

static void mpi_rsend_init_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Rsend_init(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Rsend_init(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_RSEND_INIT(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_rsend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_rsend_init(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_rsend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_rsend_init_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_rsend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_rsend_init__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_rsend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }

/*--------------------------------------------- MPI_Scan */

static int
umpi_mpi_MPI_Scan_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Scan_pre( 
void * pc , const void *buf ,
void *recvbuf ,
int count ,
MPI_Datatype datatype ,
MPI_Op op ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Scan;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.op = op;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Scan_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Scan_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = COUNT | DATATYPE | MPI_OP | COMM;
  event_set_param_list(event, op->op, op->seq_num, COUNT | DATATYPE | MPI_OP | COMM);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  operation.datatype = type_to_index(record_ptr, op->data.mpi.datatype);
  add_scalar_param_int(event, TYPE_DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype));
  operation.mpi_op = op_to_index (record_ptr, op->data.mpi.op);
  add_scalar_param_int(event, TYPE_MPI_OP, op_to_index (record_ptr, op->data.mpi.op));
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Scan_post( 
int MPI_rc, 
void * pc , const void *buf ,
void *recvbuf ,
int count ,
MPI_Datatype datatype ,
MPI_Op op ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Scan;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.op = op;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Scan_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Scan( 
void * pc , const void *buf ,
void *recvbuf ,
int count ,
MPI_Datatype datatype ,
MPI_Op op ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Scan(  (buf),
(recvbuf),
(count),
(datatype),
(op),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Scan_pre(pc,
			buf,
			recvbuf,
			count,
			datatype,
			op,
			comm);
	rc = PMPI_Scan(  (buf),
(recvbuf),
(count),
(datatype),
(op),
(comm));

umpi_mpi_MPI_Scan_post(rc, pc,
			buf,
			recvbuf,
			count,
			datatype,
			op,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Scan(  const void *buf ,
		void *recvbuf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Op op ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Scan(pc,
			buf,
			recvbuf,
			count,
			datatype,
			op,
			comm );
	return rc;
}

static void mpi_scan_f_wrap(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Scan(pc ,
			buf,
			recvbuf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Op)(*(op)),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Scan(pc ,
			buf,
			recvbuf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			MPI_Op_f2c(*(op)),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_SCAN(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scan_f_wrap(buf, recvbuf, count, datatype, op, comm, ierr); }
extern void mpi_scan(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scan_f_wrap(buf, recvbuf, count, datatype, op, comm, ierr); }
extern void mpi_scan_(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scan_f_wrap(buf, recvbuf, count, datatype, op, comm, ierr); }
extern void mpi_scan__(MPI_Fint *buf , MPI_Fint *recvbuf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *op , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scan_f_wrap(buf, recvbuf, count, datatype, op, comm, ierr); }

/*--------------------------------------------- MPI_Scatter */

static int
umpi_mpi_MPI_Scatter_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Scatter_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Scatter;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Scatter_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Scatter_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, RECVCOUNT, op->data.mpi.recvcount, my_rank);
	addScalarValue(event, RECVTYPE, type_to_index(record_ptr, op->data.mpi.recvtype), my_rank);
	addScalarValue(event, RT, op->data.mpi.root, my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Scatter\n");
 }
 #endif 
 #ifdef ONLINE_CLUSTERING
   #ifdef AUTOMATIC_MARKER
   #ifdef SCATTER_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_SCATTER) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Scatter_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Scatter;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Scatter_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Scatter( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Scatter(  (buf),
(count),
(datatype),
(recvbuf),
(recvcount),
(recvtype),
(root),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Scatter_pre(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			root,
			comm);
	rc = PMPI_Scatter(  (buf),
(count),
(datatype),
(recvbuf),
(recvcount),
(recvtype),
(root),
(comm));

umpi_mpi_MPI_Scatter_post(rc, pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			root,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Scatter(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		void *recvbuf ,
		int recvcount ,
		MPI_Datatype recvtype ,
		int root ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Scatter(pc,
			buf,
			count,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			root,
			comm );
	return rc;
}

static void mpi_scatter_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Scatter(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			recvbuf,
			*(recvcount),
			(MPI_Datatype)(*(recvtype)),
			*(root),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Scatter(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			recvbuf,
			*(recvcount),
			MPI_Type_f2c(*(recvtype)),
			*(root),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_SCATTER(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scatter_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }
extern void mpi_scatter(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scatter_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }
extern void mpi_scatter_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scatter_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }
extern void mpi_scatter__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scatter_f_wrap(buf, count, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }

/*--------------------------------------------- MPI_Scatterv */

static int
umpi_mpi_MPI_Scatterv_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Scatterv_pre( 
void * pc , const void *buf ,
const int *counts ,
const int *displs ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Scatterv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
{
int temp_umpi_comm_size, temp_umpi_lrank;
/* counts only significant at root... */
PMPI_Comm_rank (comm, &temp_umpi_lrank);
if (temp_umpi_lrank == root) {
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.counts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.counts);
bcopy (counts, uop->data.mpi.counts, temp_umpi_comm_size * sizeof (int));
}
else {
uop->data.mpi.counts = NULL;
}
}
{
int temp_umpi_comm_size, temp_umpi_lrank;
/* displs only significant at root... */
PMPI_Comm_rank (comm, &temp_umpi_lrank);
if (temp_umpi_lrank == root) {
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.displs = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.displs);
bcopy (displs, uop->data.mpi.displs, temp_umpi_comm_size * sizeof (int));
}
else {
uop->data.mpi.displs = NULL;
}
}
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Scatterv_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Scatterv_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
    addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
    addScalarValue(event, RT, op->data.mpi.root, my_rank);
    addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
    addScalarValue(event, RECVTYPE, type_to_index(record_ptr, op->data.mpi.recvtype), my_rank);
	addScalarValue(event, RECVCOUNT, op->data.mpi.recvcount, my_rank);
    addVectorValue(event, COUNTS, op->data.mpi.size, op->data.mpi.counts, my_rank);
    addVectorValue(event, DISPLS, op->data.mpi.size, op->data.mpi.displs, my_rank);
    appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Scatterv\n");
 }
 #endif 
 #ifdef ONLINE_CLUSTERING
   #ifdef AUTOMATIC_MARKER
   #ifdef SCATTERV_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_Scatterv) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Scatterv_post( 
int MPI_rc, 
void * pc , const void *buf ,
const int *counts ,
const int *displs ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Scatterv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
{
int temp_umpi_comm_size, temp_umpi_lrank;
/* counts only significant at root... */
PMPI_Comm_rank (comm, &temp_umpi_lrank);
if (temp_umpi_lrank == root) {
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.counts = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.counts);
bcopy (counts, uop->data.mpi.counts, temp_umpi_comm_size * sizeof (int));
}
else {
uop->data.mpi.counts = NULL;
}
}
{
int temp_umpi_comm_size, temp_umpi_lrank;
/* displs only significant at root... */
PMPI_Comm_rank (comm, &temp_umpi_lrank);
if (temp_umpi_lrank == root) {
PMPI_Comm_size (comm, &temp_umpi_comm_size);
uop->data.mpi.displs = (int *) malloc (temp_umpi_comm_size * sizeof (int));
assert (uop->data.mpi.displs);
bcopy (displs, uop->data.mpi.displs, temp_umpi_comm_size * sizeof (int));
}
else {
uop->data.mpi.displs = NULL;
}
}
		uop->data.mpi.datatype = datatype;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.root = root;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Scatterv_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Scatterv( 
void * pc , const void *buf ,
const int *counts ,
const int *displs ,
MPI_Datatype datatype ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int root ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Scatterv(  (buf),
(counts),
(displs),
(datatype),
(recvbuf),
(recvcount),
(recvtype),
(root),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Scatterv_pre(pc,
			buf,
			counts,
			displs,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			root,
			comm);
	rc = PMPI_Scatterv(  (buf),
(counts),
(displs),
(datatype),
(recvbuf),
(recvcount),
(recvtype),
(root),
(comm));

umpi_mpi_MPI_Scatterv_post(rc, pc,
			buf,
			counts,
			displs,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			root,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Scatterv(  const void *buf ,
		const int *counts ,
		const int *displs ,
		MPI_Datatype datatype ,
		void *recvbuf ,
		int recvcount ,
		MPI_Datatype recvtype ,
		int root ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Scatterv(pc,
			buf,
			counts,
			displs,
			datatype,
			recvbuf,
			recvcount,
			recvtype,
			root,
			comm );
	return rc;
}

static void mpi_scatterv_f_wrap(MPI_Fint *buf , MPI_Fint *counts , MPI_Fint *displs , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Scatterv(pc ,
			buf,
			counts,
			displs,
			(MPI_Datatype)(*(datatype)),
			recvbuf,
			*(recvcount),
			(MPI_Datatype)(*(recvtype)),
			*(root),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Scatterv(pc ,
			buf,
			counts,
			displs,
			MPI_Type_f2c(*(datatype)),
			recvbuf,
			*(recvcount),
			MPI_Type_f2c(*(recvtype)),
			*(root),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_SCATTERV(MPI_Fint *buf , MPI_Fint *counts , MPI_Fint *displs , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scatterv_f_wrap(buf, counts, displs, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }
extern void mpi_scatterv(MPI_Fint *buf , MPI_Fint *counts , MPI_Fint *displs , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scatterv_f_wrap(buf, counts, displs, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }
extern void mpi_scatterv_(MPI_Fint *buf , MPI_Fint *counts , MPI_Fint *displs , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scatterv_f_wrap(buf, counts, displs, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }
extern void mpi_scatterv__(MPI_Fint *buf , MPI_Fint *counts , MPI_Fint *displs , MPI_Fint *datatype , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *root , MPI_Fint *comm , MPI_Fint * ierr) { mpi_scatterv_f_wrap(buf, counts, displs, datatype, recvbuf, recvcount, recvtype, root, comm, ierr); }

/*--------------------------------------------- MPI_Send */

static int
umpi_mpi_MPI_Send_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Send_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Send;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Send_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Send_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
    addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
    addScalarValue(event, DEST, ENCODE_DEST(op->data.mpi.dest), my_rank);
    addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
    addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
    appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Send_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Send;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Send_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Send( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Send(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Send_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm);
	rc = PMPI_Send(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm));

umpi_mpi_MPI_Send_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Send(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Send(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm );
	return rc;
}

static void mpi_send_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Send(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Send(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_SEND(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_send_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_send(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_send_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_send_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_send_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_send__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_send_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }

/*--------------------------------------------- MPI_Send_init */

static int
umpi_mpi_MPI_Send_init_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Send_init_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Send_init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Send_init_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Send_init_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	addScalarValue(event, DEST, ENCODE_DEST(op->data.mpi.dest), my_rank);
	addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
	addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
	appendEvent(&trace, event);
  #ifndef FEATURE_PARAM_HISTO
	add_request_entry (record_req, op->data.mpi.request);
  #endif
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Send_init_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Send_init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Send_init_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Send_init( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Send_init(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Send_init_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
	rc = PMPI_Send_init(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

umpi_mpi_MPI_Send_init_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Send_init(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Send_init(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request );
	return rc;
}

static void mpi_send_init_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Send_init(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Send_init(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_SEND_INIT(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_send_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_send_init(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_send_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_send_init_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_send_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_send_init__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_send_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }

/*--------------------------------------------- MPI_Sendrecv */

static int
umpi_mpi_MPI_Sendrecv_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Sendrecv_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int sendtag ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int source ,
int recvtag ,
MPI_Comm comm ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Sendrecv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.sendtag = sendtag;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.source = source;
		uop->data.mpi.recvtag = recvtag;
		uop->data.mpi.comm = comm;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Sendrecv_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Sendrecv_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
    addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
    addScalarValue(event, DEST, ENCODE_DEST(op->data.mpi.dest), my_rank);
    addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.sendtag), my_rank);
	addScalarValue(event, RECVCOUNT, op->data.mpi.recvcount, my_rank);
    addScalarValue(event, RECVTYPE, type_to_index(record_ptr, op->data.mpi.recvtype), my_rank);
    addScalarValue(event, SOURCE, ENCODE_SOURCE(op->data.mpi.source), my_rank);
    addScalarValue(event, RECVTAG, ENCODE_TAG(op->data.mpi.recvtag), my_rank);
    addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
    appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Sendrecv_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int sendtag ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int source ,
int recvtag ,
MPI_Comm comm ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Sendrecv;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.sendtag = sendtag;
uop->data.mpi.recvbuf = recvbuf;
		uop->data.mpi.recvcount = recvcount;
		uop->data.mpi.recvtype = recvtype;
		uop->data.mpi.source = source;
		uop->data.mpi.recvtag = recvtag;
		uop->data.mpi.comm = comm;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Sendrecv_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Sendrecv( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int sendtag ,
void *recvbuf ,
int recvcount ,
MPI_Datatype recvtype ,
int source ,
int recvtag ,
MPI_Comm comm ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Sendrecv(  (buf),
(count),
(datatype),
(dest),
(sendtag),
(recvbuf),
(recvcount),
(recvtype),
(source),
(recvtag),
(comm),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Sendrecv_pre(pc,
			buf,
			count,
			datatype,
			dest,
			sendtag,
			recvbuf,
			recvcount,
			recvtype,
			source,
			recvtag,
			comm,
			status);
	rc = PMPI_Sendrecv(  (buf),
(count),
(datatype),
(dest),
(sendtag),
(recvbuf),
(recvcount),
(recvtype),
(source),
(recvtag),
(comm),
(status));

umpi_mpi_MPI_Sendrecv_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			sendtag,
			recvbuf,
			recvcount,
			recvtype,
			source,
			recvtag,
			comm,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Sendrecv(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int sendtag ,
		void *recvbuf ,
		int recvcount ,
		MPI_Datatype recvtype ,
		int source ,
		int recvtag ,
		MPI_Comm comm ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Sendrecv(pc,
			buf,
			count,
			datatype,
			dest,
			sendtag,
			recvbuf,
			recvcount,
			recvtype,
			source,
			recvtag,
			comm,
			status );
	return rc;
}

static void mpi_sendrecv_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *sendtag , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *source , MPI_Fint *recvtag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Sendrecv(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(sendtag),
			recvbuf,
			*(recvcount),
			(MPI_Datatype)(*(recvtype)),
			*(source),
			*(recvtag),
			(MPI_Comm)(*(comm)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_Sendrecv(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(sendtag),
			recvbuf,
			*(recvcount),
			MPI_Type_f2c(*(recvtype)),
			*(source),
			*(recvtag),
			MPI_Comm_f2c(*(comm)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_SENDRECV(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *sendtag , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *source , MPI_Fint *recvtag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_sendrecv_f_wrap(buf, count, datatype, dest, sendtag, recvbuf, recvcount, recvtype, source, recvtag, comm, status, ierr); }
extern void mpi_sendrecv(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *sendtag , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *source , MPI_Fint *recvtag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_sendrecv_f_wrap(buf, count, datatype, dest, sendtag, recvbuf, recvcount, recvtype, source, recvtag, comm, status, ierr); }
extern void mpi_sendrecv_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *sendtag , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *source , MPI_Fint *recvtag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_sendrecv_f_wrap(buf, count, datatype, dest, sendtag, recvbuf, recvcount, recvtype, source, recvtag, comm, status, ierr); }
extern void mpi_sendrecv__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *sendtag , MPI_Fint *recvbuf , MPI_Fint *recvcount , MPI_Fint *recvtype , MPI_Fint *source , MPI_Fint *recvtag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_sendrecv_f_wrap(buf, count, datatype, dest, sendtag, recvbuf, recvcount, recvtype, source, recvtag, comm, status, ierr); }

/*--------------------------------------------- MPI_Sendrecv_replace */

static int
umpi_mpi_MPI_Sendrecv_replace_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
	op->data.mpi.recvbuf = op->data.mpi.buf;
	op->data.mpi.recvcount = op->data.mpi.count;
	op->data.mpi.recvtype = op->data.mpi.datatype;
*/
}
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Sendrecv_replace_pre( 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int sendtag ,
int source ,
int recvtag ,
MPI_Comm comm ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Sendrecv_replace;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.sendtag = sendtag;
		uop->data.mpi.source = source;
		uop->data.mpi.recvtag = recvtag;
		uop->data.mpi.comm = comm;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Sendrecv_replace_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Sendrecv_replace_immediate_local_post(umpi_op_t *op)
{

{
{
/*
	op->data.mpi.recvbuf = op->data.mpi.buf;
	op->data.mpi.recvcount = op->data.mpi.count;
	op->data.mpi.recvtype =  op->data.mpi.datatype;
    operation.fields = COUNT | DATATYPE | DEST | TAG | SOURCE | RECVTAG | COMM | RECVCOUNT | RECVTYPE;
  event_set_param_list(event, op->op, op->seq_num, COUNT | DATATYPE | DEST | TAG | SOURCE | RECVTAG | COMM | RECVCOUNT | RECVTYPE);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  operation.datatype = type_to_index(record_ptr, op->data.mpi.datatype);
  add_scalar_param_int(event, TYPE_DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype));
  operation.dest = ENCODE_DEST(op->data.mpi.dest);
  add_scalar_param_int(event, TYPE_DEST, ENCODE_DEST(op->data.mpi.dest));
  operation.tag = op->data.mpi.sendtag;
  add_scalar_param_int(event, TYPE_TAG, op->data.mpi.sendtag);
  operation.source = ENCODE_SOURCE(op->data.mpi.source);
  add_scalar_param_int(event, TYPE_SOURCE, ENCODE_SOURCE(op->data.mpi.source));
  operation.recvtag = op->data.mpi.recvtag;
  add_scalar_param_int(event, TYPE_RECVTAG, op->data.mpi.recvtag);
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  operation.recvcount = op->data.mpi.recvcount;
  add_scalar_param_int(event, TYPE_RECVCOUNT, op->data.mpi.recvcount);
  operation.recvtype = type_to_index(record_ptr, op->data.mpi.recvtype);
  add_scalar_param_int(event, TYPE_RECVTYPE, type_to_index(record_ptr, op->data.mpi.recvtype));
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Sendrecv_replace_post( 
int MPI_rc, 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int sendtag ,
int source ,
int recvtag ,
MPI_Comm comm ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Sendrecv_replace;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.sendtag = sendtag;
		uop->data.mpi.source = source;
		uop->data.mpi.recvtag = recvtag;
		uop->data.mpi.comm = comm;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Sendrecv_replace_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Sendrecv_replace( 
void * pc , void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int sendtag ,
int source ,
int recvtag ,
MPI_Comm comm ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Sendrecv_replace(  (buf),
(count),
(datatype),
(dest),
(sendtag),
(source),
(recvtag),
(comm),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Sendrecv_replace_pre(pc,
			buf,
			count,
			datatype,
			dest,
			sendtag,
			source,
			recvtag,
			comm,
			status);
	rc = PMPI_Sendrecv_replace(  (buf),
(count),
(datatype),
(dest),
(sendtag),
(source),
(recvtag),
(comm),
(status));

umpi_mpi_MPI_Sendrecv_replace_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			sendtag,
			source,
			recvtag,
			comm,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Sendrecv_replace(  void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int sendtag ,
		int source ,
		int recvtag ,
		MPI_Comm comm ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Sendrecv_replace(pc,
			buf,
			count,
			datatype,
			dest,
			sendtag,
			source,
			recvtag,
			comm,
			status );
	return rc;
}

static void mpi_sendrecv_replace_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *sendtag , MPI_Fint *source , MPI_Fint *recvtag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Sendrecv_replace(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(sendtag),
			*(source),
			*(recvtag),
			(MPI_Comm)(*(comm)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_Sendrecv_replace(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(sendtag),
			*(source),
			*(recvtag),
			MPI_Comm_f2c(*(comm)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_SENDRECV_REPLACE(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *sendtag , MPI_Fint *source , MPI_Fint *recvtag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_sendrecv_replace_f_wrap(buf, count, datatype, dest, sendtag, source, recvtag, comm, status, ierr); }
extern void mpi_sendrecv_replace(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *sendtag , MPI_Fint *source , MPI_Fint *recvtag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_sendrecv_replace_f_wrap(buf, count, datatype, dest, sendtag, source, recvtag, comm, status, ierr); }
extern void mpi_sendrecv_replace_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *sendtag , MPI_Fint *source , MPI_Fint *recvtag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_sendrecv_replace_f_wrap(buf, count, datatype, dest, sendtag, source, recvtag, comm, status, ierr); }
extern void mpi_sendrecv_replace__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *sendtag , MPI_Fint *source , MPI_Fint *recvtag , MPI_Fint *comm , MPI_Fint *status , MPI_Fint * ierr) { mpi_sendrecv_replace_f_wrap(buf, count, datatype, dest, sendtag, source, recvtag, comm, status, ierr); }

/*--------------------------------------------- MPI_Ssend */

static int
umpi_mpi_MPI_Ssend_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Ssend_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Ssend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Ssend_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Ssend_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
    addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
    addScalarValue(event, DEST, ENCODE_DEST(op->data.mpi.dest), my_rank);
    addScalarValue(event, TAG, ENCODE_TAG(op->data.mpi.tag), my_rank);
    addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
    appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Ssend_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Ssend;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Ssend_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Ssend( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Ssend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Ssend_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm);
	rc = PMPI_Ssend(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm));

umpi_mpi_MPI_Ssend_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Ssend(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Ssend(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm );
	return rc;
}

static void mpi_ssend_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Ssend(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Ssend(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_SSEND(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_ssend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_ssend(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_ssend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_ssend_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_ssend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }
extern void mpi_ssend__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint * ierr) { mpi_ssend_f_wrap(buf, count, datatype, dest, tag, comm, ierr); }

/*--------------------------------------------- MPI_Ssend_init */

static int
umpi_mpi_MPI_Ssend_init_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Ssend_init_pre( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Ssend_init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Ssend_init_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Ssend_init_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = COUNT | DATATYPE | DEST | TAG | COMM;
  event_set_param_list(event, op->op, op->seq_num, COUNT | DATATYPE | DEST | TAG | COMM);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  operation.datatype = type_to_index(record_ptr, op->data.mpi.datatype);
  add_scalar_param_int(event, TYPE_DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype));
  operation.dest = ENCODE_DEST(op->data.mpi.dest);
  add_scalar_param_int(event, TYPE_DEST, ENCODE_DEST(op->data.mpi.dest));
  operation.tag = ENCODE_TAG ( op->data.mpi.tag );
  add_scalar_param_int(event, TYPE_TAG, ENCODE_TAG ( op->data.mpi.tag ));
  operation.comm = comm_to_index (record_ptr,  op->data.mpi.comm);
  add_scalar_param_int(event, TYPE_COMM, comm_to_index (record_ptr,  op->data.mpi.comm));
  add_request_entry (record_req, op->data.mpi.request);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Ssend_init_post( 
int MPI_rc, 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Ssend_init;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.dest = dest;
		uop->data.mpi.tag = tag;
		uop->data.mpi.comm = comm;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Ssend_init_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Ssend_init( 
void * pc , const void *buf ,
int count ,
MPI_Datatype datatype ,
int dest ,
int tag ,
MPI_Comm comm ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Ssend_init(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Ssend_init_pre(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
	rc = PMPI_Ssend_init(  (buf),
(count),
(datatype),
(dest),
(tag),
(comm),
(request));

umpi_mpi_MPI_Ssend_init_post(rc, pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Ssend_init(  const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		int dest ,
		int tag ,
		MPI_Comm comm ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Ssend_init(pc,
			buf,
			count,
			datatype,
			dest,
			tag,
			comm,
			request );
	return rc;
}

static void mpi_ssend_init_f_wrap(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Ssend_init(pc ,
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			*(dest),
			*(tag),
			(MPI_Comm)(*(comm)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Ssend_init(pc ,
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			*(dest),
			*(tag),
			MPI_Comm_f2c(*(comm)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_SSEND_INIT(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_ssend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_ssend_init(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_ssend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_ssend_init_(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_ssend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }
extern void mpi_ssend_init__(MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *dest , MPI_Fint *tag , MPI_Fint *comm , MPI_Fint *request , MPI_Fint * ierr) { mpi_ssend_init_f_wrap(buf, count, datatype, dest, tag, comm, request, ierr); }

/*--------------------------------------------- MPI_Start */

static int
umpi_mpi_MPI_Start_immediate_local_pre(umpi_op_t *op)
{

{
{
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	resetStats(event, PHASE_COMM);
/*
  init_op (&operation);
  init_event(event);
    operation.fields = REQUEST;
  event_set_param_list(event, op->op, op->seq_num, REQUEST);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  int req = 0;
  lookup_offset (record_req, op->data.mpi.request, &req);
  add_scalar_param_int(event, TYPE_REQUEST, req);
  recordTime (&operation, 0);
  recordEventTime(event, 0);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Start_pre( 
void * pc , MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Start;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Start_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Start_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  recordTime (&operation, 1);
  if (op->data.mpi.request == MPI_REQUEST_NULL)
  {
    int req = 0;
    event_get_param (event, TYPE_REQUEST, (void *)&req);
    reset_offset (record_req, req);
  }
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Start_post( 
int MPI_rc, 
void * pc , MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Start;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Start_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Start( 
void * pc , MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Start(  (request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Start_pre(pc,
			request);
	rc = PMPI_Start(  (request));

umpi_mpi_MPI_Start_post(rc, pc,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Start(  MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Start(pc,
			request );
	return rc;
}

static void mpi_start_f_wrap(MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Start(pc ,
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_Start(pc ,
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_START(MPI_Fint *request , MPI_Fint * ierr) { mpi_start_f_wrap(request, ierr); }
extern void mpi_start(MPI_Fint *request , MPI_Fint * ierr) { mpi_start_f_wrap(request, ierr); }
extern void mpi_start_(MPI_Fint *request , MPI_Fint * ierr) { mpi_start_f_wrap(request, ierr); }
extern void mpi_start__(MPI_Fint *request , MPI_Fint * ierr) { mpi_start_f_wrap(request, ierr); }

/*--------------------------------------------- MPI_Startall */

static int
umpi_mpi_MPI_Startall_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Startall_pre( 
void * pc , int count ,
MPI_Request array_of_requests [])

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Startall;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_requests = (MPI_Request *) malloc (uop->data.mpi.count * sizeof (MPI_Request));
assert (uop->data.mpi.array_of_requests);
bcopy (array_of_requests, uop->data.mpi.array_of_requests, uop->data.mpi.count * sizeof (MPI_Request));
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Startall_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Startall_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = COUNT;
  event_set_param_list(event, op->op, op->seq_num, COUNT | ARRAY_OF_REQUESTS);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.arrays = ARRAY_OF_REQUESTS;
  operation.array1 = OP_ARRAY_ALLOC(op->data.mpi.count);
  operation.array_size1 = op->data.mpi.count;
  if(!operation.array1) {
    perror("malloc operation.array1");
    exit(0);
  }
  lookup_offsetlist(record_req, op->data.mpi.array_of_requests, operation.array_size1, operation.array1, 0);
  int *req_arr = OP_ARRAY_ALLOC(op->data.mpi.count);
  lookup_offsetlist(record_req, op->data.mpi.array_of_requests, op->data.mpi.count, req_arr, 0);
  add_vector_param(event, TYPE_ARRAY_OF_REQUESTS, req_arr, op->data.mpi.count);
  free(req_arr);
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Startall_post( 
int MPI_rc, 
void * pc , int count ,
MPI_Request array_of_requests [])

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Startall;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_requests = (MPI_Request *) malloc (uop->data.mpi.count * sizeof (MPI_Request));
assert (uop->data.mpi.array_of_requests);
bcopy (array_of_requests, uop->data.mpi.array_of_requests, uop->data.mpi.count * sizeof (MPI_Request));
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Startall_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Startall( 
void * pc , int count ,
MPI_Request array_of_requests [])
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Startall(  (count),
(array_of_requests));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Startall_pre(pc,
			count,
			array_of_requests);
	rc = PMPI_Startall(  (count),
(array_of_requests));

umpi_mpi_MPI_Startall_post(rc, pc,
			count,
			array_of_requests);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Startall(  int count ,
		MPI_Request array_of_requests [])
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Startall(pc,
			count,
			array_of_requests );
	return rc;
}

static void mpi_startall_f_wrap(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Startall(pc ,
			*(count),
			(MPI_Request *)array_of_requests);
#else /* other mpi's need conversions */
		int temp_i;
		MPI_Request *temp_array_of_requests = (MPI_Request *)malloc(sizeof(MPI_Request) * *(count));
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			temp_array_of_requests[temp_i] = MPI_Request_f2c(array_of_requests[temp_i]);
		}
		rc = gwrap_MPI_Startall(pc ,
			*(count),
			temp_array_of_requests);
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			array_of_requests[temp_i] = MPI_Request_c2f(temp_array_of_requests[temp_i]);
		}
		free(temp_array_of_requests);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_STARTALL(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint * ierr) { mpi_startall_f_wrap(count, array_of_requests, ierr); }
extern void mpi_startall(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint * ierr) { mpi_startall_f_wrap(count, array_of_requests, ierr); }
extern void mpi_startall_(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint * ierr) { mpi_startall_f_wrap(count, array_of_requests, ierr); }
extern void mpi_startall__(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint * ierr) { mpi_startall_f_wrap(count, array_of_requests, ierr); }

/*--------------------------------------------- MPI_Status_c2f */

/*-------------------- Wrapper for MPI_Status_c2f omitted */

/*--------------------------------------------- MPI_Status_f2c */

/*-------------------- Wrapper for MPI_Status_f2c omitted */

/*--------------------------------------------- MPI_Test */

/*-------------------- Wrapper for MPI_Test omitted */

/*--------------------------------------------- MPI_Test_cancelled */

/*-------------------- Wrapper for MPI_Test_cancelled omitted */

/*--------------------------------------------- MPI_Testall */

/*-------------------- Wrapper for MPI_Testall omitted */

/*--------------------------------------------- MPI_Testany */

/*-------------------- Wrapper for MPI_Testany omitted */

/*--------------------------------------------- MPI_Testsome */

/*-------------------- Wrapper for MPI_Testsome omitted */

/*--------------------------------------------- MPI_Topo_test */

/*-------------------- Wrapper for MPI_Topo_test omitted */

/*--------------------------------------------- MPI_Type_commit */

static int
umpi_mpi_MPI_Type_commit_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_commit_pre( 
void * pc , MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_commit;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Type_commit_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Type_commit_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, NEWTYPE, type_to_index(record_ptr, op->data.mpi.newtype), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_commit_post( 
int MPI_rc, 
void * pc , MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_commit;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Type_commit_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Type_commit( 
void * pc , MPI_Datatype *newtype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Type_commit(  (newtype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Type_commit_pre(pc,
			newtype);
	rc = PMPI_Type_commit(  (newtype));

umpi_mpi_MPI_Type_commit_post(rc, pc,
			newtype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Type_commit(  MPI_Datatype *newtype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Type_commit(pc,
			newtype );
	return rc;
}

static void mpi_type_commit_f_wrap(MPI_Fint *newtype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Type_commit(pc ,
			(MPI_Datatype *)newtype);
#else /* other mpi's need conversions */
		MPI_Datatype temp_newtype;
		temp_newtype = MPI_Type_f2c(*(newtype));
		rc = gwrap_MPI_Type_commit(pc ,
			&temp_newtype);
		*(newtype) = MPI_Type_c2f(temp_newtype);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_TYPE_COMMIT(MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_commit_f_wrap(newtype, ierr); }
extern void mpi_type_commit(MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_commit_f_wrap(newtype, ierr); }
extern void mpi_type_commit_(MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_commit_f_wrap(newtype, ierr); }
extern void mpi_type_commit__(MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_commit_f_wrap(newtype, ierr); }

/*--------------------------------------------- MPI_Type_contiguous */

static int
umpi_mpi_MPI_Type_contiguous_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_contiguous_pre( 
void * pc , int count ,
MPI_Datatype old_type ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_contiguous;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
		uop->data.mpi.old_type = old_type;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Type_contiguous_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Type_contiguous_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.old_type), my_rank);
	add_type_entry(record_ptr, op->data.mpi.newtype);
	addScalarValue(event, NEWTYPE, type_to_index(record_ptr, op->data.mpi.newtype), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace),PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_contiguous_post( 
int MPI_rc, 
void * pc , int count ,
MPI_Datatype old_type ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_contiguous;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
		uop->data.mpi.old_type = old_type;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Type_contiguous_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Type_contiguous( 
void * pc , int count ,
MPI_Datatype old_type ,
MPI_Datatype *newtype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Type_contiguous(  (count),
(old_type),
(newtype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Type_contiguous_pre(pc,
			count,
			old_type,
			newtype);
	rc = PMPI_Type_contiguous(  (count),
(old_type),
(newtype));

umpi_mpi_MPI_Type_contiguous_post(rc, pc,
			count,
			old_type,
			newtype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Type_contiguous(  int count ,
		MPI_Datatype old_type ,
		MPI_Datatype *newtype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Type_contiguous(pc,
			count,
			old_type,
			newtype );
	return rc;
}

static void mpi_type_contiguous_f_wrap(MPI_Fint *count , MPI_Fint *old_type , MPI_Fint *newtype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Type_contiguous(pc ,
			*(count),
			(MPI_Datatype)(*(old_type)),
			(MPI_Datatype *)newtype);
#else /* other mpi's need conversions */
		MPI_Datatype temp_newtype;
		temp_newtype = MPI_Type_f2c(*(newtype));
		rc = gwrap_MPI_Type_contiguous(pc ,
			*(count),
			MPI_Type_f2c(*(old_type)),
			&temp_newtype);
		*(newtype) = MPI_Type_c2f(temp_newtype);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_TYPE_CONTIGUOUS(MPI_Fint *count , MPI_Fint *old_type , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_contiguous_f_wrap(count, old_type, newtype, ierr); }
extern void mpi_type_contiguous(MPI_Fint *count , MPI_Fint *old_type , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_contiguous_f_wrap(count, old_type, newtype, ierr); }
extern void mpi_type_contiguous_(MPI_Fint *count , MPI_Fint *old_type , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_contiguous_f_wrap(count, old_type, newtype, ierr); }
extern void mpi_type_contiguous__(MPI_Fint *count , MPI_Fint *old_type , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_contiguous_f_wrap(count, old_type, newtype, ierr); }

/*--------------------------------------------- MPI_Type_count */

/*-------------------- Wrapper for MPI_Type_count omitted */

/*--------------------------------------------- MPI_Type_create_indexed_block */

static int
umpi_mpi_MPI_Type_create_indexed_block_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_create_indexed_block_pre( 
void * pc , int count ,
int blocklength ,
const int array_of_displacements [],
MPI_Datatype datatype ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_create_indexed_block;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
		uop->data.mpi.blocklength = blocklength;
uop->data.mpi.array_of_displacements = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_displacements);
bcopy (array_of_displacements, uop->data.mpi.array_of_displacements, uop->data.mpi.count * sizeof (int));
		uop->data.mpi.datatype = datatype;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Type_create_indexed_block_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Type_create_indexed_block_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = NO_FIELDS;
  event_set_param_list(event, op->op, op->seq_num, NO_FIELDS);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  // probably should throw an error if we try to replay
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_create_indexed_block_post( 
int MPI_rc, 
void * pc , int count ,
int blocklength ,
const int array_of_displacements [],
MPI_Datatype datatype ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_create_indexed_block;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
		uop->data.mpi.blocklength = blocklength;
uop->data.mpi.array_of_displacements = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_displacements);
bcopy (array_of_displacements, uop->data.mpi.array_of_displacements, uop->data.mpi.count * sizeof (int));
		uop->data.mpi.datatype = datatype;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Type_create_indexed_block_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Type_create_indexed_block( 
void * pc , int count ,
int blocklength ,
const int array_of_displacements [],
MPI_Datatype datatype ,
MPI_Datatype *newtype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Type_create_indexed_block(  (count),
(blocklength),
(array_of_displacements),
(datatype),
(newtype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Type_create_indexed_block_pre(pc,
			count,
			blocklength,
			array_of_displacements,
			datatype,
			newtype);
	rc = PMPI_Type_create_indexed_block(  (count),
(blocklength),
(array_of_displacements),
(datatype),
(newtype));

umpi_mpi_MPI_Type_create_indexed_block_post(rc, pc,
			count,
			blocklength,
			array_of_displacements,
			datatype,
			newtype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Type_create_indexed_block(  int count ,
		int blocklength ,
		const int array_of_displacements [],
		MPI_Datatype datatype ,
		MPI_Datatype *newtype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Type_create_indexed_block(pc,
			count,
			blocklength,
			array_of_displacements,
			datatype,
			newtype );
	return rc;
}

static void mpi_type_create_indexed_block_f_wrap(MPI_Fint *count , MPI_Fint *blocklength , MPI_Fint array_of_displacements [], MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Type_create_indexed_block(pc ,
			*(count),
			*(blocklength),
			array_of_displacements,
			(MPI_Datatype)(*(datatype)),
			(MPI_Datatype *)newtype);
#else /* other mpi's need conversions */
		MPI_Datatype temp_newtype;
		temp_newtype = MPI_Type_f2c(*(newtype));
		rc = gwrap_MPI_Type_create_indexed_block(pc ,
			*(count),
			*(blocklength),
			array_of_displacements,
			MPI_Type_f2c(*(datatype)),
			&temp_newtype);
		*(newtype) = MPI_Type_c2f(temp_newtype);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_TYPE_CREATE_INDEXED_BLOCK(MPI_Fint *count , MPI_Fint *blocklength , MPI_Fint array_of_displacements [], MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_indexed_block_f_wrap(count, blocklength, array_of_displacements, datatype, newtype, ierr); }
extern void mpi_type_create_indexed_block(MPI_Fint *count , MPI_Fint *blocklength , MPI_Fint array_of_displacements [], MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_indexed_block_f_wrap(count, blocklength, array_of_displacements, datatype, newtype, ierr); }
extern void mpi_type_create_indexed_block_(MPI_Fint *count , MPI_Fint *blocklength , MPI_Fint array_of_displacements [], MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_indexed_block_f_wrap(count, blocklength, array_of_displacements, datatype, newtype, ierr); }
extern void mpi_type_create_indexed_block__(MPI_Fint *count , MPI_Fint *blocklength , MPI_Fint array_of_displacements [], MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_indexed_block_f_wrap(count, blocklength, array_of_displacements, datatype, newtype, ierr); }

/*--------------------------------------------- MPI_Type_create_subarray */

static int
umpi_mpi_MPI_Type_create_subarray_immediate_local_pre(umpi_op_t *op)
{

{
{
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event,PHASE_COMM);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_create_subarray_pre( 
void * pc , int count ,
const int *array_of_sizes ,
const int *array_of_subsizes ,
const int *array_of_starts ,
int order ,
MPI_Datatype datatype ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_create_subarray;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_sizes = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_sizes);
bcopy (array_of_sizes, uop->data.mpi.array_of_sizes, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_subsizes = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_subsizes);
bcopy (array_of_subsizes, uop->data.mpi.array_of_subsizes, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_starts = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_starts);
bcopy (array_of_starts, uop->data.mpi.array_of_starts, uop->data.mpi.count * sizeof (int));
		uop->data.mpi.order = order;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Type_create_subarray_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Type_create_subarray_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, ORDER, op->data.mpi.order, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
	add_type_entry(record_ptr, op->data.mpi.newtype);
	addScalarValue(event, NEWTYPE, type_to_index(record_ptr, op->data.mpi.newtype), my_rank);
	addVectorValue(event, ARRAY_OF_SIZES, op->data.mpi.count, op->data.mpi.array_of_sizes, my_rank);
	addVectorValue(event, ARRAY_OF_SUBSIZES, op->data.mpi.count, op->data.mpi.array_of_subsizes, my_rank);
	addVectorValue(event, ARRAY_OF_STARTS, op->data.mpi.count, op->data.mpi.array_of_starts, my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_create_subarray_post( 
int MPI_rc, 
void * pc , int count ,
const int *array_of_sizes ,
const int *array_of_subsizes ,
const int *array_of_starts ,
int order ,
MPI_Datatype datatype ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_create_subarray;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_sizes = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_sizes);
bcopy (array_of_sizes, uop->data.mpi.array_of_sizes, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_subsizes = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_subsizes);
bcopy (array_of_subsizes, uop->data.mpi.array_of_subsizes, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_starts = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_starts);
bcopy (array_of_starts, uop->data.mpi.array_of_starts, uop->data.mpi.count * sizeof (int));
		uop->data.mpi.order = order;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Type_create_subarray_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Type_create_subarray( 
void * pc , int count ,
const int *array_of_sizes ,
const int *array_of_subsizes ,
const int *array_of_starts ,
int order ,
MPI_Datatype datatype ,
MPI_Datatype *newtype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Type_create_subarray(  (count),
(array_of_sizes),
(array_of_subsizes),
(array_of_starts),
(order),
(datatype),
(newtype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Type_create_subarray_pre(pc,
			count,
			array_of_sizes,
			array_of_subsizes,
			array_of_starts,
			order,
			datatype,
			newtype);
	rc = PMPI_Type_create_subarray(  (count),
(array_of_sizes),
(array_of_subsizes),
(array_of_starts),
(order),
(datatype),
(newtype));

umpi_mpi_MPI_Type_create_subarray_post(rc, pc,
			count,
			array_of_sizes,
			array_of_subsizes,
			array_of_starts,
			order,
			datatype,
			newtype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Type_create_subarray(  int count ,
		const int *array_of_sizes ,
		const int *array_of_subsizes ,
		const int *array_of_starts ,
		int order ,
		MPI_Datatype datatype ,
		MPI_Datatype *newtype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Type_create_subarray(pc,
			count,
			array_of_sizes,
			array_of_subsizes,
			array_of_starts,
			order,
			datatype,
			newtype );
	return rc;
}

static void mpi_type_create_subarray_f_wrap(MPI_Fint *count , MPI_Fint *array_of_sizes , MPI_Fint *array_of_subsizes , MPI_Fint *array_of_starts , MPI_Fint *order , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Type_create_subarray(pc ,
			*(count),
			array_of_sizes,
			array_of_subsizes,
			array_of_starts,
			*(order),
			(MPI_Datatype)(*(datatype)),
			(MPI_Datatype *)newtype);
#else /* other mpi's need conversions */
		MPI_Datatype temp_newtype;
		temp_newtype = MPI_Type_f2c(*(newtype));
		rc = gwrap_MPI_Type_create_subarray(pc ,
			*(count),
			array_of_sizes,
			array_of_subsizes,
			array_of_starts,
			*(order),
			MPI_Type_f2c(*(datatype)),
			&temp_newtype);
		*(newtype) = MPI_Type_c2f(temp_newtype);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_TYPE_CREATE_SUBARRAY(MPI_Fint *count , MPI_Fint *array_of_sizes , MPI_Fint *array_of_subsizes , MPI_Fint *array_of_starts , MPI_Fint *order , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_subarray_f_wrap(count, array_of_sizes, array_of_subsizes, array_of_starts, order, datatype, newtype, ierr); }
extern void mpi_type_create_subarray(MPI_Fint *count , MPI_Fint *array_of_sizes , MPI_Fint *array_of_subsizes , MPI_Fint *array_of_starts , MPI_Fint *order , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_subarray_f_wrap(count, array_of_sizes, array_of_subsizes, array_of_starts, order, datatype, newtype, ierr); }
extern void mpi_type_create_subarray_(MPI_Fint *count , MPI_Fint *array_of_sizes , MPI_Fint *array_of_subsizes , MPI_Fint *array_of_starts , MPI_Fint *order , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_subarray_f_wrap(count, array_of_sizes, array_of_subsizes, array_of_starts, order, datatype, newtype, ierr); }
extern void mpi_type_create_subarray__(MPI_Fint *count , MPI_Fint *array_of_sizes , MPI_Fint *array_of_subsizes , MPI_Fint *array_of_starts , MPI_Fint *order , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_subarray_f_wrap(count, array_of_sizes, array_of_subsizes, array_of_starts, order, datatype, newtype, ierr); }

/*--------------------------------------------- MPI_Type_extent */

/*-------------------- Wrapper for MPI_Type_extent omitted */

/*--------------------------------------------- MPI_Type_free */

static int
umpi_mpi_MPI_Type_free_immediate_local_pre(umpi_op_t *op)
{

{
{
	createEvent(&event, op->op, my_rank);
	addScalarValue(event, NEWTYPE, type_to_index(record_ptr, op->data.mpi.newtype), my_rank);
	remove_type_entry(record_ptr, op->data.mpi.newtype);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	appendEvent(&trace, event);
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_free_pre( 
void * pc , MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Type_free_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Type_free_immediate_local_post(umpi_op_t *op)
{

{
{
	resetStats(headEvent(&trace), PHASE_COMP);
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_free_post( 
int MPI_rc, 
void * pc , MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_free;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Type_free_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Type_free( 
void * pc , MPI_Datatype *newtype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Type_free(  (newtype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Type_free_pre(pc,
			newtype);
	rc = PMPI_Type_free(  (newtype));

umpi_mpi_MPI_Type_free_post(rc, pc,
			newtype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Type_free(  MPI_Datatype *newtype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Type_free(pc,
			newtype );
	return rc;
}

static void mpi_type_free_f_wrap(MPI_Fint *newtype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Type_free(pc ,
			(MPI_Datatype *)newtype);
#else /* other mpi's need conversions */
		MPI_Datatype temp_newtype;
		temp_newtype = MPI_Type_f2c(*(newtype));
		rc = gwrap_MPI_Type_free(pc ,
			&temp_newtype);
		*(newtype) = MPI_Type_c2f(temp_newtype);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_TYPE_FREE(MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_free_f_wrap(newtype, ierr); }
extern void mpi_type_free(MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_free_f_wrap(newtype, ierr); }
extern void mpi_type_free_(MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_free_f_wrap(newtype, ierr); }
extern void mpi_type_free__(MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_free_f_wrap(newtype, ierr); }

/*--------------------------------------------- MPI_Type_get_contents */

static int
umpi_mpi_MPI_Type_get_contents_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_get_contents_pre( 
void * pc , MPI_Datatype datatype ,
int count ,
int addrcount ,
int dtypecount ,
int *array_of_integers ,
MPI_Aint *the_addresses ,
MPI_Datatype *the_datatypes )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_get_contents;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.count = count;
		uop->data.mpi.addrcount = addrcount;
		uop->data.mpi.dtypecount = dtypecount;
uop->data.mpi.array_of_integers = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_integers);
bcopy (array_of_integers, uop->data.mpi.array_of_integers, uop->data.mpi.count * sizeof (int));
uop->data.mpi.the_addresses = (MPI_Aint *) malloc (uop->data.mpi.addrcount * sizeof (MPI_Aint));
assert (uop->data.mpi.the_addresses);
bcopy (the_addresses, uop->data.mpi.the_addresses, uop->data.mpi.addrcount * sizeof (int));
uop->data.mpi.the_datatypes = (MPI_Datatype *) malloc (uop->data.mpi.dtypecount * sizeof (MPI_Datatype));
assert (uop->data.mpi.the_datatypes);
bcopy (the_datatypes, uop->data.mpi.the_datatypes, uop->data.mpi.dtypecount * sizeof (int));
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Type_get_contents_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Type_get_contents_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = NO_FIELDS;
  event_set_param_list(event, op->op, op->seq_num, NO_FIELDS);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  // probably should throw an error if we try to replay
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_get_contents_post( 
int MPI_rc, 
void * pc , MPI_Datatype datatype ,
int count ,
int addrcount ,
int dtypecount ,
int *array_of_integers ,
MPI_Aint *the_addresses ,
MPI_Datatype *the_datatypes )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_get_contents;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.datatype = datatype;
		uop->data.mpi.count = count;
		uop->data.mpi.addrcount = addrcount;
		uop->data.mpi.dtypecount = dtypecount;
uop->data.mpi.array_of_integers = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_integers);
bcopy (array_of_integers, uop->data.mpi.array_of_integers, uop->data.mpi.count * sizeof (int));
uop->data.mpi.the_addresses = (MPI_Aint *) malloc (uop->data.mpi.addrcount * sizeof (MPI_Aint));
assert (uop->data.mpi.the_addresses);
bcopy (the_addresses, uop->data.mpi.the_addresses, uop->data.mpi.addrcount * sizeof (int));
uop->data.mpi.the_datatypes = (MPI_Datatype *) malloc (uop->data.mpi.dtypecount * sizeof (MPI_Datatype));
assert (uop->data.mpi.the_datatypes);
bcopy (the_datatypes, uop->data.mpi.the_datatypes, uop->data.mpi.dtypecount * sizeof (int));
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Type_get_contents_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Type_get_contents( 
void * pc , MPI_Datatype datatype ,
int count ,
int addrcount ,
int dtypecount ,
int *array_of_integers ,
MPI_Aint *the_addresses ,
MPI_Datatype *the_datatypes )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Type_get_contents(  (datatype),
(count),
(addrcount),
(dtypecount),
(array_of_integers),
(the_addresses),
(the_datatypes));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Type_get_contents_pre(pc,
			datatype,
			count,
			addrcount,
			dtypecount,
			array_of_integers,
			the_addresses,
			the_datatypes);
	rc = PMPI_Type_get_contents(  (datatype),
(count),
(addrcount),
(dtypecount),
(array_of_integers),
(the_addresses),
(the_datatypes));

umpi_mpi_MPI_Type_get_contents_post(rc, pc,
			datatype,
			count,
			addrcount,
			dtypecount,
			array_of_integers,
			the_addresses,
			the_datatypes);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Type_get_contents(  MPI_Datatype datatype ,
		int count ,
		int addrcount ,
		int dtypecount ,
		int *array_of_integers ,
		MPI_Aint *the_addresses ,
		MPI_Datatype *the_datatypes )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Type_get_contents(pc,
			datatype,
			count,
			addrcount,
			dtypecount,
			array_of_integers,
			the_addresses,
			the_datatypes );
	return rc;
}

static void mpi_type_get_contents_f_wrap(MPI_Fint *datatype , MPI_Fint *count , MPI_Fint *addrcount , MPI_Fint *dtypecount , MPI_Fint *array_of_integers , MPI_Aint *the_addresses , MPI_Fint *the_datatypes , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Type_get_contents(pc ,
			(MPI_Datatype)(*(datatype)),
			*(count),
			*(addrcount),
			*(dtypecount),
			array_of_integers,
			the_addresses,
			(MPI_Datatype *)the_datatypes);
#else /* other mpi's need conversions */
		int temp_i;
		MPI_Datatype *temp_the_datatypes = (MPI_Datatype *)malloc(sizeof(MPI_Datatype) * *(count));
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			temp_the_datatypes[temp_i] = MPI_Type_f2c(the_datatypes[temp_i]);
		}
		rc = gwrap_MPI_Type_get_contents(pc ,
			MPI_Type_f2c(*(datatype)),
			*(count),
			*(addrcount),
			*(dtypecount),
			array_of_integers,
			the_addresses,
			temp_the_datatypes);
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			the_datatypes[temp_i] = MPI_Type_c2f(temp_the_datatypes[temp_i]);
		}
		free(temp_the_datatypes);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_TYPE_GET_CONTENTS(MPI_Fint *datatype , MPI_Fint *count , MPI_Fint *addrcount , MPI_Fint *dtypecount , MPI_Fint *array_of_integers , MPI_Aint *the_addresses , MPI_Fint *the_datatypes , MPI_Fint * ierr) { mpi_type_get_contents_f_wrap(datatype, count, addrcount, dtypecount, array_of_integers, the_addresses, the_datatypes, ierr); }
extern void mpi_type_get_contents(MPI_Fint *datatype , MPI_Fint *count , MPI_Fint *addrcount , MPI_Fint *dtypecount , MPI_Fint *array_of_integers , MPI_Aint *the_addresses , MPI_Fint *the_datatypes , MPI_Fint * ierr) { mpi_type_get_contents_f_wrap(datatype, count, addrcount, dtypecount, array_of_integers, the_addresses, the_datatypes, ierr); }
extern void mpi_type_get_contents_(MPI_Fint *datatype , MPI_Fint *count , MPI_Fint *addrcount , MPI_Fint *dtypecount , MPI_Fint *array_of_integers , MPI_Aint *the_addresses , MPI_Fint *the_datatypes , MPI_Fint * ierr) { mpi_type_get_contents_f_wrap(datatype, count, addrcount, dtypecount, array_of_integers, the_addresses, the_datatypes, ierr); }
extern void mpi_type_get_contents__(MPI_Fint *datatype , MPI_Fint *count , MPI_Fint *addrcount , MPI_Fint *dtypecount , MPI_Fint *array_of_integers , MPI_Aint *the_addresses , MPI_Fint *the_datatypes , MPI_Fint * ierr) { mpi_type_get_contents_f_wrap(datatype, count, addrcount, dtypecount, array_of_integers, the_addresses, the_datatypes, ierr); }

/*--------------------------------------------- MPI_Type_get_envelope */

static int
umpi_mpi_MPI_Type_get_envelope_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_get_envelope_pre( 
void * pc , MPI_Datatype datatype ,
int *num_integers ,
int *num_addresses ,
int *num_datatypes ,
int *combiner )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_get_envelope;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.num_integers = *(num_integers);
uop->data.mpi.num_addresses = *(num_addresses);
uop->data.mpi.num_datatypes = *(num_datatypes);
uop->data.mpi.combiner = *(combiner);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Type_get_envelope_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Type_get_envelope_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = NO_FIELDS;
  event_set_param_list(event, op->op, op->seq_num, NO_FIELDS);
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  // probably should throw an error if we try to replay
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_get_envelope_post( 
int MPI_rc, 
void * pc , MPI_Datatype datatype ,
int *num_integers ,
int *num_addresses ,
int *num_datatypes ,
int *combiner )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_get_envelope;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.num_integers = *(num_integers);
uop->data.mpi.num_addresses = *(num_addresses);
uop->data.mpi.num_datatypes = *(num_datatypes);
uop->data.mpi.combiner = *(combiner);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Type_get_envelope_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Type_get_envelope( 
void * pc , MPI_Datatype datatype ,
int *num_integers ,
int *num_addresses ,
int *num_datatypes ,
int *combiner )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Type_get_envelope(  (datatype),
(num_integers),
(num_addresses),
(num_datatypes),
(combiner));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Type_get_envelope_pre(pc,
			datatype,
			num_integers,
			num_addresses,
			num_datatypes,
			combiner);
	rc = PMPI_Type_get_envelope(  (datatype),
(num_integers),
(num_addresses),
(num_datatypes),
(combiner));

umpi_mpi_MPI_Type_get_envelope_post(rc, pc,
			datatype,
			num_integers,
			num_addresses,
			num_datatypes,
			combiner);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Type_get_envelope(  MPI_Datatype datatype ,
		int *num_integers ,
		int *num_addresses ,
		int *num_datatypes ,
		int *combiner )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Type_get_envelope(pc,
			datatype,
			num_integers,
			num_addresses,
			num_datatypes,
			combiner );
	return rc;
}

static void mpi_type_get_envelope_f_wrap(MPI_Fint *datatype , MPI_Fint *num_integers , MPI_Fint *num_addresses , MPI_Fint *num_datatypes , MPI_Fint *combiner , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Type_get_envelope(pc ,
			(MPI_Datatype)(*(datatype)),
			num_integers,
			num_addresses,
			num_datatypes,
			combiner);
#else /* other mpi's need conversions */
		rc = gwrap_MPI_Type_get_envelope(pc ,
			MPI_Type_f2c(*(datatype)),
			num_integers,
			num_addresses,
			num_datatypes,
			combiner);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_TYPE_GET_ENVELOPE(MPI_Fint *datatype , MPI_Fint *num_integers , MPI_Fint *num_addresses , MPI_Fint *num_datatypes , MPI_Fint *combiner , MPI_Fint * ierr) { mpi_type_get_envelope_f_wrap(datatype, num_integers, num_addresses, num_datatypes, combiner, ierr); }
extern void mpi_type_get_envelope(MPI_Fint *datatype , MPI_Fint *num_integers , MPI_Fint *num_addresses , MPI_Fint *num_datatypes , MPI_Fint *combiner , MPI_Fint * ierr) { mpi_type_get_envelope_f_wrap(datatype, num_integers, num_addresses, num_datatypes, combiner, ierr); }
extern void mpi_type_get_envelope_(MPI_Fint *datatype , MPI_Fint *num_integers , MPI_Fint *num_addresses , MPI_Fint *num_datatypes , MPI_Fint *combiner , MPI_Fint * ierr) { mpi_type_get_envelope_f_wrap(datatype, num_integers, num_addresses, num_datatypes, combiner, ierr); }
extern void mpi_type_get_envelope__(MPI_Fint *datatype , MPI_Fint *num_integers , MPI_Fint *num_addresses , MPI_Fint *num_datatypes , MPI_Fint *combiner , MPI_Fint * ierr) { mpi_type_get_envelope_f_wrap(datatype, num_integers, num_addresses, num_datatypes, combiner, ierr); }

/*--------------------------------------------- MPI_Type_create_darray */

static int
umpi_mpi_MPI_Type_create_darray_immediate_local_pre(umpi_op_t *op)
{

{
{
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifdef COM_COMM
	resetStats(event, PHASE_COMM);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_create_darray_pre( 
void * pc , int size ,
int rank ,
int count ,
const int array_of_gsizes [],
const int array_of_distribs [],
const int array_of_dargs [],
const int array_of_psizes [],
int order ,
MPI_Datatype datatype ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_create_darray;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.size = size;
		uop->data.mpi.rank = rank;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_gsizes = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_gsizes);
bcopy (array_of_gsizes, uop->data.mpi.array_of_gsizes, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_distribs = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_distribs);
bcopy (array_of_distribs, uop->data.mpi.array_of_distribs, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_dargs = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_dargs);
bcopy (array_of_dargs, uop->data.mpi.array_of_dargs, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_psizes = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_psizes);
bcopy (array_of_psizes, uop->data.mpi.array_of_psizes, uop->data.mpi.count * sizeof (int));
		uop->data.mpi.order = order;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Type_create_darray_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Type_create_darray_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifdef COM_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #endif
	addScalarValue(event, SIZE, op->data.mpi.size, my_rank);
	addScalarValue(event, RANK, op->data.mpi.rank, my_rank);
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	addScalarValue(event, ORDER, op->data.mpi.order, my_rank);
	addVectorValue(event, ARRAY_OF_GSIZES,op->data.mpi.count, op->data.mpi.array_of_gsizes, my_rank);
	addVectorValue(event, ARRAY_OF_DISTRIBS, op->data.mpi.count, op->data.mpi.array_of_distribs, my_rank);
	addVectorValue(event, ARRAY_OF_DARGS, op->data.mpi.count, op->data.mpi.array_of_dargs, my_rank);
	addVectorValue(event, ARRAY_OF_PSIZES, op->data.mpi.count, op->data.mpi.array_of_psizes, my_rank);
	addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
	add_type_entry(record_ptr, op->data.mpi.newtype);
	addScalarValue(event, NEWTYPE,type_to_index(record_ptr, op->data.mpi.newtype), my_rank);
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_create_darray_post( 
int MPI_rc, 
void * pc , int size ,
int rank ,
int count ,
const int array_of_gsizes [],
const int array_of_distribs [],
const int array_of_dargs [],
const int array_of_psizes [],
int order ,
MPI_Datatype datatype ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_create_darray;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.size = size;
		uop->data.mpi.rank = rank;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_gsizes = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_gsizes);
bcopy (array_of_gsizes, uop->data.mpi.array_of_gsizes, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_distribs = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_distribs);
bcopy (array_of_distribs, uop->data.mpi.array_of_distribs, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_dargs = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_dargs);
bcopy (array_of_dargs, uop->data.mpi.array_of_dargs, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_psizes = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_psizes);
bcopy (array_of_psizes, uop->data.mpi.array_of_psizes, uop->data.mpi.count * sizeof (int));
		uop->data.mpi.order = order;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Type_create_darray_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Type_create_darray( 
void * pc , int size ,
int rank ,
int count ,
const int array_of_gsizes [],
const int array_of_distribs [],
const int array_of_dargs [],
const int array_of_psizes [],
int order ,
MPI_Datatype datatype ,
MPI_Datatype *newtype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Type_create_darray(  (size),
(rank),
(count),
(array_of_gsizes),
(array_of_distribs),
(array_of_dargs),
(array_of_psizes),
(order),
(datatype),
(newtype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Type_create_darray_pre(pc,
			size,
			rank,
			count,
			array_of_gsizes,
			array_of_distribs,
			array_of_dargs,
			array_of_psizes,
			order,
			datatype,
			newtype);
	rc = PMPI_Type_create_darray(  (size),
(rank),
(count),
(array_of_gsizes),
(array_of_distribs),
(array_of_dargs),
(array_of_psizes),
(order),
(datatype),
(newtype));

umpi_mpi_MPI_Type_create_darray_post(rc, pc,
			size,
			rank,
			count,
			array_of_gsizes,
			array_of_distribs,
			array_of_dargs,
			array_of_psizes,
			order,
			datatype,
			newtype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Type_create_darray(  int size ,
		int rank ,
		int count ,
		const int array_of_gsizes [],
		const int array_of_distribs [],
		const int array_of_dargs [],
		const int array_of_psizes [],
		int order ,
		MPI_Datatype datatype ,
		MPI_Datatype *newtype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Type_create_darray(pc,
			size,
			rank,
			count,
			array_of_gsizes,
			array_of_distribs,
			array_of_dargs,
			array_of_psizes,
			order,
			datatype,
			newtype );
	return rc;
}

static void mpi_type_create_darray_f_wrap(MPI_Fint *size , MPI_Fint *rank , MPI_Fint *count , MPI_Fint array_of_gsizes [], MPI_Fint array_of_distribs [], MPI_Fint array_of_dargs [], MPI_Fint array_of_psizes [], MPI_Fint *order , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Type_create_darray(pc ,
			*(size),
			*(rank),
			*(count),
			array_of_gsizes,
			array_of_distribs,
			array_of_dargs,
			array_of_psizes,
			*(order),
			(MPI_Datatype)(*(datatype)),
			(MPI_Datatype *)newtype);
#else /* other mpi's need conversions */
		MPI_Datatype temp_newtype;
		temp_newtype = MPI_Type_f2c(*(newtype));
		rc = gwrap_MPI_Type_create_darray(pc ,
			*(size),
			*(rank),
			*(count),
			array_of_gsizes,
			array_of_distribs,
			array_of_dargs,
			array_of_psizes,
			*(order),
			MPI_Type_f2c(*(datatype)),
			&temp_newtype);
		*(newtype) = MPI_Type_c2f(temp_newtype);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_TYPE_CREATE_DARRAY(MPI_Fint *size , MPI_Fint *rank , MPI_Fint *count , MPI_Fint array_of_gsizes [], MPI_Fint array_of_distribs [], MPI_Fint array_of_dargs [], MPI_Fint array_of_psizes [], MPI_Fint *order , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_darray_f_wrap(size, rank, count, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, order, datatype, newtype, ierr); }
extern void mpi_type_create_darray(MPI_Fint *size , MPI_Fint *rank , MPI_Fint *count , MPI_Fint array_of_gsizes [], MPI_Fint array_of_distribs [], MPI_Fint array_of_dargs [], MPI_Fint array_of_psizes [], MPI_Fint *order , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_darray_f_wrap(size, rank, count, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, order, datatype, newtype, ierr); }
extern void mpi_type_create_darray_(MPI_Fint *size , MPI_Fint *rank , MPI_Fint *count , MPI_Fint array_of_gsizes [], MPI_Fint array_of_distribs [], MPI_Fint array_of_dargs [], MPI_Fint array_of_psizes [], MPI_Fint *order , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_darray_f_wrap(size, rank, count, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, order, datatype, newtype, ierr); }
extern void mpi_type_create_darray__(MPI_Fint *size , MPI_Fint *rank , MPI_Fint *count , MPI_Fint array_of_gsizes [], MPI_Fint array_of_distribs [], MPI_Fint array_of_dargs [], MPI_Fint array_of_psizes [], MPI_Fint *order , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_create_darray_f_wrap(size, rank, count, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, order, datatype, newtype, ierr); }

/*--------------------------------------------- MPI_Type_indexed */

static int
umpi_mpi_MPI_Type_indexed_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_indexed_pre( 
void * pc , int count ,
const int array_of_blocklens [],
const int array_of_int_displacements [],
MPI_Datatype datatype ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_indexed;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_blocklens = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_blocklens);
bcopy (array_of_blocklens, uop->data.mpi.array_of_blocklens, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_int_displacements = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_int_displacements);
bcopy (array_of_int_displacements, uop->data.mpi.array_of_int_displacements, uop->data.mpi.count * sizeof (int));
		uop->data.mpi.datatype = datatype;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Type_indexed_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Type_indexed_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  recordTime (&operation, 1);
  operation.fields = COUNT | DATATYPE  ;
  event_set_param_list(event, op->op, op->seq_num, COUNT | DATATYPE | ARRAY_OF_BLOCKLENS | DISPLS);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.arrays = ARRAY_OF_BLOCKLENS | DISPLS;
  operation.array1 = OP_ARRAY_ALLOC(op->data.mpi.count);
  operation.array_size1 = op->data.mpi.count;
  if(!operation.array1) {
    perror("malloc operation.array1");
    exit(0);
  }
  OP_ARRAY_COPY(operation.array1, op->data.mpi.array_of_blocklens, op->data.mpi.count);
  add_vector_param(event, TYPE_ARRAY_OF_BLOCKLENS, op->data.mpi.array_of_blocklens, op->data.mpi.count);
  operation.array2 = OP_ARRAY_ALLOC(op->data.mpi.count);
  operation.array_size2 = op->data.mpi.count;
  if(!operation.array2) {
    perror("malloc operation.array2");
    exit(0);
  }
  OP_ARRAY_COPY(operation.array2, op->data.mpi.array_of_int_displacements, op->data.mpi.count);
  add_vector_param(event, TYPE_DISPLS, op->data.mpi.array_of_int_displacements, op->data.mpi.count);
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  operation.datatype = type_to_index(record_ptr, op->data.mpi.datatype);
  add_scalar_param_int(event, TYPE_DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype));
  add_type_entry (record_ptr, op->data.mpi.newtype);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_indexed_post( 
int MPI_rc, 
void * pc , int count ,
const int array_of_blocklens [],
const int array_of_int_displacements [],
MPI_Datatype datatype ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_indexed;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_blocklens = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_blocklens);
bcopy (array_of_blocklens, uop->data.mpi.array_of_blocklens, uop->data.mpi.count * sizeof (int));
uop->data.mpi.array_of_int_displacements = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_int_displacements);
bcopy (array_of_int_displacements, uop->data.mpi.array_of_int_displacements, uop->data.mpi.count * sizeof (int));
		uop->data.mpi.datatype = datatype;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Type_indexed_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Type_indexed( 
void * pc , int count ,
const int array_of_blocklens [],
const int array_of_int_displacements [],
MPI_Datatype datatype ,
MPI_Datatype *newtype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Type_indexed(  (count),
(array_of_blocklens),
(array_of_int_displacements),
(datatype),
(newtype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Type_indexed_pre(pc,
			count,
			array_of_blocklens,
			array_of_int_displacements,
			datatype,
			newtype);
	rc = PMPI_Type_indexed(  (count),
(array_of_blocklens),
(array_of_int_displacements),
(datatype),
(newtype));

umpi_mpi_MPI_Type_indexed_post(rc, pc,
			count,
			array_of_blocklens,
			array_of_int_displacements,
			datatype,
			newtype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Type_indexed(  int count ,
		const int array_of_blocklens [],
		const int array_of_int_displacements [],
		MPI_Datatype datatype ,
		MPI_Datatype *newtype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Type_indexed(pc,
			count,
			array_of_blocklens,
			array_of_int_displacements,
			datatype,
			newtype );
	return rc;
}

static void mpi_type_indexed_f_wrap(MPI_Fint *count , MPI_Fint array_of_blocklens [], MPI_Fint array_of_int_displacements [], MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Type_indexed(pc ,
			*(count),
			array_of_blocklens,
			array_of_int_displacements,
			(MPI_Datatype)(*(datatype)),
			(MPI_Datatype *)newtype);
#else /* other mpi's need conversions */
		MPI_Datatype temp_newtype;
		temp_newtype = MPI_Type_f2c(*(newtype));
		rc = gwrap_MPI_Type_indexed(pc ,
			*(count),
			array_of_blocklens,
			array_of_int_displacements,
			MPI_Type_f2c(*(datatype)),
			&temp_newtype);
		*(newtype) = MPI_Type_c2f(temp_newtype);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_TYPE_INDEXED(MPI_Fint *count , MPI_Fint array_of_blocklens [], MPI_Fint array_of_int_displacements [], MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_indexed_f_wrap(count, array_of_blocklens, array_of_int_displacements, datatype, newtype, ierr); }
extern void mpi_type_indexed(MPI_Fint *count , MPI_Fint array_of_blocklens [], MPI_Fint array_of_int_displacements [], MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_indexed_f_wrap(count, array_of_blocklens, array_of_int_displacements, datatype, newtype, ierr); }
extern void mpi_type_indexed_(MPI_Fint *count , MPI_Fint array_of_blocklens [], MPI_Fint array_of_int_displacements [], MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_indexed_f_wrap(count, array_of_blocklens, array_of_int_displacements, datatype, newtype, ierr); }
extern void mpi_type_indexed__(MPI_Fint *count , MPI_Fint array_of_blocklens [], MPI_Fint array_of_int_displacements [], MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_indexed_f_wrap(count, array_of_blocklens, array_of_int_displacements, datatype, newtype, ierr); }

/*--------------------------------------------- MPI_Type_lb */

/*-------------------- Wrapper for MPI_Type_lb omitted */

/*--------------------------------------------- MPI_Type_size */

/*-------------------- Wrapper for MPI_Type_size omitted */

/*--------------------------------------------- MPI_Type_ub */

/*-------------------- Wrapper for MPI_Type_ub omitted */

/*--------------------------------------------- MPI_Type_vector */

static int
umpi_mpi_MPI_Type_vector_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op (&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_vector_pre( 
void * pc , int count ,
int blocklen ,
int stride ,
MPI_Datatype datatype ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_vector;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
		uop->data.mpi.blocklen = blocklen;
		uop->data.mpi.stride = stride;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Type_vector_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Type_vector_immediate_local_post(umpi_op_t *op)
{

{
{
/*
    operation.fields = COUNT | BLOCKLEN | STRIDE | DATATYPE ;
  event_set_param_list(event, op->op, op->seq_num, COUNT | BLOCKLEN | STRIDE | DATATYPE );
    operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  operation.blocklen = op->data.mpi.blocklen;
  add_scalar_param_int(event, TYPE_BLOCKLEN, op->data.mpi.blocklen);
  operation.stride = op->data.mpi.stride;
  add_scalar_param_int(event, TYPE_STRIDE, op->data.mpi.stride);
  operation.datatype = type_to_index(record_ptr, op->data.mpi.datatype);
  add_scalar_param_int(event, TYPE_DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype));
  add_type_entry (record_ptr, op->data.mpi.newtype);
  recordTime (&operation, 1);
  recordEventTime(event, 1);
  compress_rsd_event( &op_queue, &operation, event);
  recordEventTime(0,0);
recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Type_vector_post( 
int MPI_rc, 
void * pc , int count ,
int blocklen ,
int stride ,
MPI_Datatype datatype ,
MPI_Datatype *newtype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Type_vector;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
		uop->data.mpi.blocklen = blocklen;
		uop->data.mpi.stride = stride;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.newtype = *(newtype);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Type_vector_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Type_vector( 
void * pc , int count ,
int blocklen ,
int stride ,
MPI_Datatype datatype ,
MPI_Datatype *newtype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Type_vector(  (count),
(blocklen),
(stride),
(datatype),
(newtype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Type_vector_pre(pc,
			count,
			blocklen,
			stride,
			datatype,
			newtype);
	rc = PMPI_Type_vector(  (count),
(blocklen),
(stride),
(datatype),
(newtype));

umpi_mpi_MPI_Type_vector_post(rc, pc,
			count,
			blocklen,
			stride,
			datatype,
			newtype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Type_vector(  int count ,
		int blocklen ,
		int stride ,
		MPI_Datatype datatype ,
		MPI_Datatype *newtype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Type_vector(pc,
			count,
			blocklen,
			stride,
			datatype,
			newtype );
	return rc;
}

static void mpi_type_vector_f_wrap(MPI_Fint *count , MPI_Fint *blocklen , MPI_Fint *stride , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Type_vector(pc ,
			*(count),
			*(blocklen),
			*(stride),
			(MPI_Datatype)(*(datatype)),
			(MPI_Datatype *)newtype);
#else /* other mpi's need conversions */
		MPI_Datatype temp_newtype;
		temp_newtype = MPI_Type_f2c(*(newtype));
		rc = gwrap_MPI_Type_vector(pc ,
			*(count),
			*(blocklen),
			*(stride),
			MPI_Type_f2c(*(datatype)),
			&temp_newtype);
		*(newtype) = MPI_Type_c2f(temp_newtype);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_TYPE_VECTOR(MPI_Fint *count , MPI_Fint *blocklen , MPI_Fint *stride , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_vector_f_wrap(count, blocklen, stride, datatype, newtype, ierr); }
extern void mpi_type_vector(MPI_Fint *count , MPI_Fint *blocklen , MPI_Fint *stride , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_vector_f_wrap(count, blocklen, stride, datatype, newtype, ierr); }
extern void mpi_type_vector_(MPI_Fint *count , MPI_Fint *blocklen , MPI_Fint *stride , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_vector_f_wrap(count, blocklen, stride, datatype, newtype, ierr); }
extern void mpi_type_vector__(MPI_Fint *count , MPI_Fint *blocklen , MPI_Fint *stride , MPI_Fint *datatype , MPI_Fint *newtype , MPI_Fint * ierr) { mpi_type_vector_f_wrap(count, blocklen, stride, datatype, newtype, ierr); }

/*--------------------------------------------- MPI_Unpack */

/*-------------------- Wrapper for MPI_Unpack omitted */

/*--------------------------------------------- MPI_Wait */

static int
umpi_mpi_MPI_Wait_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifndef FEATURE_PARAM_HISTO
	int offset;
	lookup_offset(record_req, op->data.mpi.request, &offset);
	addScalarValue(event, REQUEST, offset, my_rank);
  #endif
	resetStats(event, PHASE_COMM);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Wait_pre( 
void * pc , MPI_Request *request ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Wait;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.request = *(request);
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Wait_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Wait_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #ifndef FEATURE_PARAM_HISTO
	int *offsets;
	getParamValues(event, REQUEST, 0, NULL, &offsets);
	if(op->data.mpi.request == MPI_REQUEST_NULL){
		reset_offset(record_req, offsets[0]);
	}
	free(offsets);
  #endif
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Wait_post( 
int MPI_rc, 
void * pc , MPI_Request *request ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Wait;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.request = *(request);
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Wait_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Wait( 
void * pc , MPI_Request *request ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Wait(  (request),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Wait_pre(pc,
			request,
			status);
	rc = PMPI_Wait(  (request),
(status));

umpi_mpi_MPI_Wait_post(rc, pc,
			request,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Wait(  MPI_Request *request ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Wait(pc,
			request,
			status );
	return rc;
}

static void mpi_wait_f_wrap(MPI_Fint *request , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Wait(pc ,
			(MPI_Request *)request,
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		MPI_Status temp_status;
		temp_request = MPI_Request_f2c(*(request));
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_Wait(pc ,
			&temp_request,
			&temp_status);
		*(request) = MPI_Request_c2f(temp_request);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_WAIT(MPI_Fint *request , MPI_Fint *status , MPI_Fint * ierr) { mpi_wait_f_wrap(request, status, ierr); }
extern void mpi_wait(MPI_Fint *request , MPI_Fint *status , MPI_Fint * ierr) { mpi_wait_f_wrap(request, status, ierr); }
extern void mpi_wait_(MPI_Fint *request , MPI_Fint *status , MPI_Fint * ierr) { mpi_wait_f_wrap(request, status, ierr); }
extern void mpi_wait__(MPI_Fint *request , MPI_Fint *status , MPI_Fint * ierr) { mpi_wait_f_wrap(request, status, ierr); }

/*--------------------------------------------- MPI_Waitall */

static int
umpi_mpi_MPI_Waitall_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
  #ifndef FEATURE_PARAM_HISTO
	addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
	int *offsets = (int *)malloc(op->data.mpi.count * sizeof(int) );
	if( !offsets ){
		fprintf(stderr, "malloc failed, offsets\n");
		exit(1);
	}
	lookup_offsetlist(record_req, op->data.mpi.array_of_requests, op->data.mpi.count, offsets, 0);
	addVectorValue(event, ARRAY_OF_REQUESTS, op->data.mpi.count, offsets, my_rank);
	free(offsets);
  #endif
	resetStats(event, PHASE_COMM);
  #endif
 #ifdef PRINT_POTENTIAL_MARKER
  if (my_rank==0){
 	printf("#### Chameleon: Potential Marker => MPI_Waitall\n");
 }
 #endif 
 #ifdef CHECK_WAITALL_MARKER
	PMPI_Barrier (MPI_COMM_WORLD);
 #endif 
 #ifdef ONLINE_CLUSTERING
   #ifdef AUTOMATIC_MARKER
   #ifdef WAITALL_MARKER
	if (my_rank == 0){
		printf("#### Chameleon: <==================== Automatic Marker (MPI_WAITALL) ===============>\n");
	}
	if(getenv("K_CLUSTERS")) {
	  // K_CLUSTER is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: ###################################################\n");
			printf("#### Chameleon: Please set K_CLUSTERS (e.g., ~: export K_CLUSTERS=9)\n");
			printf("#### Chameleon: ###################################################\n\n");
		}
		exit(0);
	}
	if(getenv("MARKER_CALL_FREQ")) {
	  // Number of Calls is set
	} else {
		if(my_rank==0){
			printf("\n\n#### Chameleon: #################################################################\n");
			printf("#### Chameleon: Please set MARKER_CALL_FREQ (e.g., ~: export MARKER_CALL_FREQ=1)\n");
			printf("#### Chameleon: #################################################################\n\n");
		}
		exit(0);
	}
	if(my_rank==0){
		printf("#### Chameleon: Start of Marker - Overhead reported by Rank %d\n", my_rank);
		startTime(&trace);
	}
	//  Check whether clustering is required or not, if so remove old clusters
	//  1) also if there oldCallPath is "Zero"! initial value, set the value an ret 1
	//  2) If OldCallPath is different from the current version, then 
	//	2.1) delete old cluster
	//	2.2) init a new cluster, and prepare for clustering
	int TIMER=0;
	if(getNoClusteringFlag(&trace)==1 && onlyHeadCluster(&trace, my_rank)==-1)
		TIMER=1;
	clock_t	t;
	if(my_rank==0)
		t = clock();
	long int NumberCalls = atol(getenv("MARKER_CALL_FREQ"));
	int CLUSTERING = matchSignatures(&trace, my_rank, my_size, NumberCalls);
//	printf("\n<-------------My Rank: %d Clustering:%d ---------------------> \n", my_rank, CLUSTERING); 
	//1) Find Signature Format
	//2) Apply Clustering
	//3) Merge Clustering Representatives
	//4) Simply go over all events and replace existing nodes and add new events
	//print only events w/ flags up!
	//make the flags off
	//if(getPRSDClusterFlag(&trace))
	//-1 is for the case there is only one or no event
	if(my_rank==0){
		if (CLUSTERING == -4){
			printf("\n#### Chameleon: <---------------------- re-Clustering ---------------------> \n"); 
		}
		else if(CLUSTERING == 0){
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering ---------------------> \n"); 
		}
		else if (CLUSTERING>0) {
			printf("\n#### Chameleon: <---------------------- Waiting Phase ---------------------> \n"); 
		}
		else if (CLUSTERING == -1) {
			printf("\n#### Chameleon: <---------------------- No need for re-Clustering (No new events for reclustering) ---------------------> \n"); 
		}
		else{
			printf("\n#### Chameleon: <---------------------- ERROR: Out of Range Case ---------------------> \n"); 
			return;
		}
	}
	//add new and modified events to the partial trace 	
	//IGNORE: NOt a good idea; the size of events goes up, so everytime we  must merge 
	//the full event which increases the overhead 
	setOFFOnlineClusteringFlags(&trace, my_rank);
	if(CLUSTERING ==-4 || CLUSTERING==0)  
//	if(CLUSTERING >0 || CLUSTERING==0)  
	{
		resetNumEvents(&trace, my_rank);
		if(CLUSTERING == -4)
		//if(CLUSTERING > 0)
		{
			//CALL_PATH Signature	
			setClusteringType(&trace, 0 ); 	
			int K = atoi(getenv("K_CLUSTERS"));
			//*****************Find Cluster Format******************
			int Clchild = get_child(my_rank, LEFT); 
        		if(Clchild < my_size) {
				recvClusterFormat(&trace, Clchild, my_rank); 
		    	}
        		int Crchild = get_child(my_rank, RIGHT);
		        if(Crchild < my_size) {
				recvClusterFormat(&trace, Crchild, my_rank);
  		  	}
		        if(my_rank != 0) {
        	      		int Cparent = get_parent(my_rank);
		                sendClusterFormat(&trace, Cparent);
        		}
			BcastClusterFormat(&trace, my_rank);
			//****************Done with Cluster Format******************
			initSigClusters(&trace);
			//***************** Set K ******************
			setK(&trace, K); // atoi(K_Clusters));
			setLevel(&trace, 0); // non-hierarchical=0; // Hirarchical => level one (Cal-Path), and level two (Other sigs) 
			//******************Start Merging Clusters*****************
			int Lchild = get_child(my_rank, LEFT); 
		        if(Lchild < my_size) {
				recvSigClusters(&trace, Lchild); 
				recvKClusters(&trace, Lchild,-1); // "LEFT"
                		mergeSigClusters(&trace);
	    		}
        		int Rchild = get_child(my_rank, RIGHT);
		        if(Rchild < my_size) {
				recvSigClusters(&trace, Rchild);
				recvKClusters(&trace, Rchild, -2); //, Rchild,-2); // "RIGHT"
                		mergeSigClusters(&trace);
		    	}
			if (Rchild < my_size || Lchild < my_size)
			{
				mergeKClusters(&trace, my_rank);
			}
        		if(my_rank != 0) {
        	      		int parent = get_parent(my_rank);
	                	sendSigClusters(&trace, parent);
				sendKClusters(&trace, parent);
		       	 } else {
        		        char path[1024];
                		sprintf(path, "trace_%d/", my_size);//, my_rank); 
	               	// outputSigClusters(&trace, path); 
        		}
			//*****************Done Merging Clusters******************
			BcastKCluster(&trace, my_rank);
			//*Broadcast ranks of K representatives
			// and prepare for mereging representative traces!*
			initSigClusters_L2(&trace, my_rank, 3);
		}// end of re-Clustering
		if(my_rank==0){
			t = clock() - t;
			printf ("#### Chameleon: Clustering Elapsed Time %d clicks (%f seconds).\n", (int)t,((float)t)/CLOCKS_PER_SEC);
		}
		//Over another radix tree merge all representative traces
		if((onlyHeadCluster(&trace, my_rank))!=-1)
		{
			//offNoClusteringFlag(&trace);
			time_t rawtime;
			printf("#### Chameleon: selected rank is : %d\n", my_rank);
			//Update ranklist so that it covers all the memebers of your cluster
			updateTraceRanklist(&trace, my_rank);
			//initSigClusters_L2(&trace, my_rank, 3);
			char path[1024];
			//Find your rank in the tree of representatives
			int temp_rank = findUnrolledRank(&trace, my_rank);	
			int temp_size = findUnrolledSize(&trace);
			//printf("temp rank is : %d\n", temp_rank);
			//printf("temp size is : %d\n", temp_size);
			int Clchild =get_child(temp_rank, LEFT); 
		        if(Clchild < temp_size) {
				Clchild =  convertUnrolledRank(&trace, Clchild);	
				recvTrace(&left_trace, Clchild);
	        	        mergeTrace(&trace, &left_trace);
				//NEW
				deleteTrace(&left_trace);
			}
	        	int Crchild = get_child(temp_rank, RIGHT);
	       		if(Crchild < temp_size) {
				Crchild =  convertUnrolledRank(&trace, Crchild);	
				recvTrace(&right_trace, Crchild);
	                	mergeTrace(&trace, &right_trace);
				//NEW
				deleteTrace(&right_trace);
			}
		        if(temp_rank != 0) {
        		        int Cparent = convertUnrolledRank(&trace, get_parent(temp_rank));
                		sendTrace(&trace, Cparent);
			}
		}
		//if(CLUSTERING>0) // Only save tail event of the new sequnce otherwise, keep the old one which is the start of a loop 
			saveLastTail(&trace);
		if ((convertUnrolledRank(&trace, 0) == my_rank) || my_rank==0 ){ //&& CLUSTERING!=0){
			// merge the output with ONLINE_tace stored in node 0
			if(convertUnrolledRank(&trace, 0) == 0) { // || convertUnrolledRank(&trace, 1) == my_rank){
				//printf("<============= Merging Rank is the same as Node 0 ========>\n\n ");
				if (convertUnrolledRank(&trace, 0) == my_rank){
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				 	copyTrace(&trace, &OnlineTrace);
				}
			}
			else 
			{//if (my_rank == 0 || my_rank == convertUnrolledRank(&trace, 0)){
				if (my_rank == 0 ){
				//	printf("<============= Merging Rank is different from Node 0 -- Head Node: %d ========>\n\n ", convertUnrolledRank(&trace, 0));
					deleteTrace(&trace);
				//	printf("<============= Delete is different from Node 0 ========>\n\n ");
					recvTrace(&trace, convertUnrolledRank(&trace, 0));
				//	printf("<============= Receive is different from Node 0 ========>\n\n ");
	                               	mergeOnlineTrace(&trace, &OnlineTrace, CLUSTERING);
				//	printf("<============= merge Online is different from Node 0 ========>\n\n ");
					copyTrace(&trace, &OnlineTrace);
				//	printf("<============= Copy is different from Node 0 ========>\n\n ");
				}
				else{
				//	printf("My rank is *********: %d SENDING \n ", my_rank);
					time_t rawtime;
                			sendTrace(&trace, 0);
	                               // char path[1024];
                	               // sprintf(path, "trace_%d/CLUSTER%d_%d", my_size, my_rank, ctime(&rawtime)); // temp_rank);
                        	       // outputTrace(&trace, path);
				}
			}
		}	
		//cleanup internal traces
		deleteTrace(&trace);
	}
	if(my_rank==0){
		//printf("End of Marker - Overhead reported by Rank %d\n", my_rank);
		endTime(&trace);
	}
	PMPI_Barrier (MPI_COMM_WORLD);
   #endif
  #endif
 #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Waitall_pre( 
void * pc , int count ,
MPI_Request array_of_requests [],
MPI_Status array_of_statuses [])

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Waitall;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_requests = (MPI_Request *) malloc (uop->data.mpi.count * sizeof (MPI_Request));
assert (uop->data.mpi.array_of_requests);
bcopy (array_of_requests, uop->data.mpi.array_of_requests, uop->data.mpi.count * sizeof (MPI_Request));
uop->data.mpi.statuses_srcs = (int *) malloc (uop->data.mpi.count * sizeof(int));
assert(uop->data.mpi.statuses_srcs);
{
int i;
for(i=0;i<uop->data.mpi.count;i++) {
uop->data.mpi.statuses_srcs[i] = array_of_statuses[i].MPI_SOURCE;
}
}
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Waitall_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Waitall_immediate_local_post(umpi_op_t *op)
{

{
{
  #ifndef FEATURE_SKIP_COMM
	recordStats(event, tailEvent(&trace), PHASE_COMM);
  #ifndef FEATURE_PARAM_HISTO
	int *offsets;
	getParamValues(event, ARRAY_OF_REQUESTS, 0, NULL, &offsets);
	for (int i = 0; i < op->data.mpi.count; i++) {
		if (op->data.mpi.array_of_requests[i] == MPI_REQUEST_NULL){
			reset_offset (record_req, offsets[i]);
		}
	}
	free(offsets);
  #endif
	appendEvent(&trace, event);
	resetStats(headEvent(&trace), PHASE_COMP);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_Waitall_post( 
int MPI_rc, 
void * pc , int count ,
MPI_Request array_of_requests [],
MPI_Status array_of_statuses [])

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Waitall;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_requests = (MPI_Request *) malloc (uop->data.mpi.count * sizeof (MPI_Request));
assert (uop->data.mpi.array_of_requests);
bcopy (array_of_requests, uop->data.mpi.array_of_requests, uop->data.mpi.count * sizeof (MPI_Request));
uop->data.mpi.statuses_srcs = (int *) malloc (uop->data.mpi.count * sizeof(int));
assert(uop->data.mpi.statuses_srcs);
{
int i;
for(i=0;i<uop->data.mpi.count;i++) {
uop->data.mpi.statuses_srcs[i] = array_of_statuses[i].MPI_SOURCE;
}
}
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Waitall_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Waitall( 
void * pc , int count ,
MPI_Request array_of_requests [],
MPI_Status array_of_statuses [])
{
int rc;
	int free_temp_statuses = 0;
	if (array_of_statuses == MPI_STATUSES_IGNORE) {
		array_of_statuses = (MPI_Status*)malloc(sizeof(MPI_Status) * count);
		free_temp_statuses = 1;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Waitall(  (count),
(array_of_requests),
(array_of_statuses));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Waitall_pre(pc,
			count,
			array_of_requests,
			array_of_statuses);
	rc = PMPI_Waitall(  (count),
(array_of_requests),
(array_of_statuses));

umpi_mpi_MPI_Waitall_post(rc, pc,
			count,
			array_of_requests,
			array_of_statuses);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	if (free_temp_statuses) {
		free(array_of_statuses);
	}
	return rc;
}

extern int  MPI_Waitall(  int count ,
		MPI_Request array_of_requests [],
		MPI_Status array_of_statuses [])
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Waitall(pc,
			count,
			array_of_requests,
			array_of_statuses );
	return rc;
}

static void mpi_waitall_f_wrap(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint array_of_statuses [], MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Waitall(pc ,
			*(count),
			(MPI_Request *)array_of_requests,
			(MPI_Status *)array_of_statuses);
#else /* other mpi's need conversions */
		int temp_i;
		MPI_Request *temp_array_of_requests = (MPI_Request *)malloc(sizeof(MPI_Request) * *(count));
		MPI_Status *temp_array_of_statuses = (MPI_Status *)malloc(sizeof(MPI_Status) * *(count));
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			temp_array_of_requests[temp_i] = MPI_Request_f2c(array_of_requests[temp_i]);
		}
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			PMPI_Status_f2c(&array_of_statuses[temp_i], &temp_array_of_statuses[temp_i]);
		}
		rc = gwrap_MPI_Waitall(pc ,
			*(count),
			temp_array_of_requests,
			temp_array_of_statuses);
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			array_of_requests[temp_i] = MPI_Request_c2f(temp_array_of_requests[temp_i]);
		}
		free(temp_array_of_requests);
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			PMPI_Status_c2f(&temp_array_of_statuses[temp_i], &array_of_statuses[temp_i]);
		}
		free(temp_array_of_statuses);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_WAITALL(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint array_of_statuses [], MPI_Fint * ierr) { mpi_waitall_f_wrap(count, array_of_requests, array_of_statuses, ierr); }
extern void mpi_waitall(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint array_of_statuses [], MPI_Fint * ierr) { mpi_waitall_f_wrap(count, array_of_requests, array_of_statuses, ierr); }
extern void mpi_waitall_(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint array_of_statuses [], MPI_Fint * ierr) { mpi_waitall_f_wrap(count, array_of_requests, array_of_statuses, ierr); }
extern void mpi_waitall__(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint array_of_statuses [], MPI_Fint * ierr) { mpi_waitall_f_wrap(count, array_of_requests, array_of_statuses, ierr); }

/*--------------------------------------------- MPI_Waitany */

static int
umpi_mpi_MPI_Waitany_immediate_local_pre(umpi_op_t *op)
{

{
{ 
/* 
  init_op(&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
  operation.fields = COUNT;
  event_set_param_list(event, op->op, op->seq_num, COUNT | ARRAY_OF_REQUESTS);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.arrays = ARRAY_OF_REQUESTS;
  operation.array1 = OP_ARRAY_ALLOC(op->data.mpi.count);
  operation.array_size1 = op->data.mpi.count;
  if(!operation.array1) {
    perror("malloc operation.array1");
    exit(0);
  }
  lookup_offsetlist (record_req, op->data.mpi.array_of_requests, operation.array_size1, operation.array1, 0);
  int *req_arr = OP_ARRAY_ALLOC(op->data.mpi.count);
  lookup_offsetlist(record_req, op->data.mpi.array_of_requests, op->data.mpi.count, req_arr, 0);
  add_vector_param(event, TYPE_ARRAY_OF_REQUESTS, req_arr, op->data.mpi.count);
  free(req_arr);
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Waitany_pre( 
void * pc , int count ,
MPI_Request array_of_requests [],
int *index ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Waitany;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_requests = (MPI_Request *) malloc (uop->data.mpi.count * sizeof (MPI_Request));
assert (uop->data.mpi.array_of_requests);
bcopy (array_of_requests, uop->data.mpi.array_of_requests, uop->data.mpi.count * sizeof (MPI_Request));
uop->data.mpi.index = *(index);
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Waitany_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Waitany_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  compress_rsd_event( &op_queue, &operation, event);
  int * arrreq = OP_ARRAY_ALLOC(op->data.mpi.count);
  event_get_param(event, TYPE_ARRAY_OF_REQUESTS, (void *)arrreq);
  int i = op->data.mpi.index;
  recordTime (&operation, 1);
  recordEventTime(event, 1);
    if (op->data.mpi.array_of_requests[i] == MPI_REQUEST_NULL)
      reset_offset (record_req, arrreq[i]);
  free(arrreq);
  recordEventTime(0,0);
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Waitany_post( 
int MPI_rc, 
void * pc , int count ,
MPI_Request array_of_requests [],
int *index ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Waitany;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_requests = (MPI_Request *) malloc (uop->data.mpi.count * sizeof (MPI_Request));
assert (uop->data.mpi.array_of_requests);
bcopy (array_of_requests, uop->data.mpi.array_of_requests, uop->data.mpi.count * sizeof (MPI_Request));
uop->data.mpi.index = *(index);
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Waitany_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Waitany( 
void * pc , int count ,
MPI_Request array_of_requests [],
int *index ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Waitany(  (count),
(array_of_requests),
(index),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Waitany_pre(pc,
			count,
			array_of_requests,
			index,
			status);
	rc = PMPI_Waitany(  (count),
(array_of_requests),
(index),
(status));

umpi_mpi_MPI_Waitany_post(rc, pc,
			count,
			array_of_requests,
			index,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_Waitany(  int count ,
		MPI_Request array_of_requests [],
		int *index ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Waitany(pc,
			count,
			array_of_requests,
			index,
			status );
	return rc;
}

static void mpi_waitany_f_wrap(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint *index , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Waitany(pc ,
			*(count),
			(MPI_Request *)array_of_requests,
			index,
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		int temp_i;
		MPI_Request *temp_array_of_requests = (MPI_Request *)malloc(sizeof(MPI_Request) * *(count));
		MPI_Status temp_status;
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			temp_array_of_requests[temp_i] = MPI_Request_f2c(array_of_requests[temp_i]);
		}
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_Waitany(pc ,
			*(count),
			temp_array_of_requests,
			index,
			&temp_status);
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			array_of_requests[temp_i] = MPI_Request_c2f(temp_array_of_requests[temp_i]);
		}
		free(temp_array_of_requests);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_WAITANY(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint *index , MPI_Fint *status , MPI_Fint * ierr) { mpi_waitany_f_wrap(count, array_of_requests, index, status, ierr); }
extern void mpi_waitany(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint *index , MPI_Fint *status , MPI_Fint * ierr) { mpi_waitany_f_wrap(count, array_of_requests, index, status, ierr); }
extern void mpi_waitany_(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint *index , MPI_Fint *status , MPI_Fint * ierr) { mpi_waitany_f_wrap(count, array_of_requests, index, status, ierr); }
extern void mpi_waitany__(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint *index , MPI_Fint *status , MPI_Fint * ierr) { mpi_waitany_f_wrap(count, array_of_requests, index, status, ierr); }

/*--------------------------------------------- MPI_Waitsome */

static int
umpi_mpi_MPI_Waitsome_immediate_local_pre(umpi_op_t *op)
{

{
{
/*
  init_op(&operation);
  init_event(event);
  recordEventTime(event, 0);
  recordTime (&operation, 0);
  operation.fields = COUNT;
  event_set_param_list(event, op->op, op->seq_num, COUNT | ARRAY_OF_REQUESTS);
  operation.op = op->op;
  operation.seq_num = op->seq_num;
  operation.arrays = ARRAY_OF_REQUESTS;
  operation.array1 = OP_ARRAY_ALLOC(op->data.mpi.count);
  operation.array_size1 = op->data.mpi.count;
  if(!operation.array1) {
    perror("malloc operation.array1");
    exit(0);
  }
  lookup_offsetlist (record_req, op->data.mpi.array_of_requests, operation.array_size1, operation.array1, 0);
  int *req_arr = OP_ARRAY_ALLOC(op->data.mpi.count);
  lookup_offsetlist(record_req, op->data.mpi.array_of_requests, op->data.mpi.count, req_arr, 0);
  add_vector_param(event, TYPE_ARRAY_OF_REQUESTS, req_arr, op->data.mpi.count);
  free(req_arr);
  operation.count = op->data.mpi.count;
  add_scalar_param_int(event, TYPE_COUNT, op->data.mpi.count);
  //TODO: Take care of special case
  if(op_queue.last_op_added != umpi_MPI_Waitsome) {
    operation.fields |= NULL_REQ; 
    rsd_push(&op_queue, &operation, event);     
  } else {
    free(operation.array1);
  }
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Waitsome_pre( 
void * pc , int count ,
MPI_Request array_of_requests [],
int *outcount ,
int array_of_indices [],
MPI_Status array_of_statuses [])

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Waitsome;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_requests = (MPI_Request *) malloc (uop->data.mpi.count * sizeof (MPI_Request));
assert (uop->data.mpi.array_of_requests);
bcopy (array_of_requests, uop->data.mpi.array_of_requests, uop->data.mpi.count * sizeof (MPI_Request));
uop->data.mpi.outcount = *(outcount);
uop->data.mpi.array_of_indices = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_indices);
bcopy (array_of_indices, uop->data.mpi.array_of_indices, uop->data.mpi.count * sizeof (int));
uop->data.mpi.statuses_srcs = (int *) malloc (uop->data.mpi.count * sizeof(int));
assert(uop->data.mpi.statuses_srcs);
{
int i;
for(i=0;i<uop->data.mpi.count;i++) {
uop->data.mpi.statuses_srcs[i] = array_of_statuses[i].MPI_SOURCE;
}
}
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_Waitsome_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_Waitsome_immediate_local_post(umpi_op_t *op)
{

{
{
/*
  recordTime (&operation, 1);
  if(op_queue.tail->data.fields & NULL_REQ) {
    op_queue.tail->data.outcount = op->data.mpi.outcount;
    op_queue.tail->data.fields &= ~NULL_REQ;
  }
  else {
    op_queue.tail->data.outcount += op->data.mpi.outcount;
  }
  int * arrreq = OP_ARRAY_ALLOC(op->data.mpi.count);
  event_get_param(event, TYPE_ARRAY_OF_REQUESTS, (void *)arrreq);
  for (int i = 0; i < op->data.mpi.count; i++)
  {
    if (op->data.mpi.array_of_requests[i] == MPI_REQUEST_NULL)
      reset_offset (record_req, arrreq[i]);
  }
  free(arrreq);
  recordTime (0, 0);
*/
}
}
return 0;
}


static int
umpi_mpi_MPI_Waitsome_post( 
int MPI_rc, 
void * pc , int count ,
MPI_Request array_of_requests [],
int *outcount ,
int array_of_indices [],
MPI_Status array_of_statuses [])

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_Waitsome;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.count = count;
uop->data.mpi.array_of_requests = (MPI_Request *) malloc (uop->data.mpi.count * sizeof (MPI_Request));
assert (uop->data.mpi.array_of_requests);
bcopy (array_of_requests, uop->data.mpi.array_of_requests, uop->data.mpi.count * sizeof (MPI_Request));
uop->data.mpi.outcount = *(outcount);
uop->data.mpi.array_of_indices = (int *) malloc (uop->data.mpi.count * sizeof (int));
assert (uop->data.mpi.array_of_indices);
bcopy (array_of_indices, uop->data.mpi.array_of_indices, uop->data.mpi.count * sizeof (int));
uop->data.mpi.statuses_srcs = (int *) malloc (uop->data.mpi.count * sizeof(int));
assert(uop->data.mpi.statuses_srcs);
{
int i;
for(i=0;i<uop->data.mpi.count;i++) {
uop->data.mpi.statuses_srcs[i] = array_of_statuses[i].MPI_SOURCE;
}
}
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_Waitsome_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_Waitsome( 
void * pc , int count ,
MPI_Request array_of_requests [],
int *outcount ,
int array_of_indices [],
MPI_Status array_of_statuses [])
{
int rc;
	int free_temp_statuses = 0;
	if (array_of_statuses == MPI_STATUSES_IGNORE) {
		array_of_statuses = (MPI_Status*)malloc(sizeof(MPI_Status) * count);
		free_temp_statuses = 1;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_Waitsome(  (count),
(array_of_requests),
(outcount),
(array_of_indices),
(array_of_statuses));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_Waitsome_pre(pc,
			count,
			array_of_requests,
			outcount,
			array_of_indices,
			array_of_statuses);
	rc = PMPI_Waitsome(  (count),
(array_of_requests),
(outcount),
(array_of_indices),
(array_of_statuses));

umpi_mpi_MPI_Waitsome_post(rc, pc,
			count,
			array_of_requests,
			outcount,
			array_of_indices,
			array_of_statuses);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	if (free_temp_statuses) {
		free(array_of_statuses);
	}
	return rc;
}

extern int  MPI_Waitsome(  int count ,
		MPI_Request array_of_requests [],
		int *outcount ,
		int array_of_indices [],
		MPI_Status array_of_statuses [])
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_Waitsome(pc,
			count,
			array_of_requests,
			outcount,
			array_of_indices,
			array_of_statuses );
	return rc;
}

static void mpi_waitsome_f_wrap(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint *outcount , MPI_Fint array_of_indices [], MPI_Fint array_of_statuses [], MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_Waitsome(pc ,
			*(count),
			(MPI_Request *)array_of_requests,
			outcount,
			array_of_indices,
			(MPI_Status *)array_of_statuses);
#else /* other mpi's need conversions */
		int temp_i;
		MPI_Request *temp_array_of_requests = (MPI_Request *)malloc(sizeof(MPI_Request) * *(count));
		MPI_Status *temp_array_of_statuses = (MPI_Status *)malloc(sizeof(MPI_Status) * *(count));
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			temp_array_of_requests[temp_i] = MPI_Request_f2c(array_of_requests[temp_i]);
		}
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			PMPI_Status_f2c(&array_of_statuses[temp_i], &temp_array_of_statuses[temp_i]);
		}
		rc = gwrap_MPI_Waitsome(pc ,
			*(count),
			temp_array_of_requests,
			outcount,
			array_of_indices,
			temp_array_of_statuses);
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			array_of_requests[temp_i] = MPI_Request_c2f(temp_array_of_requests[temp_i]);
		}
		free(temp_array_of_requests);
		for (temp_i = 0; temp_i < *(count); temp_i++) {
			PMPI_Status_c2f(&temp_array_of_statuses[temp_i], &array_of_statuses[temp_i]);
		}
		free(temp_array_of_statuses);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_WAITSOME(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint *outcount , MPI_Fint array_of_indices [], MPI_Fint array_of_statuses [], MPI_Fint * ierr) { mpi_waitsome_f_wrap(count, array_of_requests, outcount, array_of_indices, array_of_statuses, ierr); }
extern void mpi_waitsome(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint *outcount , MPI_Fint array_of_indices [], MPI_Fint array_of_statuses [], MPI_Fint * ierr) { mpi_waitsome_f_wrap(count, array_of_requests, outcount, array_of_indices, array_of_statuses, ierr); }
extern void mpi_waitsome_(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint *outcount , MPI_Fint array_of_indices [], MPI_Fint array_of_statuses [], MPI_Fint * ierr) { mpi_waitsome_f_wrap(count, array_of_requests, outcount, array_of_indices, array_of_statuses, ierr); }
extern void mpi_waitsome__(MPI_Fint *count , MPI_Fint array_of_requests [], MPI_Fint *outcount , MPI_Fint array_of_indices [], MPI_Fint array_of_statuses [], MPI_Fint * ierr) { mpi_waitsome_f_wrap(count, array_of_requests, outcount, array_of_indices, array_of_statuses, ierr); }

/*--------------------------------------------- MPI_Wtick */

/*-------------------- Wrapper for MPI_Wtick omitted */

/*--------------------------------------------- MPI_Wtime */

/*-------------------- Wrapper for MPI_Wtime omitted */

/*--------------------------------------------- fcntl */

/*-------------------- Wrapper for fcntl omitted */

/*--------------------------------------------- dup */

/*-------------------- Wrapper for dup omitted */

/*--------------------------------------------- dup2 */

/*-------------------- Wrapper for dup2 omitted */

/*--------------------------------------------- MPI_File_open */

static int
umpi_mpi_MPI_File_open_immediate_local_pre(umpi_op_t *op)
{

{
{
  #ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	resetStats(event, PHASE_COMM);
  #endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_open_pre( 
void * pc , MPI_Comm comm ,
const char *filename ,
int amode ,
MPI_Info info ,
MPI_File *fh )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_open;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
uop->data.mpi.filename = (char *) malloc((strlen(filename)+1) * sizeof(char));
assert (uop->data.mpi.filename);
bcopy (filename, uop->data.mpi.filename, (strlen(filename)+1) * sizeof(char));
		uop->data.mpi.amode = amode;
		uop->data.mpi.info = info;
uop->data.mpi.fh = *(fh);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_open_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_open_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		recordStats(event, tailEvent(&trace), PHASE_COMM);
		add_file_handle_entry(record_ptr, op->data.mpi.fh);
		addScalarValue(event, COMM, comm_to_index(record_ptr, op->data.mpi.comm), my_rank);
		addScalarValue(event, AMODE, op->data.mpi.amode, my_rank);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addCharValue(event, FILENAME,op->data.mpi.filename,my_rank);
		gettimeofday(&IOend, NULL);
		long long iotime = (IOend.tv_sec-IObegin.tv_sec)*1000000+(IOend.tv_usec-IObegin.tv_usec);
		addScalarValue(event, TIME, iotime, my_rank);
		gettimeofday(&IObegin, NULL);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_open_post( 
int MPI_rc, 
void * pc , MPI_Comm comm ,
const char *filename ,
int amode ,
MPI_Info info ,
MPI_File *fh )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_open;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.comm = comm;
uop->data.mpi.filename = (char *) malloc((strlen(filename)+1) * sizeof(char));
assert (uop->data.mpi.filename);
bcopy (filename, uop->data.mpi.filename, (strlen(filename)+1) * sizeof(char));
		uop->data.mpi.amode = amode;
		uop->data.mpi.info = info;
uop->data.mpi.fh = *(fh);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_open_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_open( 
void * pc , MPI_Comm comm ,
const char *filename ,
int amode ,
MPI_Info info ,
MPI_File *fh )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_open(  (comm),
(filename),
(amode),
(info),
(fh));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_open_pre(pc,
			comm,
			filename,
			amode,
			info,
			fh);
	rc = PMPI_File_open(  (comm),
(filename),
(amode),
(info),
(fh));

umpi_mpi_MPI_File_open_post(rc, pc,
			comm,
			filename,
			amode,
			info,
			fh);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_open(  MPI_Comm comm ,
		const char *filename ,
		int amode ,
		MPI_Info info ,
		MPI_File *fh )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_open(pc,
			comm,
			filename,
			amode,
			info,
			fh );
	return rc;
}

static void mpi_file_open_f_wrap(MPI_Fint *comm , MPI_Fint *filename , MPI_Fint *amode , MPI_Fint *info , MPI_Fint *fh , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_open(pc ,
			(MPI_Comm)(*(comm)),
			filename,
			*(amode),
			(MPI_Info)(*(info)),
			(MPI_File *)fh);
#else /* other mpi's need conversions */
		MPI_File temp_fh;
		temp_fh = MPI_File_f2c(*(fh));
		rc = gwrap_MPI_File_open(pc ,
			MPI_Comm_f2c(*(comm)),
			filename,
			*(amode),
			MPI_Info_f2c(*(info)),
			&temp_fh);
		*(fh) = MPI_File_c2f(temp_fh);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_OPEN(MPI_Fint *comm , MPI_Fint *filename , MPI_Fint *amode , MPI_Fint *info , MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_open_f_wrap(comm, filename, amode, info, fh, ierr); }
extern void mpi_file_open(MPI_Fint *comm , MPI_Fint *filename , MPI_Fint *amode , MPI_Fint *info , MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_open_f_wrap(comm, filename, amode, info, fh, ierr); }
extern void mpi_file_open_(MPI_Fint *comm , MPI_Fint *filename , MPI_Fint *amode , MPI_Fint *info , MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_open_f_wrap(comm, filename, amode, info, fh, ierr); }
extern void mpi_file_open__(MPI_Fint *comm , MPI_Fint *filename , MPI_Fint *amode , MPI_Fint *info , MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_open_f_wrap(comm, filename, amode, info, fh, ierr); }

/*--------------------------------------------- MPI_File_set_view */

static int
umpi_mpi_MPI_File_set_view_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_set_view_pre( 
void * pc , MPI_File fh ,
MPI_Offset disp ,
MPI_Datatype etype ,
MPI_Datatype filetype ,
const char *datarep ,
MPI_Info info )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_set_view;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.disp = disp;
		uop->data.mpi.etype = etype;
		uop->data.mpi.filetype = filetype;
uop->data.mpi.datarep = (char *) malloc((strlen(datarep)+1) * sizeof(char));
assert (uop->data.mpi.datarep);
bcopy (datarep, uop->data.mpi.datarep, (strlen(datarep)+1) * sizeof(char));
		uop->data.mpi.info = info;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_set_view_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_set_view_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.disp, my_rank);
		addScalarValue(event, ETYPE, type_to_index(record_ptr, op->data.mpi.etype), my_rank);
		addScalarValue(event, FILETYPE, type_to_index(record_ptr, op->data.mpi.filetype), my_rank);
		addCharValue(event, DATAREP, op->data.mpi.datarep, my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_set_view_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset disp ,
MPI_Datatype etype ,
MPI_Datatype filetype ,
const char *datarep ,
MPI_Info info )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_set_view;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.disp = disp;
		uop->data.mpi.etype = etype;
		uop->data.mpi.filetype = filetype;
uop->data.mpi.datarep = (char *) malloc((strlen(datarep)+1) * sizeof(char));
assert (uop->data.mpi.datarep);
bcopy (datarep, uop->data.mpi.datarep, (strlen(datarep)+1) * sizeof(char));
		uop->data.mpi.info = info;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_set_view_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_set_view( 
void * pc , MPI_File fh ,
MPI_Offset disp ,
MPI_Datatype etype ,
MPI_Datatype filetype ,
const char *datarep ,
MPI_Info info )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_set_view(  (fh),
(disp),
(etype),
(filetype),
(datarep),
(info));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_set_view_pre(pc,
			fh,
			disp,
			etype,
			filetype,
			datarep,
			info);
	rc = PMPI_File_set_view(  (fh),
(disp),
(etype),
(filetype),
(datarep),
(info));

umpi_mpi_MPI_File_set_view_post(rc, pc,
			fh,
			disp,
			etype,
			filetype,
			datarep,
			info);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_set_view(  MPI_File fh ,
		MPI_Offset disp ,
		MPI_Datatype etype ,
		MPI_Datatype filetype ,
		const char *datarep ,
		MPI_Info info )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_set_view(pc,
			fh,
			disp,
			etype,
			filetype,
			datarep,
			info );
	return rc;
}

static void mpi_file_set_view_f_wrap(MPI_Fint *fh , MPI_Fint *disp , MPI_Fint *etype , MPI_Fint *filetype , MPI_Fint *datarep , MPI_Fint *info , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_set_view(pc ,
			(MPI_File)(*(fh)),
			*(disp),
			(MPI_Datatype)(*(etype)),
			(MPI_Datatype)(*(filetype)),
			datarep,
			(MPI_Info)(*(info)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_set_view(pc ,
			MPI_File_f2c(*(fh)),
			*(disp),
			MPI_Type_f2c(*(etype)),
			MPI_Type_f2c(*(filetype)),
			datarep,
			MPI_Info_f2c(*(info)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_SET_VIEW(MPI_Fint *fh , MPI_Fint *disp , MPI_Fint *etype , MPI_Fint *filetype , MPI_Fint *datarep , MPI_Fint *info , MPI_Fint * ierr) { mpi_file_set_view_f_wrap(fh, disp, etype, filetype, datarep, info, ierr); }
extern void mpi_file_set_view(MPI_Fint *fh , MPI_Fint *disp , MPI_Fint *etype , MPI_Fint *filetype , MPI_Fint *datarep , MPI_Fint *info , MPI_Fint * ierr) { mpi_file_set_view_f_wrap(fh, disp, etype, filetype, datarep, info, ierr); }
extern void mpi_file_set_view_(MPI_Fint *fh , MPI_Fint *disp , MPI_Fint *etype , MPI_Fint *filetype , MPI_Fint *datarep , MPI_Fint *info , MPI_Fint * ierr) { mpi_file_set_view_f_wrap(fh, disp, etype, filetype, datarep, info, ierr); }
extern void mpi_file_set_view__(MPI_Fint *fh , MPI_Fint *disp , MPI_Fint *etype , MPI_Fint *filetype , MPI_Fint *datarep , MPI_Fint *info , MPI_Fint * ierr) { mpi_file_set_view_f_wrap(fh, disp, etype, filetype, datarep, info, ierr); }

/*--------------------------------------------- MPI_File_write_at */

static int
umpi_mpi_MPI_File_write_at_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_at_pre( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_at;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_at_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_at_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.offset, my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_at_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_at;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_at_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_at( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_at(  (fh),
(offset),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_at_pre(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_write_at(  (fh),
(offset),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_write_at_post(rc, pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_at(  MPI_File fh ,
		MPI_Offset offset ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_at(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_write_at_f_wrap(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_at(pc ,
			(MPI_File)(*(fh)),
			*(offset),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_write_at(pc ,
			MPI_File_f2c(*(fh)),
			*(offset),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_AT(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_write_at(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_write_at_(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_write_at__(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_f_wrap(fh, offset, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_write_all */

static int
umpi_mpi_MPI_File_write_all_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
		createEvent(&event, op->op, my_rank);
	#ifndef STATIC
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		/*resetStats(event, PHASE_COMM);*/
	#else
		appendEvent(&trace, event);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_all_pre( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_all;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_all_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_all_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_all_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_all;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_all_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_all( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_all(  (fh),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_all_pre(pc,
			fh,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_write_all(  (fh),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_write_all_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_all(  MPI_File fh ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_all(pc,
			fh,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_write_all_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_all(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_write_all(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_ALL(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_all_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write_all(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_all_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write_all_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_all_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write_all__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_all_f_wrap(fh, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_write */

static int
umpi_mpi_MPI_File_write_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		/*resetStats(event, PHASE_COMM);*/
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_pre( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write(  (fh),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_pre(pc,
			fh,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_write(  (fh),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_write_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write(  MPI_File fh ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write(pc,
			fh,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_write_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_write(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_f_wrap(fh, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_write_at_all */

static int
umpi_mpi_MPI_File_write_at_all_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
  		createEvent(&event, op->op, my_rank);
	#ifndef STATIC
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		/*resetStats(event, PHASE_COMM);*/
	#else
		appendEvent(&trace, event);	
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_at_all_pre( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_at_all;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_at_all_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_at_all_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.offset, my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_at_all_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_at_all;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_at_all_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_at_all( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_at_all(  (fh),
(offset),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_at_all_pre(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_write_at_all(  (fh),
(offset),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_write_at_all_post(rc, pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_at_all(  MPI_File fh ,
		MPI_Offset offset ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_at_all(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_write_at_all_f_wrap(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_at_all(pc ,
			(MPI_File)(*(fh)),
			*(offset),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_write_at_all(pc ,
			MPI_File_f2c(*(fh)),
			*(offset),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_AT_ALL(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_all_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_write_at_all(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_all_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_write_at_all_(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_all_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_write_at_all__(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_all_f_wrap(fh, offset, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_read_all */

static int
umpi_mpi_MPI_File_read_all_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
		createEvent(&event, op->op, my_rank);
	#ifndef STATIC
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		/*resetStats(event, PHASE_COMM);*/
	#else
		appendEvent(&trace, event);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_all_pre( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_all;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_all_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_all_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_all_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_all;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_all_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_all( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_all(  (fh),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_all_pre(pc,
			fh,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_read_all(  (fh),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_read_all_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_all(  MPI_File fh ,
		void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_all(pc,
			fh,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_read_all_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_all(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_read_all(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_ALL(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_all_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read_all(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_all_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read_all_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_all_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read_all__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_all_f_wrap(fh, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_read */

static int
umpi_mpi_MPI_File_read_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
  		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		/*resetStats(event, PHASE_COMM);*/
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_pre( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace),PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read(  (fh),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_pre(pc,
			fh,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_read(  (fh),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_read_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read(  MPI_File fh ,
		void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read(pc,
			fh,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_read_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_read(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_f_wrap(fh, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_read_at */

static int
umpi_mpi_MPI_File_read_at_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
  		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		/*resetStats(event, PHASE_COMM);*/
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_at_pre( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_at;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_at_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_at_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.offset, my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace),PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_at_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_at;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_at_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_at( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_at(  (fh),
(offset),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_at_pre(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_read_at(  (fh),
(offset),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_read_at_post(rc, pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_at(  MPI_File fh ,
		MPI_Offset offset ,
		void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_at(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_read_at_f_wrap(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_at(pc ,
			(MPI_File)(*(fh)),
			*(offset),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_read_at(pc ,
			MPI_File_f2c(*(fh)),
			*(offset),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_AT(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_read_at(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_read_at_(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_read_at__(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_f_wrap(fh, offset, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_read_at_all */

static int
umpi_mpi_MPI_File_read_at_all_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
		createEvent(&event, op->op, my_rank);
	#ifndef STATIC
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		//resetStats(event, PHASE_COMM);
	#else
		appendEvent(&trace, event);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_at_all_pre( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_at_all;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_at_all_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_at_all_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.offset, my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_at_all_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_at_all;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_at_all_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_at_all( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_at_all(  (fh),
(offset),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_at_all_pre(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_read_at_all(  (fh),
(offset),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_read_at_all_post(rc, pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_at_all(  MPI_File fh ,
		MPI_Offset offset ,
		void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_at_all(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_read_at_all_f_wrap(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_at_all(pc ,
			(MPI_File)(*(fh)),
			*(offset),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_read_at_all(pc ,
			MPI_File_f2c(*(fh)),
			*(offset),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_AT_ALL(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_all_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_read_at_all(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_all_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_read_at_all_(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_all_f_wrap(fh, offset, buf, count, datatype, status, ierr); }
extern void mpi_file_read_at_all__(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_all_f_wrap(fh, offset, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_seek */

static int
umpi_mpi_MPI_File_seek_immediate_local_pre(umpi_op_t *op)
{

{
{ 
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_seek_pre( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
int whence )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_seek;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
		uop->data.mpi.whence = whence;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_seek_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_seek_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.offset, my_rank);
		addScalarValue(event, WHENCE, op->data.mpi.whence, my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_seek_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset offset ,
int whence )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_seek;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
		uop->data.mpi.whence = whence;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_seek_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_seek( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
int whence )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_seek(  (fh),
(offset),
(whence));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_seek_pre(pc,
			fh,
			offset,
			whence);
	rc = PMPI_File_seek(  (fh),
(offset),
(whence));

umpi_mpi_MPI_File_seek_post(rc, pc,
			fh,
			offset,
			whence);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_seek(  MPI_File fh ,
		MPI_Offset offset ,
		int whence )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_seek(pc,
			fh,
			offset,
			whence );
	return rc;
}

static void mpi_file_seek_f_wrap(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *whence , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_seek(pc ,
			(MPI_File)(*(fh)),
			*(offset),
			*(whence));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_seek(pc ,
			MPI_File_f2c(*(fh)),
			*(offset),
			*(whence));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_SEEK(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *whence , MPI_Fint * ierr) { mpi_file_seek_f_wrap(fh, offset, whence, ierr); }
extern void mpi_file_seek(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *whence , MPI_Fint * ierr) { mpi_file_seek_f_wrap(fh, offset, whence, ierr); }
extern void mpi_file_seek_(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *whence , MPI_Fint * ierr) { mpi_file_seek_f_wrap(fh, offset, whence, ierr); }
extern void mpi_file_seek__(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *whence , MPI_Fint * ierr) { mpi_file_seek_f_wrap(fh, offset, whence, ierr); }

/*--------------------------------------------- MPI_File_seek_shared */

static int
umpi_mpi_MPI_File_seek_shared_immediate_local_pre(umpi_op_t *op)
{

{
{ 
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_seek_shared_pre( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
int whence )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_seek_shared;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
		uop->data.mpi.whence = whence;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_seek_shared_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_seek_shared_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.offset, my_rank);
		addScalarValue(event, WHENCE, op->data.mpi.whence, my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_seek_shared_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset offset ,
int whence )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_seek_shared;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
		uop->data.mpi.whence = whence;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_seek_shared_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_seek_shared( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
int whence )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_seek_shared(  (fh),
(offset),
(whence));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_seek_shared_pre(pc,
			fh,
			offset,
			whence);
	rc = PMPI_File_seek_shared(  (fh),
(offset),
(whence));

umpi_mpi_MPI_File_seek_shared_post(rc, pc,
			fh,
			offset,
			whence);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_seek_shared(  MPI_File fh ,
		MPI_Offset offset ,
		int whence )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_seek_shared(pc,
			fh,
			offset,
			whence );
	return rc;
}

static void mpi_file_seek_shared_f_wrap(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *whence , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_seek_shared(pc ,
			(MPI_File)(*(fh)),
			*(offset),
			*(whence));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_seek_shared(pc ,
			MPI_File_f2c(*(fh)),
			*(offset),
			*(whence));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_SEEK_SHARED(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *whence , MPI_Fint * ierr) { mpi_file_seek_shared_f_wrap(fh, offset, whence, ierr); }
extern void mpi_file_seek_shared(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *whence , MPI_Fint * ierr) { mpi_file_seek_shared_f_wrap(fh, offset, whence, ierr); }
extern void mpi_file_seek_shared_(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *whence , MPI_Fint * ierr) { mpi_file_seek_shared_f_wrap(fh, offset, whence, ierr); }
extern void mpi_file_seek_shared__(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *whence , MPI_Fint * ierr) { mpi_file_seek_shared_f_wrap(fh, offset, whence, ierr); }

/*--------------------------------------------- MPI_File_close */

static int
umpi_mpi_MPI_File_close_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		remove_file_handle_entry(record_ptr, op->data.mpi.fh);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_close_pre( 
void * pc , MPI_File *fh )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_close;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.fh = *(fh);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_close_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_close_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		gettimeofday(&IOend, NULL);
		long long int iotime = (IOend.tv_sec-IObegin.tv_sec)*1000000+(IOend.tv_usec-IObegin.tv_usec);
		addScalarValue(event, TIME, iotime, my_rank);
		gettimeofday(&IObegin, NULL);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_close_post( 
int MPI_rc, 
void * pc , MPI_File *fh )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_close;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.fh = *(fh);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_close_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_close( 
void * pc , MPI_File *fh )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_close(  (fh));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_close_pre(pc,
			fh);
	rc = PMPI_File_close(  (fh));

umpi_mpi_MPI_File_close_post(rc, pc,
			fh);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_close(  MPI_File *fh )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_close(pc,
			fh );
	return rc;
}

static void mpi_file_close_f_wrap(MPI_Fint *fh , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_close(pc ,
			(MPI_File *)fh);
#else /* other mpi's need conversions */
		MPI_File temp_fh;
		temp_fh = MPI_File_f2c(*(fh));
		rc = gwrap_MPI_File_close(pc ,
			&temp_fh);
		*(fh) = MPI_File_c2f(temp_fh);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_CLOSE(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_close_f_wrap(fh, ierr); }
extern void mpi_file_close(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_close_f_wrap(fh, ierr); }
extern void mpi_file_close_(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_close_f_wrap(fh, ierr); }
extern void mpi_file_close__(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_close_f_wrap(fh, ierr); }

/*--------------------------------------------- MPI_File_delete */

static int
umpi_mpi_MPI_File_delete_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_delete_pre( 
void * pc , const char *filename ,
MPI_Info info )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_delete;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.filename = (char *) malloc((strlen(filename)+1) * sizeof(char));
assert (uop->data.mpi.filename);
bcopy (filename, uop->data.mpi.filename, (strlen(filename)+1) * sizeof(char));
		uop->data.mpi.info = info;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_delete_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_delete_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addCharValue(event, FILENAME,op->data.mpi.filename,my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_delete_post( 
int MPI_rc, 
void * pc , const char *filename ,
MPI_Info info )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_delete;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
uop->data.mpi.filename = (char *) malloc((strlen(filename)+1) * sizeof(char));
assert (uop->data.mpi.filename);
bcopy (filename, uop->data.mpi.filename, (strlen(filename)+1) * sizeof(char));
		uop->data.mpi.info = info;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_delete_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_delete( 
void * pc , const char *filename ,
MPI_Info info )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_delete(  (filename),
(info));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_delete_pre(pc,
			filename,
			info);
	rc = PMPI_File_delete(  (filename),
(info));

umpi_mpi_MPI_File_delete_post(rc, pc,
			filename,
			info);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_delete(  const char *filename ,
		MPI_Info info )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_delete(pc,
			filename,
			info );
	return rc;
}

static void mpi_file_delete_f_wrap(MPI_Fint *filename , MPI_Fint *info , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_delete(pc ,
			filename,
			(MPI_Info)(*(info)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_delete(pc ,
			filename,
			MPI_Info_f2c(*(info)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_DELETE(MPI_Fint *filename , MPI_Fint *info , MPI_Fint * ierr) { mpi_file_delete_f_wrap(filename, info, ierr); }
extern void mpi_file_delete(MPI_Fint *filename , MPI_Fint *info , MPI_Fint * ierr) { mpi_file_delete_f_wrap(filename, info, ierr); }
extern void mpi_file_delete_(MPI_Fint *filename , MPI_Fint *info , MPI_Fint * ierr) { mpi_file_delete_f_wrap(filename, info, ierr); }
extern void mpi_file_delete__(MPI_Fint *filename , MPI_Fint *info , MPI_Fint * ierr) { mpi_file_delete_f_wrap(filename, info, ierr); }

/*--------------------------------------------- MPI_File_set_size */

static int
umpi_mpi_MPI_File_set_size_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_set_size_pre( 
void * pc , MPI_File fh ,
MPI_Offset size )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_set_size;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.size = size;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_set_size_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_set_size_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.size, my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_set_size_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset size )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_set_size;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.size = size;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_set_size_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_set_size( 
void * pc , MPI_File fh ,
MPI_Offset size )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_set_size(  (fh),
(size));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_set_size_pre(pc,
			fh,
			size);
	rc = PMPI_File_set_size(  (fh),
(size));

umpi_mpi_MPI_File_set_size_post(rc, pc,
			fh,
			size);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_set_size(  MPI_File fh ,
		MPI_Offset size )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_set_size(pc,
			fh,
			size );
	return rc;
}

static void mpi_file_set_size_f_wrap(MPI_Fint *fh , MPI_Fint *size , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_set_size(pc ,
			(MPI_File)(*(fh)),
			*(size));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_set_size(pc ,
			MPI_File_f2c(*(fh)),
			*(size));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_SET_SIZE(MPI_Fint *fh , MPI_Fint *size , MPI_Fint * ierr) { mpi_file_set_size_f_wrap(fh, size, ierr); }
extern void mpi_file_set_size(MPI_Fint *fh , MPI_Fint *size , MPI_Fint * ierr) { mpi_file_set_size_f_wrap(fh, size, ierr); }
extern void mpi_file_set_size_(MPI_Fint *fh , MPI_Fint *size , MPI_Fint * ierr) { mpi_file_set_size_f_wrap(fh, size, ierr); }
extern void mpi_file_set_size__(MPI_Fint *fh , MPI_Fint *size , MPI_Fint * ierr) { mpi_file_set_size_f_wrap(fh, size, ierr); }

/*--------------------------------------------- MPI_File_preallocate */

static int
umpi_mpi_MPI_File_preallocate_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
        createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_preallocate_pre( 
void * pc , MPI_File fh ,
MPI_Offset size )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_preallocate;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.size = size;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_preallocate_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_preallocate_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
        	addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
       		addScalarValue(event, OFFSET, op->data.mpi.size, my_rank);
        	appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_preallocate_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset size )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_preallocate;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.size = size;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_preallocate_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_preallocate( 
void * pc , MPI_File fh ,
MPI_Offset size )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_preallocate(  (fh),
(size));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_preallocate_pre(pc,
			fh,
			size);
	rc = PMPI_File_preallocate(  (fh),
(size));

umpi_mpi_MPI_File_preallocate_post(rc, pc,
			fh,
			size);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_preallocate(  MPI_File fh ,
		MPI_Offset size )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_preallocate(pc,
			fh,
			size );
	return rc;
}

static void mpi_file_preallocate_f_wrap(MPI_Fint *fh , MPI_Fint *size , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_preallocate(pc ,
			(MPI_File)(*(fh)),
			*(size));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_preallocate(pc ,
			MPI_File_f2c(*(fh)),
			*(size));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_PREALLOCATE(MPI_Fint *fh , MPI_Fint *size , MPI_Fint * ierr) { mpi_file_preallocate_f_wrap(fh, size, ierr); }
extern void mpi_file_preallocate(MPI_Fint *fh , MPI_Fint *size , MPI_Fint * ierr) { mpi_file_preallocate_f_wrap(fh, size, ierr); }
extern void mpi_file_preallocate_(MPI_Fint *fh , MPI_Fint *size , MPI_Fint * ierr) { mpi_file_preallocate_f_wrap(fh, size, ierr); }
extern void mpi_file_preallocate__(MPI_Fint *fh , MPI_Fint *size , MPI_Fint * ierr) { mpi_file_preallocate_f_wrap(fh, size, ierr); }

/*--------------------------------------------- MPI_File_sync */

static int
umpi_mpi_MPI_File_sync_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
        createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_sync_pre( 
void * pc , MPI_File fh )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_sync;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_sync_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_sync_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
        	addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
        	appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_sync_post( 
int MPI_rc, 
void * pc , MPI_File fh )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_sync;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_sync_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_sync( 
void * pc , MPI_File fh )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_sync(  (fh));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_sync_pre(pc,
			fh);
	rc = PMPI_File_sync(  (fh));

umpi_mpi_MPI_File_sync_post(rc, pc,
			fh);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_sync(  MPI_File fh )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_sync(pc,
			fh );
	return rc;
}

static void mpi_file_sync_f_wrap(MPI_Fint *fh , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_sync(pc ,
			(MPI_File)(*(fh)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_sync(pc ,
			MPI_File_f2c(*(fh)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_SYNC(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_sync_f_wrap(fh, ierr); }
extern void mpi_file_sync(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_sync_f_wrap(fh, ierr); }
extern void mpi_file_sync_(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_sync_f_wrap(fh, ierr); }
extern void mpi_file_sync__(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_sync_f_wrap(fh, ierr); }

/*--------------------------------------------- MPI_File_set_atomicity */

static int
umpi_mpi_MPI_File_set_atomicity_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_set_atomicity_pre( 
void * pc , MPI_File fh ,
int flag )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_set_atomicity;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.flag = flag;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_set_atomicity_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_set_atomicity_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, FLAG, op->data.mpi.flag, my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_set_atomicity_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
int flag )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_set_atomicity;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.flag = flag;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_set_atomicity_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_set_atomicity( 
void * pc , MPI_File fh ,
int flag )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_set_atomicity(  (fh),
(flag));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_set_atomicity_pre(pc,
			fh,
			flag);
	rc = PMPI_File_set_atomicity(  (fh),
(flag));

umpi_mpi_MPI_File_set_atomicity_post(rc, pc,
			fh,
			flag);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_set_atomicity(  MPI_File fh ,
		int flag )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_set_atomicity(pc,
			fh,
			flag );
	return rc;
}

static void mpi_file_set_atomicity_f_wrap(MPI_Fint *fh , MPI_Fint *flag , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_set_atomicity(pc ,
			(MPI_File)(*(fh)),
			*(flag));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_set_atomicity(pc ,
			MPI_File_f2c(*(fh)),
			*(flag));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_SET_ATOMICITY(MPI_Fint *fh , MPI_Fint *flag , MPI_Fint * ierr) { mpi_file_set_atomicity_f_wrap(fh, flag, ierr); }
extern void mpi_file_set_atomicity(MPI_Fint *fh , MPI_Fint *flag , MPI_Fint * ierr) { mpi_file_set_atomicity_f_wrap(fh, flag, ierr); }
extern void mpi_file_set_atomicity_(MPI_Fint *fh , MPI_Fint *flag , MPI_Fint * ierr) { mpi_file_set_atomicity_f_wrap(fh, flag, ierr); }
extern void mpi_file_set_atomicity__(MPI_Fint *fh , MPI_Fint *flag , MPI_Fint * ierr) { mpi_file_set_atomicity_f_wrap(fh, flag, ierr); }

/*--------------------------------------------- MPI_File_iread */

static int
umpi_mpi_MPI_File_iread_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iread_pre( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iread;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_iread_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_iread_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh),my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace,event);
		#ifndef FEATURE_PARAM_HISTO
			add_request_entry(record_req, op->data.mpi.request);
		#endif
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iread_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iread;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_iread_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_iread( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_iread(  (fh),
(buf),
(count),
(datatype),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_iread_pre(pc,
			fh,
			buf,
			count,
			datatype,
			request);
	rc = PMPI_File_iread(  (fh),
(buf),
(count),
(datatype),
(request));

umpi_mpi_MPI_File_iread_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_iread(  MPI_File fh ,
		void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_iread(pc,
			fh,
			buf,
			count,
			datatype,
			request );
	return rc;
}

static void mpi_file_iread_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_iread(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_File_iread(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_IREAD(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iread(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iread_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iread__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_f_wrap(fh, buf, count, datatype, request, ierr); }

/*--------------------------------------------- MPI_File_iread_at */

static int
umpi_mpi_MPI_File_iread_at_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iread_at_pre( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iread_at;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_iread_at_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_iread_at_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh),my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.offset, my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace,event);
		#ifndef FEATURE_PARAM_HISTO
			add_request_entry(record_req, op->data.mpi.request);
		#endif
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iread_at_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iread_at;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_iread_at_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_iread_at( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_iread_at(  (fh),
(offset),
(buf),
(count),
(datatype),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_iread_at_pre(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			request);
	rc = PMPI_File_iread_at(  (fh),
(offset),
(buf),
(count),
(datatype),
(request));

umpi_mpi_MPI_File_iread_at_post(rc, pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_iread_at(  MPI_File fh ,
		MPI_Offset offset ,
		void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_iread_at(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			request );
	return rc;
}

static void mpi_file_iread_at_f_wrap(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_iread_at(pc ,
			(MPI_File)(*(fh)),
			*(offset),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_File_iread_at(pc ,
			MPI_File_f2c(*(fh)),
			*(offset),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_IREAD_AT(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_at_f_wrap(fh, offset, buf, count, datatype, request, ierr); }
extern void mpi_file_iread_at(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_at_f_wrap(fh, offset, buf, count, datatype, request, ierr); }
extern void mpi_file_iread_at_(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_at_f_wrap(fh, offset, buf, count, datatype, request, ierr); }
extern void mpi_file_iread_at__(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_at_f_wrap(fh, offset, buf, count, datatype, request, ierr); }

/*--------------------------------------------- MPI_File_iwrite */

static int
umpi_mpi_MPI_File_iwrite_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iwrite_pre( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iwrite;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_iwrite_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_iwrite_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh),my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace,event);
		#ifndef FEATURE_PARAM_HISTO
			add_request_entry(record_req, op->data.mpi.request);
		#endif
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iwrite_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iwrite;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_iwrite_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_iwrite( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_iwrite(  (fh),
(buf),
(count),
(datatype),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_iwrite_pre(pc,
			fh,
			buf,
			count,
			datatype,
			request);
	rc = PMPI_File_iwrite(  (fh),
(buf),
(count),
(datatype),
(request));

umpi_mpi_MPI_File_iwrite_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_iwrite(  MPI_File fh ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_iwrite(pc,
			fh,
			buf,
			count,
			datatype,
			request );
	return rc;
}

static void mpi_file_iwrite_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_iwrite(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_File_iwrite(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_IWRITE(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iwrite(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iwrite_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iwrite__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_f_wrap(fh, buf, count, datatype, request, ierr); }

/*--------------------------------------------- MPI_File_iwrite_at */

static int
umpi_mpi_MPI_File_iwrite_at_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iwrite_at_pre( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iwrite_at;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_iwrite_at_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_iwrite_at_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh),my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.offset, my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace,event);
		#ifndef FEATURE_PARAM_HISTO
			add_request_entry(record_req, op->data.mpi.request);
		#endif
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iwrite_at_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iwrite_at;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_iwrite_at_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_iwrite_at( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_iwrite_at(  (fh),
(offset),
(buf),
(count),
(datatype),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_iwrite_at_pre(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			request);
	rc = PMPI_File_iwrite_at(  (fh),
(offset),
(buf),
(count),
(datatype),
(request));

umpi_mpi_MPI_File_iwrite_at_post(rc, pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_iwrite_at(  MPI_File fh ,
		MPI_Offset offset ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_iwrite_at(pc,
			fh,
			offset,
			buf,
			count,
			datatype,
			request );
	return rc;
}

static void mpi_file_iwrite_at_f_wrap(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_iwrite_at(pc ,
			(MPI_File)(*(fh)),
			*(offset),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_File_iwrite_at(pc ,
			MPI_File_f2c(*(fh)),
			*(offset),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_IWRITE_AT(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_at_f_wrap(fh, offset, buf, count, datatype, request, ierr); }
extern void mpi_file_iwrite_at(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_at_f_wrap(fh, offset, buf, count, datatype, request, ierr); }
extern void mpi_file_iwrite_at_(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_at_f_wrap(fh, offset, buf, count, datatype, request, ierr); }
extern void mpi_file_iwrite_at__(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_at_f_wrap(fh, offset, buf, count, datatype, request, ierr); }

/*--------------------------------------------- MPI_File_read_shared */

static int
umpi_mpi_MPI_File_read_shared_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		/*resetStats(event, PHASE_COMM);*/
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_shared_pre( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_shared;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_shared_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_shared_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);	
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_shared_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_shared;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_shared_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_shared( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_shared(  (fh),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_shared_pre(pc,
			fh,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_read_shared(  (fh),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_read_shared_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_shared(  MPI_File fh ,
		void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_shared(pc,
			fh,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_read_shared_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_shared(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_read_shared(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_SHARED(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_shared_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read_shared(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_shared_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read_shared_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_shared_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read_shared__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_shared_f_wrap(fh, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_write_shared */

static int
umpi_mpi_MPI_File_write_shared_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		/*resetStats(event, PHASE_COMM);*/
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_shared_pre( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_shared;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_shared_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_shared_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);	
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_shared_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_shared;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_shared_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_shared( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_shared(  (fh),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_shared_pre(pc,
			fh,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_write_shared(  (fh),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_write_shared_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_shared(  MPI_File fh ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_shared(pc,
			fh,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_write_shared_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_shared(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_write_shared(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_SHARED(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_shared_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write_shared(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_shared_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write_shared_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_shared_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write_shared__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_shared_f_wrap(fh, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_iread_shared */

static int
umpi_mpi_MPI_File_iread_shared_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iread_shared_pre( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iread_shared;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_iread_shared_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_iread_shared_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh),my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace,event);
		#ifndef FEATURE_PARAM_HISTO
			add_request_entry(record_req, op->data.mpi.request);
		#endif
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iread_shared_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iread_shared;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_iread_shared_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_iread_shared( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_iread_shared(  (fh),
(buf),
(count),
(datatype),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_iread_shared_pre(pc,
			fh,
			buf,
			count,
			datatype,
			request);
	rc = PMPI_File_iread_shared(  (fh),
(buf),
(count),
(datatype),
(request));

umpi_mpi_MPI_File_iread_shared_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_iread_shared(  MPI_File fh ,
		void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_iread_shared(pc,
			fh,
			buf,
			count,
			datatype,
			request );
	return rc;
}

static void mpi_file_iread_shared_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_iread_shared(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_File_iread_shared(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_IREAD_SHARED(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_shared_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iread_shared(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_shared_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iread_shared_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_shared_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iread_shared__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iread_shared_f_wrap(fh, buf, count, datatype, request, ierr); }

/*--------------------------------------------- MPI_File_iwrite_shared */

static int
umpi_mpi_MPI_File_iwrite_shared_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iwrite_shared_pre( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iwrite_shared;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_iwrite_shared_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_iwrite_shared_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh),my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace,event);
		#ifndef FEATURE_PARAM_HISTO
			add_request_entry(record_req, op->data.mpi.request);
		#endif
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_iwrite_shared_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_iwrite_shared;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.request = *(request);
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_iwrite_shared_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_iwrite_shared( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Request *request )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_iwrite_shared(  (fh),
(buf),
(count),
(datatype),
(request));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_iwrite_shared_pre(pc,
			fh,
			buf,
			count,
			datatype,
			request);
	rc = PMPI_File_iwrite_shared(  (fh),
(buf),
(count),
(datatype),
(request));

umpi_mpi_MPI_File_iwrite_shared_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			request);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_iwrite_shared(  MPI_File fh ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Request *request )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_iwrite_shared(pc,
			fh,
			buf,
			count,
			datatype,
			request );
	return rc;
}

static void mpi_file_iwrite_shared_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_iwrite_shared(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Request *)request);
#else /* other mpi's need conversions */
		MPI_Request temp_request;
		temp_request = MPI_Request_f2c(*(request));
		rc = gwrap_MPI_File_iwrite_shared(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_request);
		*(request) = MPI_Request_c2f(temp_request);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_IWRITE_SHARED(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_shared_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iwrite_shared(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_shared_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iwrite_shared_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_shared_f_wrap(fh, buf, count, datatype, request, ierr); }
extern void mpi_file_iwrite_shared__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *request , MPI_Fint * ierr) { mpi_file_iwrite_shared_f_wrap(fh, buf, count, datatype, request, ierr); }

/*--------------------------------------------- MPI_File_read_ordered */

static int
umpi_mpi_MPI_File_read_ordered_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		/*resetStats(event, PHASE_COMM);*/
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_ordered_pre( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_ordered;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_ordered_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_ordered_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);	
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_ordered_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_ordered;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_ordered_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_ordered( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_ordered(  (fh),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_ordered_pre(pc,
			fh,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_read_ordered(  (fh),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_read_ordered_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_ordered(  MPI_File fh ,
		void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_ordered(pc,
			fh,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_read_ordered_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_ordered(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_read_ordered(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_ORDERED(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_ordered_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read_ordered(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_ordered_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read_ordered_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_ordered_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_read_ordered__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_ordered_f_wrap(fh, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_write_ordered */

static int
umpi_mpi_MPI_File_write_ordered_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
		/*resetStats(event, PHASE_COMM);*/
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_ordered_pre( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_ordered;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_ordered_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_ordered_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		//recordStats(event, tailEvent(&trace), PHASE_COMM);
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);	
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_ordered_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_ordered;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_ordered_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_ordered( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_ordered(  (fh),
(buf),
(count),
(datatype),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_ordered_pre(pc,
			fh,
			buf,
			count,
			datatype,
			status);
	rc = PMPI_File_write_ordered(  (fh),
(buf),
(count),
(datatype),
(status));

umpi_mpi_MPI_File_write_ordered_post(rc, pc,
			fh,
			buf,
			count,
			datatype,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_ordered(  MPI_File fh ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_ordered(pc,
			fh,
			buf,
			count,
			datatype,
			status );
	return rc;
}

static void mpi_file_write_ordered_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_ordered(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)),
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_write_ordered(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)),
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_ORDERED(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_ordered_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write_ordered(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_ordered_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write_ordered_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_ordered_f_wrap(fh, buf, count, datatype, status, ierr); }
extern void mpi_file_write_ordered__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_ordered_f_wrap(fh, buf, count, datatype, status, ierr); }

/*--------------------------------------------- MPI_File_read_at_all_begin */

static int
umpi_mpi_MPI_File_read_at_all_begin_immediate_local_pre(umpi_op_t *op)
{

{
{
	createEvent(&event, op->op, my_rank);
	#ifndef STATIC
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#else
		appendEvent(&trace, event);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_at_all_begin_pre( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_at_all_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_at_all_begin_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_at_all_begin_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.offset, my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_at_all_begin_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_at_all_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_at_all_begin_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_at_all_begin( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
void *buf ,
int count ,
MPI_Datatype datatype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_at_all_begin(  (fh),
(offset),
(buf),
(count),
(datatype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_at_all_begin_pre(pc,
			fh,
			offset,
			buf,
			count,
			datatype);
	rc = PMPI_File_read_at_all_begin(  (fh),
(offset),
(buf),
(count),
(datatype));

umpi_mpi_MPI_File_read_at_all_begin_post(rc, pc,
			fh,
			offset,
			buf,
			count,
			datatype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_at_all_begin(  MPI_File fh ,
		MPI_Offset offset ,
		void *buf ,
		int count ,
		MPI_Datatype datatype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_at_all_begin(pc,
			fh,
			offset,
			buf,
			count,
			datatype );
	return rc;
}

static void mpi_file_read_at_all_begin_f_wrap(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_at_all_begin(pc ,
			(MPI_File)(*(fh)),
			*(offset),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_read_at_all_begin(pc ,
			MPI_File_f2c(*(fh)),
			*(offset),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_AT_ALL_BEGIN(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_at_all_begin_f_wrap(fh, offset, buf, count, datatype, ierr); }
extern void mpi_file_read_at_all_begin(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_at_all_begin_f_wrap(fh, offset, buf, count, datatype, ierr); }
extern void mpi_file_read_at_all_begin_(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_at_all_begin_f_wrap(fh, offset, buf, count, datatype, ierr); }
extern void mpi_file_read_at_all_begin__(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_at_all_begin_f_wrap(fh, offset, buf, count, datatype, ierr); }

/*--------------------------------------------- MPI_File_read_at_all_end */

static int
umpi_mpi_MPI_File_read_at_all_end_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_at_all_end_pre( 
void * pc , MPI_File fh ,
void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_at_all_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_at_all_end_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_at_all_end_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_at_all_end_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_at_all_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_at_all_end_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_at_all_end( 
void * pc , MPI_File fh ,
void *buf ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_at_all_end(  (fh),
(buf),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_at_all_end_pre(pc,
			fh,
			buf,
			status);
	rc = PMPI_File_read_at_all_end(  (fh),
(buf),
(status));

umpi_mpi_MPI_File_read_at_all_end_post(rc, pc,
			fh,
			buf,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_at_all_end(  MPI_File fh ,
		void *buf ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_at_all_end(pc,
			fh,
			buf,
			status );
	return rc;
}

static void mpi_file_read_at_all_end_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_at_all_end(pc ,
			(MPI_File)(*(fh)),
			buf,
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_read_at_all_end(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_AT_ALL_END(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_read_at_all_end(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_read_at_all_end_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_read_at_all_end__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_at_all_end_f_wrap(fh, buf, status, ierr); }

/*--------------------------------------------- MPI_File_write_at_all_begin */

static int
umpi_mpi_MPI_File_write_at_all_begin_immediate_local_pre(umpi_op_t *op)
{

{
{
	createEvent(&event, op->op, my_rank);
	#ifndef STATIC
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#else
		appendEvent(&trace, event);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_at_all_begin_pre( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_at_all_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_at_all_begin_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_at_all_begin_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		addScalarValue(event, OFFSET, op->data.mpi.offset, my_rank);
		addScalarValue(event, COUNT, op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype), my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_at_all_begin_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_at_all_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
		uop->data.mpi.offset = offset;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_at_all_begin_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_at_all_begin( 
void * pc , MPI_File fh ,
MPI_Offset offset ,
const void *buf ,
int count ,
MPI_Datatype datatype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_at_all_begin(  (fh),
(offset),
(buf),
(count),
(datatype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_at_all_begin_pre(pc,
			fh,
			offset,
			buf,
			count,
			datatype);
	rc = PMPI_File_write_at_all_begin(  (fh),
(offset),
(buf),
(count),
(datatype));

umpi_mpi_MPI_File_write_at_all_begin_post(rc, pc,
			fh,
			offset,
			buf,
			count,
			datatype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_at_all_begin(  MPI_File fh ,
		MPI_Offset offset ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_at_all_begin(pc,
			fh,
			offset,
			buf,
			count,
			datatype );
	return rc;
}

static void mpi_file_write_at_all_begin_f_wrap(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_at_all_begin(pc ,
			(MPI_File)(*(fh)),
			*(offset),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_write_at_all_begin(pc ,
			MPI_File_f2c(*(fh)),
			*(offset),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_AT_ALL_BEGIN(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_at_all_begin_f_wrap(fh, offset, buf, count, datatype, ierr); }
extern void mpi_file_write_at_all_begin(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_at_all_begin_f_wrap(fh, offset, buf, count, datatype, ierr); }
extern void mpi_file_write_at_all_begin_(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_at_all_begin_f_wrap(fh, offset, buf, count, datatype, ierr); }
extern void mpi_file_write_at_all_begin__(MPI_Fint *fh , MPI_Fint *offset , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_at_all_begin_f_wrap(fh, offset, buf, count, datatype, ierr); }

/*--------------------------------------------- MPI_File_write_at_all_end */

static int
umpi_mpi_MPI_File_write_at_all_end_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_at_all_end_pre( 
void * pc , MPI_File fh ,
const void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_at_all_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_at_all_end_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_at_all_end_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_at_all_end_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_at_all_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_at_all_end_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_at_all_end( 
void * pc , MPI_File fh ,
const void *buf ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_at_all_end(  (fh),
(buf),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_at_all_end_pre(pc,
			fh,
			buf,
			status);
	rc = PMPI_File_write_at_all_end(  (fh),
(buf),
(status));

umpi_mpi_MPI_File_write_at_all_end_post(rc, pc,
			fh,
			buf,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_at_all_end(  MPI_File fh ,
		const void *buf ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_at_all_end(pc,
			fh,
			buf,
			status );
	return rc;
}

static void mpi_file_write_at_all_end_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_at_all_end(pc ,
			(MPI_File)(*(fh)),
			buf,
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_write_at_all_end(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_AT_ALL_END(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_write_at_all_end(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_write_at_all_end_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_write_at_all_end__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_at_all_end_f_wrap(fh, buf, status, ierr); }

/*--------------------------------------------- MPI_File_read_all_begin */

static int
umpi_mpi_MPI_File_read_all_begin_immediate_local_pre(umpi_op_t *op)
{

{
{
	createEvent(&event, op->op, my_rank);
	#ifndef STATIC
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#else
		appendEvent(&trace, event);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_all_begin_pre( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_all_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_all_begin_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_all_begin_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr,op->data.mpi.fh), my_rank);
		addScalarValue(event, COUNT,op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_all_begin_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_all_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_all_begin_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_all_begin( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_all_begin(  (fh),
(buf),
(count),
(datatype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_all_begin_pre(pc,
			fh,
			buf,
			count,
			datatype);
	rc = PMPI_File_read_all_begin(  (fh),
(buf),
(count),
(datatype));

umpi_mpi_MPI_File_read_all_begin_post(rc, pc,
			fh,
			buf,
			count,
			datatype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_all_begin(  MPI_File fh ,
		void *buf ,
		int count ,
		MPI_Datatype datatype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_all_begin(pc,
			fh,
			buf,
			count,
			datatype );
	return rc;
}

static void mpi_file_read_all_begin_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_all_begin(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_read_all_begin(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_ALL_BEGIN(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_all_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_read_all_begin(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_all_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_read_all_begin_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_all_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_read_all_begin__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_all_begin_f_wrap(fh, buf, count, datatype, ierr); }

/*--------------------------------------------- MPI_File_write_all_begin */

static int
umpi_mpi_MPI_File_write_all_begin_immediate_local_pre(umpi_op_t *op)
{

{
{
	createEvent(&event, op->op, my_rank);
	#ifndef STATIC
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#else
		appendEvent(&trace, event);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_all_begin_pre( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_all_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_all_begin_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_all_begin_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr,op->data.mpi.fh), my_rank);
		addScalarValue(event, COUNT,op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_all_begin_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_all_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_all_begin_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_all_begin( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_all_begin(  (fh),
(buf),
(count),
(datatype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_all_begin_pre(pc,
			fh,
			buf,
			count,
			datatype);
	rc = PMPI_File_write_all_begin(  (fh),
(buf),
(count),
(datatype));

umpi_mpi_MPI_File_write_all_begin_post(rc, pc,
			fh,
			buf,
			count,
			datatype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_all_begin(  MPI_File fh ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_all_begin(pc,
			fh,
			buf,
			count,
			datatype );
	return rc;
}

static void mpi_file_write_all_begin_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_all_begin(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_write_all_begin(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_ALL_BEGIN(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_all_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_write_all_begin(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_all_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_write_all_begin_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_all_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_write_all_begin__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_all_begin_f_wrap(fh, buf, count, datatype, ierr); }

/*--------------------------------------------- MPI_File_read_all_end */

static int
umpi_mpi_MPI_File_read_all_end_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_all_end_pre( 
void * pc , MPI_File fh ,
void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_all_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_all_end_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_all_end_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_all_end_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_all_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_all_end_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_all_end( 
void * pc , MPI_File fh ,
void *buf ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_all_end(  (fh),
(buf),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_all_end_pre(pc,
			fh,
			buf,
			status);
	rc = PMPI_File_read_all_end(  (fh),
(buf),
(status));

umpi_mpi_MPI_File_read_all_end_post(rc, pc,
			fh,
			buf,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_all_end(  MPI_File fh ,
		void *buf ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_all_end(pc,
			fh,
			buf,
			status );
	return rc;
}

static void mpi_file_read_all_end_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_all_end(pc ,
			(MPI_File)(*(fh)),
			buf,
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_read_all_end(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_ALL_END(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_read_all_end(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_read_all_end_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_read_all_end__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_all_end_f_wrap(fh, buf, status, ierr); }

/*--------------------------------------------- MPI_File_write_all_end */

static int
umpi_mpi_MPI_File_write_all_end_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_all_end_pre( 
void * pc , MPI_File fh ,
const void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_all_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_all_end_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_all_end_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_all_end_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_all_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_all_end_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_all_end( 
void * pc , MPI_File fh ,
const void *buf ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_all_end(  (fh),
(buf),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_all_end_pre(pc,
			fh,
			buf,
			status);
	rc = PMPI_File_write_all_end(  (fh),
(buf),
(status));

umpi_mpi_MPI_File_write_all_end_post(rc, pc,
			fh,
			buf,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_all_end(  MPI_File fh ,
		const void *buf ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_all_end(pc,
			fh,
			buf,
			status );
	return rc;
}

static void mpi_file_write_all_end_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_all_end(pc ,
			(MPI_File)(*(fh)),
			buf,
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_write_all_end(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_ALL_END(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_write_all_end(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_write_all_end_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_all_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_write_all_end__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_all_end_f_wrap(fh, buf, status, ierr); }

/*--------------------------------------------- MPI_File_read_ordered_begin */

static int
umpi_mpi_MPI_File_read_ordered_begin_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_ordered_begin_pre( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_ordered_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_ordered_begin_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_ordered_begin_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr,op->data.mpi.fh), my_rank);
		addScalarValue(event, COUNT,op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_ordered_begin_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_ordered_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_ordered_begin_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_ordered_begin( 
void * pc , MPI_File fh ,
void *buf ,
int count ,
MPI_Datatype datatype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_ordered_begin(  (fh),
(buf),
(count),
(datatype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_ordered_begin_pre(pc,
			fh,
			buf,
			count,
			datatype);
	rc = PMPI_File_read_ordered_begin(  (fh),
(buf),
(count),
(datatype));

umpi_mpi_MPI_File_read_ordered_begin_post(rc, pc,
			fh,
			buf,
			count,
			datatype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_ordered_begin(  MPI_File fh ,
		void *buf ,
		int count ,
		MPI_Datatype datatype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_ordered_begin(pc,
			fh,
			buf,
			count,
			datatype );
	return rc;
}

static void mpi_file_read_ordered_begin_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_ordered_begin(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_read_ordered_begin(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_ORDERED_BEGIN(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_ordered_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_read_ordered_begin(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_ordered_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_read_ordered_begin_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_ordered_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_read_ordered_begin__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_read_ordered_begin_f_wrap(fh, buf, count, datatype, ierr); }

/*--------------------------------------------- MPI_File_read_ordered_end */

static int
umpi_mpi_MPI_File_read_ordered_end_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_ordered_end_pre( 
void * pc , MPI_File fh ,
void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_ordered_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_read_ordered_end_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_read_ordered_end_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_read_ordered_end_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_read_ordered_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_read_ordered_end_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_read_ordered_end( 
void * pc , MPI_File fh ,
void *buf ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_read_ordered_end(  (fh),
(buf),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_read_ordered_end_pre(pc,
			fh,
			buf,
			status);
	rc = PMPI_File_read_ordered_end(  (fh),
(buf),
(status));

umpi_mpi_MPI_File_read_ordered_end_post(rc, pc,
			fh,
			buf,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_read_ordered_end(  MPI_File fh ,
		void *buf ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_read_ordered_end(pc,
			fh,
			buf,
			status );
	return rc;
}

static void mpi_file_read_ordered_end_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_read_ordered_end(pc ,
			(MPI_File)(*(fh)),
			buf,
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_read_ordered_end(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_READ_ORDERED_END(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_ordered_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_read_ordered_end(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_ordered_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_read_ordered_end_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_ordered_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_read_ordered_end__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_read_ordered_end_f_wrap(fh, buf, status, ierr); }

/*--------------------------------------------- MPI_File_write_ordered_begin */

static int
umpi_mpi_MPI_File_write_ordered_begin_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_ordered_begin_pre( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_ordered_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_ordered_begin_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_ordered_begin_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr,op->data.mpi.fh), my_rank);
		addScalarValue(event, COUNT,op->data.mpi.count, my_rank);
		addScalarValue(event, DATATYPE, type_to_index(record_ptr, op->data.mpi.datatype),my_rank);
		appendEvent(&trace, event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_ordered_begin_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_ordered_begin;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
		uop->data.mpi.count = count;
		uop->data.mpi.datatype = datatype;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_ordered_begin_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_ordered_begin( 
void * pc , MPI_File fh ,
const void *buf ,
int count ,
MPI_Datatype datatype )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_ordered_begin(  (fh),
(buf),
(count),
(datatype));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_ordered_begin_pre(pc,
			fh,
			buf,
			count,
			datatype);
	rc = PMPI_File_write_ordered_begin(  (fh),
(buf),
(count),
(datatype));

umpi_mpi_MPI_File_write_ordered_begin_post(rc, pc,
			fh,
			buf,
			count,
			datatype);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_ordered_begin(  MPI_File fh ,
		const void *buf ,
		int count ,
		MPI_Datatype datatype )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_ordered_begin(pc,
			fh,
			buf,
			count,
			datatype );
	return rc;
}

static void mpi_file_write_ordered_begin_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_ordered_begin(pc ,
			(MPI_File)(*(fh)),
			buf,
			*(count),
			(MPI_Datatype)(*(datatype)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_write_ordered_begin(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			*(count),
			MPI_Type_f2c(*(datatype)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_ORDERED_BEGIN(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_ordered_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_write_ordered_begin(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_ordered_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_write_ordered_begin_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_ordered_begin_f_wrap(fh, buf, count, datatype, ierr); }
extern void mpi_file_write_ordered_begin__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *count , MPI_Fint *datatype , MPI_Fint * ierr) { mpi_file_write_ordered_begin_f_wrap(fh, buf, count, datatype, ierr); }

/*--------------------------------------------- MPI_File_write_ordered_end */

static int
umpi_mpi_MPI_File_write_ordered_end_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO
	#ifndef STATIC
		createEvent(&event, op->op, my_rank);
		recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_ordered_end_pre( 
void * pc , MPI_File fh ,
const void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_ordered_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_write_ordered_end_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_write_ordered_end_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh), my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_write_ordered_end_post( 
int MPI_rc, 
void * pc , MPI_File fh ,
const void *buf ,
MPI_Status *status )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_write_ordered_end;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
uop->data.mpi.buf = buf;
uop->data.mpi.status_src = status->MPI_SOURCE;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_write_ordered_end_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_write_ordered_end( 
void * pc , MPI_File fh ,
const void *buf ,
MPI_Status *status )
{
int rc;
	MPI_Status temp_status;
	if (status == MPI_STATUS_IGNORE) {
		status = &temp_status;
	}


#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_write_ordered_end(  (fh),
(buf),
(status));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_write_ordered_end_pre(pc,
			fh,
			buf,
			status);
	rc = PMPI_File_write_ordered_end(  (fh),
(buf),
(status));

umpi_mpi_MPI_File_write_ordered_end_post(rc, pc,
			fh,
			buf,
			status);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_write_ordered_end(  MPI_File fh ,
		const void *buf ,
		MPI_Status *status )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_write_ordered_end(pc,
			fh,
			buf,
			status );
	return rc;
}

static void mpi_file_write_ordered_end_f_wrap(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_write_ordered_end(pc ,
			(MPI_File)(*(fh)),
			buf,
			(MPI_Status *)status);
#else /* other mpi's need conversions */
		MPI_Status temp_status;
		PMPI_Status_f2c(status, &temp_status);
		rc = gwrap_MPI_File_write_ordered_end(pc ,
			MPI_File_f2c(*(fh)),
			buf,
			&temp_status);
		PMPI_Status_c2f(&temp_status, status);
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_WRITE_ORDERED_END(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_ordered_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_write_ordered_end(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_ordered_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_write_ordered_end_(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_ordered_end_f_wrap(fh, buf, status, ierr); }
extern void mpi_file_write_ordered_end__(MPI_Fint *fh , MPI_Fint *buf , MPI_Fint *status , MPI_Fint * ierr) { mpi_file_write_ordered_end_f_wrap(fh, buf, status, ierr); }

/*--------------------------------------------- MPI_File_get_size */

/*-------------------- Wrapper for MPI_File_get_size omitted */

/*--------------------------------------------- MPI_File_get_amode */

/*-------------------- Wrapper for MPI_File_get_amode omitted */

/*--------------------------------------------- MPI_File_get_group */

/*-------------------- Wrapper for MPI_File_get_group omitted */

/*--------------------------------------------- MPI_File_get_info */

/*-------------------- Wrapper for MPI_File_get_info omitted */

/*--------------------------------------------- MPI_File_get_view */

/*-------------------- Wrapper for MPI_File_get_view omitted */

/*--------------------------------------------- MPI_File_get_byte_offset */

/*-------------------- Wrapper for MPI_File_get_byte_offset omitted */

/*--------------------------------------------- MPI_File_get_position */

/*-------------------- Wrapper for MPI_File_get_position omitted */

/*--------------------------------------------- MPI_File_get_atomicity */

/*-------------------- Wrapper for MPI_File_get_atomicity omitted */

/*--------------------------------------------- MPI_File_set_Info */

/*-------------------- Wrapper for MPI_File_set_Info omitted */

/*--------------------------------------------- MPI_File_c2f */

static int
umpi_mpi_MPI_File_c2f_immediate_local_pre(umpi_op_t *op)
{

{
{
	#ifndef STATIC
	createEvent(&event, op->op, my_rank);
	recordStats(event, tailEvent(&trace), PHASE_COMP);
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_c2f_pre( 
void * pc , MPI_File fh )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_c2f;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_preop;

rc = umpi_mpi_MPI_File_c2f_immediate_local_pre(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return 0;
}

static int
umpi_mpi_MPI_File_c2f_immediate_local_post(umpi_op_t *op)
{

{
{
	#ifndef FEATURE_SKIP_IO	
	#ifndef STATIC
		addScalarValue(event, FH, file_handle_to_index(record_ptr, op->data.mpi.fh),my_rank);
		appendEvent(&trace,event);
		resetStats(headEvent(&trace), PHASE_COMP);
	#endif
	#endif
}
}
return 0;
}


static int
umpi_mpi_MPI_File_c2f_post( 
int MPI_rc, 
void * pc , MPI_File fh )

{
int rc = 0;
umpi_op_t *uop = (umpi_op_t *)calloc(sizeof(umpi_op_t), 1);

assert(uop);
uop->op = umpi_MPI_File_c2f;
uop->rank = umpi_rank;
uop->seq_num = umpi_task_seq_num++;
uop->pc = pc;
		uop->data.mpi.fh = fh;
umpi_inc_ref_count (uop);
#ifdef UMPI_DEBUG_LEVEL_2
q_append (umpi_task_op_q, uop);
#endif

uop->order = umpi_postop;
uop->res = MPI_rc;

rc = umpi_mpi_MPI_File_c2f_immediate_local_post(uop);
umpi_dec_ref_count (uop, UMPI_TASK_SYNCHRONOUS);
return rc;
}

static int
gwrap_MPI_File_c2f( 
void * pc , MPI_File fh )
{
int rc;

#ifndef NO_REENTRY_GUARD
if (in_wrapper) {	rc = PMPI_File_c2f(  (fh));

    return rc;
}
in_wrapper=1;
#endif /* NO_REENTRY_GUARD */

umpi_mpi_MPI_File_c2f_pre(pc,
			fh);
	rc = PMPI_File_c2f(  (fh));

umpi_mpi_MPI_File_c2f_post(rc, pc,
			fh);
#ifndef NO_REENTRY_GUARD
in_wrapper=0;
#endif /* NO_REENTRY_GUARD */
	return rc;
}

extern int  MPI_File_c2f(  MPI_File fh )
{
	int rc = 0;
	void *pc = 0x0;
	GET_FP;
	//DEFINE_PC;
	//GATHER_PC;
	rc = gwrap_MPI_File_c2f(pc,
			fh );
	return rc;
}

static void mpi_file_c2f_f_wrap(MPI_Fint *fh , MPI_Fint * ierr) {
	int rc;
	void *pc = 0x0;
	// DEFINE_PC;
	// GATHER_PC;
	{
#if (defined(MPICH_NAME) && (MPICH_NAME == 1)) /* MPICH test */
		rc = gwrap_MPI_File_c2f(pc ,
			(MPI_File)(*(fh)));
#else /* other mpi's need conversions */
		rc = gwrap_MPI_File_c2f(pc ,
			MPI_File_f2c(*(fh)));
#endif /* MPICH test */
		*ierr = rc;
	}
	return;
}

/* Fortran bindings delegate to wrapper above. */
extern void MPI_FILE_C2F(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_c2f_f_wrap(fh, ierr); }
extern void mpi_file_c2f(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_c2f_f_wrap(fh, ierr); }
extern void mpi_file_c2f_(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_c2f_f_wrap(fh, ierr); }
extern void mpi_file_c2f__(MPI_Fint *fh , MPI_Fint * ierr) { mpi_file_c2f_f_wrap(fh, ierr); }


/*eof*/
